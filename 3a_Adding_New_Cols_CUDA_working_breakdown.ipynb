{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee7325a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DtypeArg' from 'pandas._typing' (C:\\Users\\tdcho\\anaconda3\\lib\\site-packages\\pandas\\_typing.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-20f4a6c46053>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m# pd.set_option('display.max_columns', None)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_print_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m from pandas.io.api import (\n\u001b[0m\u001b[0;32m    145\u001b[0m     \u001b[1;31m# excel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m     \u001b[0mExcelFile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclipboards\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_clipboard\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexcel\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelWriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_excel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeather_format\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_feather\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgbq\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mread_gbq\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexcel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_base\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mExcelFile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mExcelWriter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mread_excel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexcel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_odswriter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mODSWriter\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_ODSWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexcel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_openpyxl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOpenpyxlWriter\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_OpenpyxlWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexcel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_util\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mregister_writer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexcel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_xlsxwriter\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXlsxWriter\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_XlsxWriter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\excel\\_base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0mpop_header_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m )\n\u001b[1;32m---> 33\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparsers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTextParser\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m _read_excel_doc = (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m from pandas.io.parsers.readers import (\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mTextFileReader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mTextParser\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mread_csv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mread_fwf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_libs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparsers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSTR_NA_VALUES\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m from pandas._typing import (\n\u001b[0m\u001b[0;32m     18\u001b[0m     \u001b[0mArrayLike\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0mDtypeArg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'DtypeArg' from 'pandas._typing' (C:\\Users\\tdcho\\anaconda3\\lib\\site-packages\\pandas\\_typing.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# pd.set_option('display.max_columns', None)\n",
    "\n",
    "import sklearn\n",
    "from sklearn import model_selection as ms\n",
    "# sklearn.set_config(print_changed_only=False)\n",
    "\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn import linear_model as lm\n",
    "\n",
    "# import cupy as cp\n",
    "# import cudf\n",
    "# import cuml\n",
    "# from cuml.preprocessing import OneHotEncoder\n",
    "# from cuml.preprocessing import MinMaxScaler\n",
    "# from cuml import train_test_split as cutts\n",
    "\n",
    "# import dask_ml\n",
    "# import dask_ml.model_selection as dcv\n",
    "# from dask_ml.preprocessing import DummyEncoder\n",
    "# from dask_ml.preprocessing import Categorizer\n",
    "# from dask_ml.preprocessing import OneHotEncoder\n",
    "# from dask_ml.preprocessing import MinMaxScaler\n",
    "\n",
    "import xgboost\n",
    "\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "\n",
    "import import_ipynb\n",
    "from _Self_Written_Functions_Sheet import rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a4c9bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# importing the datasets\n",
    "hp_train = pd.read_csv('Proj3_Datasets/hp_2a_ranked_edited_train.csv', index_col=0)\n",
    "hp_test = pd.read_csv('Proj3_Datasets/hp_2a_ranked_edited_test.csv', index_col=0)\n",
    "hp_logsp = pd.read_csv('Proj3_Datasets/hp_1a_no_imputation_logsaleprice.csv',\n",
    "                                index_col=0)\n",
    "hp_saleprice = pd.read_csv('Proj3_Datasets/hp_1a_no_imputation_saleprice.csv')\n",
    "\n",
    "# saving train index\n",
    "hp_index = hp_train.index\n",
    "\n",
    "combo_nonedit = pd.concat([hp_train, hp_test])\n",
    "combo_nonedit.columns = hp_train.columns\n",
    "\n",
    "# convert all possible numeric types to numeric\n",
    "for col in combo_nonedit.columns: \n",
    "    combo_nonedit[col] = combo_nonedit[col].astype('float', errors='ignore')\n",
    "\n",
    "combo_cols = list(combo_nonedit.columns)\n",
    "\n",
    "combo = copy.deepcopy(combo_nonedit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b498fae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions:\n",
    "\n",
    "def conditions_edit(df):\n",
    "    # combine Railroad Adjacent and Railroad Within 200 for both railroads and both\n",
    "    # condition1 and 2\n",
    "    df.loc[df['Condition1'].str.contains('RRN', na=False), 'Condition1'] = 'RRN'\n",
    "    df.loc[df['Condition1'].str.contains('RRA', na=False), 'Condition1'] = 'RRA'\n",
    "\n",
    "    df.loc[df['Condition2'].str.contains('RRN', na=False), 'Condition2'] = 'RRN'\n",
    "    df.loc[df['Condition2'].str.contains('RRA', na=False), 'Condition2'] = 'RRA'\n",
    "\n",
    "def nbrhd_rank(df):\n",
    "    # creating a dictionary of the neigborhoods and ranks based on median value\n",
    "    list_neigborhoods = list(df['Neighborhood'].unique())\n",
    "    n_dict_median = {}\n",
    "    \n",
    "    # get the median price for the neighborhoods\n",
    "    for n in list_neigborhoods:\n",
    "        n_dict_median[n] = hp_saleprice.loc[hp_train\\\n",
    "                                            [hp_train['Neighborhood']==n].index].median()[0]\n",
    "    \n",
    "    # sorting the dictionary\n",
    "    n_dict_median_sort = dict(sorted(n_dict_median.items(), key=lambda item:item[1],\n",
    "                                     reverse=True))\n",
    "    n_list_median_sort = list(n_dict_median_sort.keys())\n",
    "    n_dict_median_ranking = {}\n",
    "    \n",
    "    # assigning ranking\n",
    "    for i, n in enumerate(n_list_median_sort):\n",
    "        n_dict_median_ranking[n] = i + 1\n",
    "        \n",
    "    # creating a column replacing the neighborhood name with rank\n",
    "    df['NbMedianRank'] = df['Neighborhood'].replace(n_dict_median_ranking)\n",
    "\n",
    "def drop_nbrhd(df):\n",
    "    if 'Neighborhood' in df.columns:\n",
    "        df.drop('Neighborhood', axis=1, inplace = True)\n",
    "\n",
    "def totallivsf_add(df):\n",
    "    # creating a column with total square footage\n",
    "    df.insert(0, 'TotalLivSF', df['GrLivArea'] + df['BsmtFinSF1'] + df['BsmtFinSF2'])\n",
    "\n",
    "def drop_grlivarea(df):\n",
    "    if 'GrLivArea' in df.columns:\n",
    "        df.drop(['GrLivArea'], axis=1, inplace = True)\n",
    "\n",
    "def bsmtfin_add(df):\n",
    "    # adding the column for whether basement is finished\n",
    "    df.insert(0, 'BsmtFin', df['BsmtFinSF1'] + df['BsmtFinSF2'])\n",
    "    df.loc[df['BsmtFin'] > 0, 'BsmtFin'] = 1 \n",
    "\n",
    "def drop_bsmtfinsf12(df):\n",
    "    if set(['BsmtFinSF1', 'BsmtFinSF2']).issubset(df.columns):\n",
    "        df.drop(['BsmtFinSF1', 'BsmtFinSF2'], axis=1, inplace = True)\n",
    "\n",
    "def totalporchsf_add(df):\n",
    "    # adding the total SF of porches\n",
    "    df.insert(0, 'TotalPorchSF', df['OpenPorchSF'] + df['EnclosedPorch'] +\\\n",
    "              df['3SsnPorch'] + df['ScreenPorch'])\n",
    "\n",
    "def drop_porches(df):\n",
    "    if set(['OpenPorchSF', 'EnclosedPorch',\n",
    "            '3SsnPorch', 'ScreenPorch']).issubset(df.columns):\n",
    "        df.drop(['OpenPorchSF', 'EnclosedPorch',\n",
    "                 '3SsnPorch', 'ScreenPorch'], axis=1, inplace = True)\n",
    "\n",
    "def yrsremodtosold_add(df):\n",
    "    # calculating the time between remodeling and selling the house\n",
    "    df.insert(0, 'YrsRemodToSold', df['YrSold'] - df['YearRemodAdd'])\n",
    "\n",
    "def drop_yrsoldremod(df):\n",
    "    if set(['YrSold', 'YearRemodAdd']).issubset(df.columns):\n",
    "        df.drop(['YrSold', 'YearRemodAdd'], axis=1, inplace = True)\n",
    "\n",
    "def fireplace_yes(df):\n",
    "    # Replacing number of fireplaces with either yes (1) or no (0)\n",
    "    df.insert(0, 'Fireplace', [1 if x > 0 else 0 for x in df['Fireplaces']])\n",
    "\n",
    "def drop_fireplaces(df):\n",
    "    if 'Fireplaces' in df.columns:\n",
    "        df.drop(['Fireplaces'], axis=1, inplace = True)\n",
    "\n",
    "def pool_yes(df): \n",
    "    # Replacing pool area with either yes (1) or no (0)\n",
    "    df.insert(0, 'Pool', [1 if x > 0 else 0 for x in df['PoolArea']])\n",
    "\n",
    "def drop_poolarea(df):\n",
    "    if 'PoolArea' in df.columns:\n",
    "        df.drop(['PoolArea'], axis=1, inplace = True)\n",
    "\n",
    "def totalbaths_add(df):\n",
    "    # adding the number of bathrooms\n",
    "    df.insert(0, 'TotalBaths', df['FullBath'] + df['HalfBath'] * 0.5)\n",
    "\n",
    "def drop_baths(df):\n",
    "    if set(['FullBath', 'HalfBath']).issubset(df.columns):\n",
    "        df.drop(['FullBath', 'HalfBath'], axis=1, inplace = True)\n",
    "    \n",
    "def totalbsmtbaths_add(df):\n",
    "    # adding the number of bathrooms\n",
    "    df.insert(0, 'TotalBsmtBaths', df['BsmtFullBath'] + df['BsmtHalfBath'] * 0.5)\n",
    "    \n",
    "def drop_bsmtbaths(df):\n",
    "    if set(['BsmtFullBath', 'BsmtHalfBath']).issubset(df.columns):\n",
    "        df.drop(['BsmtFullBath', 'BsmtHalfBath'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519afe41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlr_model(x_trn, y_trn, x_tst, y_tst):\n",
    "    lm = LinearRegression()\n",
    "    # parameters to use\n",
    "    fitintercept = [True, False]\n",
    "    gparam_mlr_1 = {'fit_intercept': fitintercept}\n",
    "    # setting parameters\n",
    "    gs_mlr_1 = ms.GridSearchCV(lm, gparam_mlr_1, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                               scoring='neg_root_mean_squared_error',\n",
    "                               return_train_score=True)\n",
    "    gs_mlr_1.fit(x_trn, y_trn)\n",
    "    mlr_model.test_rmse = rmse(gs_mlr_1, y_tst, x_tst)    \n",
    "    \n",
    "def lasso_model(x_trn, y_trn, x_tst, y_tst):\n",
    "    lasso = Lasso(max_iter=1000000)\n",
    "    # parameters to use\n",
    "    alphas = [0.0001, 0.000112, 0.000124, 0.000136, 0.000148]\n",
    "    fitintercept = [True, False]\n",
    "    selec = ['cyclic', 'random']\n",
    "    gparam_lasso_1 = {'alpha': alphas,\n",
    "                      'fit_intercept':fitintercept,\n",
    "                      'selection': selec}\n",
    "    # setting parameters\n",
    "    gs_lasso_1 = ms.GridSearchCV(lasso, gparam_lasso_1, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                                 scoring='neg_root_mean_squared_error',\n",
    "                                 return_train_score=True)\n",
    "    gs_lasso_1.fit(x_trn, y_trn)\n",
    "    lasso_model.test_rmse = rmse(gs_lasso_1, y_tst, x_tst)\n",
    "\n",
    "# def ridge_model(x_trn, y_trn, x_tst, y_tst):\n",
    "#     ridge = Ridge(random_state=state, max_iter=10000, fit_intercept=True)\n",
    "#     # parameters to use\n",
    "#     solvers = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "#     fitintercept = [True, False]\n",
    "#     gparam_ridge_1 = {'solver': solvers,\n",
    "#                       'fit_intercept': fitintercept, \n",
    "#                       'random_state': [state]}\n",
    "#     # setting parameters\n",
    "#     gs_ridge_1 = ms.GridSearchCV(ridge, gparam_ridge_1, cv=n_folds, refit=True, n_jobs=-1,\n",
    "#                                  scoring='neg_root_mean_squared_error',\n",
    "#                                  return_train_score=True)\n",
    "#     gs_ridge_1.fit(x_trn, y_trn)\n",
    "#     ridge_model.test_rmse = rmse(gs_ridge_1, y_tst, x_tst)\n",
    "    \n",
    "def enet_model(x_trn, y_trn, x_tst, y_tst):\n",
    "    enet = ElasticNet(max_iter=10000000, selection='random')\n",
    "    # parameters to use\n",
    "    alphas = [1e-4, 2.5e-4, 5e-4]\n",
    "    l1ratio = [0.5, 0.7, 1]\n",
    "    precomputes = [True, False]\n",
    "    warmstart = [True, False]\n",
    "    fitintercept = [True, False]\n",
    "    gparam_enet_1 = {'alpha': alphas,\n",
    "                     'l1_ratio': l1ratio,\n",
    "                     'precompute': precomputes, \n",
    "                     'warm_start': warmstart,\n",
    "                     'fit_intercept': fitintercept}\n",
    "    # setting parameters\n",
    "    gs_enet_1 = ms.GridSearchCV(enet, gparam_enet_1, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                                scoring='neg_root_mean_squared_error', \n",
    "                                return_train_score=True)\n",
    "    gs_enet_1.fit(x_trn, y_trn)\n",
    "    enet_model.test_rmse = rmse(gs_enet_1, y_tst, x_tst)\n",
    "    \n",
    "def rfr_model(x_trn, y_trn, x_tst, y_tst):\n",
    "    rfr = RandomForestRegressor()\n",
    "    gparam_rfr = {}\n",
    "    # setting parameters\n",
    "    gs_rfr = ms.GridSearchCV(rfr, gparam_rfr, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                             scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    gs_rfr.fit(x_trn, y_trn)\n",
    "    rfr_model.test_rmse = rmse(gs_rfr, y_tst, x_tst)\n",
    "    \n",
    "def gbm_model(x_trn, y_trn, x_tst, y_tst):\n",
    "    gbm = GradientBoostingRegressor()\n",
    "    gparam_gbm = {}\n",
    "    gs_gbm = ms.GridSearchCV(gbm, gparam_gbm, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                          scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    gs_gbm.fit(x_trn, y_trn)\n",
    "    gbm_model.test_rmse = rmse(gs_gbm, y_tst, x_tst)\n",
    "    \n",
    "def xgb_model(x_trn, y_trn, x_tst, y_tst):\n",
    "    xgb_t = xgboost.XGBRegressor()\n",
    "    gparam_xgb_t = {}\n",
    "    gs_xgb_t = ms.GridSearchCV(xgb_t, gparam_xgb_t, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                            scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    gs_xgb_t.fit(x_trn, y_trn)\n",
    "    xgb_model.test_rmse = rmse(gs_xgb_t, y_tst, x_tst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bbe7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dum_scale():\n",
    "#     # dummify the variables\n",
    "#     hp_dum = DummyEncoder().fit_transform(combo, drop_first=True)\n",
    "#     enc = OneHotEncoder()\n",
    "#     for feat in ['MoSold', 'YrSold']:\n",
    "#         if feat in hp_dum.columns:\n",
    "#             # encode the feature\n",
    "#             enc.fit(hp_dum[[feat]])\n",
    "#             # create array and then dataframe of the array\n",
    "#             oh_labels = enc.transform(hp_dum[[feat]]).toarray()\n",
    "#             mssubcl_dum = cudf.DataFrame(oh_labels)\n",
    "#             mssubcl_dum.columns = enc.get_feature_names_out([feat])\n",
    "#             # Concatenate the dataframes and drop Id and original MSSubClass\n",
    "#             hp_dum = cudf.concat([hp_dum, mssubcl_dum], axis=1, sort=False)\n",
    "#             hp_dum = hp_dum.drop([feat], axis=1)\n",
    "#     hp_dum_cols = list(hp_dum.columns)\n",
    "#     for col in hp_dum_cols:\n",
    "#         hp_dum[col] = MinMaxScaler().fit_transform(cudf.array(hp_dum[col]).reshape(-1,1))\n",
    "#         hp_dum.columns = hp_dum_cols\n",
    "#     dum_scale.hp_dum = copy.deepcopy(hp_dum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888eb71f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dum_scale(df):\n",
    "    # dummify the variables\n",
    "    hp_dum = pd.get_dummies(df, drop_first=True)\n",
    "    hp_dum_cols = list(hp_dum.columns)\n",
    "    # scale the dataset\n",
    "    for col in hp_dum_cols:\n",
    "        hp_dum[col] = MinMaxScaler().fit_transform(np.array(hp_dum[col]).reshape(-1,1))\n",
    "        hp_dum.columns = hp_dum_cols\n",
    "    hp_dum_scale = copy.deepcopy(hp_dum)\n",
    "    return hp_dum_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9bacf37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_og_data(df):\n",
    "    return df\n",
    "\n",
    "def comb_list_generator(list_add, list_drop):\n",
    "    '''\n",
    "    This function generates a powerset for each list \n",
    "    and then generates all permutations of the 2 lists\n",
    "    '''\n",
    "    subset_list = []\n",
    "    comb_list = []\n",
    "    #https://stackoverflow.com/questions/464864/how-to-get-all-possible-combinations-of\n",
    "    #-a-list-s-elements\n",
    "    #https://stackoverflow.com/questions/61313027/python-executing-all-permutations-of\n",
    "    #-list-of-functions\n",
    "\n",
    "    # getting all the combinations of a list for both add and drop\n",
    "    for r_a, r_b in zip(range(len(stuff_add)+1), range(len(stuff_add)+1)):\n",
    "        com_list_a = list(itertools.combinations(stuff_add, r_a))\n",
    "        com_list_b = list(itertools.combinations(stuff_drop, r_b))\n",
    "\n",
    "        # getting all permutations of the 2 lists\n",
    "        for subset in itertools.product(com_list_a, com_list_b):\n",
    "\n",
    "            # adding the names of the functions we run thru to a list to track\n",
    "            if subset == ((), ()):\n",
    "                funcs_used_list = ['No changes to dataset']\n",
    "            else:\n",
    "                funcs_used_list = str(subset).split()[1::4]\n",
    "\n",
    "            # if the list of functions is not in the list, run the functions\n",
    "                # this is mostly to keep track of each permutation\n",
    "            if funcs_used_list not in subset_list:\n",
    "                # getting the functions used to add to dictionary\n",
    "                funcs_used = ', '.join(str(subset).split()[1::4])\n",
    "\n",
    "                # making the dict key sensible\n",
    "                if funcs_used == '())':\n",
    "                    funcs_used = 'use_og_data'\n",
    "                comb_list.append([funcs_used])\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    return comb_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3c4e19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp = copy.deepcopy(combo_nonedit)\n",
    "state = 0\n",
    "counter = 0\n",
    "\n",
    "# using standard kfold split\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "# lists of the functions separated by whether something is changed or added\n",
    "stuff_add = [nbrhd_rank, totallivsf_add, bsmtfin_add, totalporchsf_add, \n",
    "             yrsremodtosold_add, fireplace_yes, pool_yes, totalbaths_add,\n",
    "             totalbsmtbaths_add, conditions_edit]\n",
    "\n",
    "# list of functions that will drop columns\n",
    "stuff_drop = [drop_nbrhd, drop_grlivarea, drop_bsmtfinsf12,  drop_porches,\n",
    "              drop_yrsoldremod, drop_fireplaces, drop_poolarea,\n",
    "              drop_baths, drop_bsmtbaths]\n",
    "\n",
    "# list of models to use\n",
    "models = [mlr_model, lasso_model,  enet_model, rfr_model, gbm_model, xgb_model]\n",
    "\n",
    "# using self-written function to generate permutations of 2 powersets\n",
    "comb_list_generator(stuff_add, stuff_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7ca6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbers will be indices later down\n",
    "funcs_scores = {1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}}\n",
    "\n",
    "def apply_comb_list(com_list):\n",
    "    for cb_l in com_list:\n",
    "\n",
    "        hp = copy.deepcopy(combo_nonedit)\n",
    "\n",
    "        # converting any numbers to numeric type\n",
    "        for col in combo.columns: \n",
    "            combo[col] = combo[col].astype('float', errors='ignore')\n",
    "\n",
    "        # converting these 2 cols to string bc of difficulties with OneHotEncoding using Dask\n",
    "        mo_yr = ['MoSold', 'YrSold']\n",
    "        for my in mo_yr:\n",
    "            if my in combo.columns:\n",
    "                combo[[my]] = combo[[my]].astype(str)\n",
    "\n",
    "        for cb in cb_l:\n",
    "            eval(cb)(combo)\n",
    "\n",
    "#         # run dummification and scaling\n",
    "#         dum_scale(combo)\n",
    "\n",
    "        # getting the train dataset for modeling\n",
    "        hp_touse = dum_scale(combo).loc[hp_index]\n",
    "\n",
    "        # setting up train and test sets\n",
    "        xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_touse, hp_logsp,\n",
    "                                                           test_size=0.2, \n",
    "                                                           random_state=state)\n",
    "        # Must flatten to fit\n",
    "        ytrain = ytrain.values.flatten()\n",
    "\n",
    "        # run the models\n",
    "        for i, model in enumerate(models):\n",
    "            model(xtrain, ytrain, xtest, ytest)\n",
    "            # add 1 for dictionaries w/in dictionary\n",
    "            i+=1\n",
    "            # add scores to dictionary\n",
    "            funcs_scores[i][cb] = model.test_rmse\n",
    "\n",
    "        counter += 1\n",
    "        print(f'Completed round {counter}')\n",
    "    return funcs_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655f8fe6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "apply_comb_list(comb_list_generator(stuff_add, stuff_drop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208b9d67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0b95c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88411a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d97a9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53a3d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd54b34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de8df910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e043421",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aae2e07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a40f417",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d83ebd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ce928b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63d44f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c1d4dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1b777",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134dc2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67ff6ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
