{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a Function for Better Picture of Columns with Null Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import sklearn\n",
    "from sklearn import model_selection as ms\n",
    "sklearn.set_config(print_changed_only=False)\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import linear_model as lm\n",
    "\n",
    "import xgboost\n",
    "\n",
    "import copy\n",
    "\n",
    "import itertools\n",
    "\n",
    "import re\n",
    "\n",
    "import import_ipynb\n",
    "\n",
    "hp_train = pd.read_csv('..\\hp_2a_ranked_edited_train.csv', index_col=0)\n",
    "hp_saleprice = pd.read_csv('..\\hp_1a_no_imputation_saleprice.csv')\n",
    "#######################################################################################\n",
    "\n",
    "\n",
    "def nan_count(df):\n",
    "    '''\n",
    "    Outputs a df of the columns and their respective null counts\n",
    "    '''\n",
    "    # using dict to pair column names and null counts\n",
    "    null_count = {}\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if df[col].isnull().any():\n",
    "            null_count[col] = np.sum(df[col].isnull())\n",
    "    \n",
    "    # creating df using from_dict\n",
    "    null_count_df = pd.DataFrame.from_dict(null_count, orient='index', columns=['Null_Count'])\n",
    "    return null_count_df\n",
    "            \n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "def general_fillna(df):\n",
    "    '''\n",
    "    Inputs mode in place of null values based on \n",
    "    column's data type\n",
    "    '''\n",
    "    \n",
    "    col_name = list(df.columns)\n",
    "    \n",
    "    for colname in col_name:\n",
    "        value_dict = dict(df[colname].value_counts())\n",
    "        # what to do if the mode is 'None'\n",
    "        if max(value_dict, key=value_dict.get)=='None':\n",
    "            my_mode = list(value_dict.items())[1][0]\n",
    "        else:\n",
    "            my_mode = df[colname].mode()[0]\n",
    "            \n",
    "        if df[colname].dtype == 'O':\n",
    "            df.loc[df[colname].isnull(), colname]=my_mode\n",
    "        else:\n",
    "            feat_mean = df[colname].mean()\n",
    "            df.loc[df[colname].isnull(), colname]=np.mean(df[colname])\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "def my_fillna(df, null_col, cond_col=None, group=None):\n",
    "    '''\n",
    "    The function replaces the null in null_col to the mode\n",
    "    based on the assumption that a corresponding cond_col is not 0\n",
    "    \n",
    "    df == dataframe\n",
    "    null_col == the column where Null values will be imputed\n",
    "    cond_col == the column that will be used as the condition for imputation\n",
    "                (should not be the same as null_col)\n",
    "    group == the column whose values will determine the grouping of the data\n",
    "    '''\n",
    "    # Conditions:\n",
    "    '''creating dictionary'''\n",
    "    value_dict = dict(df[null_col].value_counts())\n",
    "    '''setting value of mode based on column type'''\n",
    "    if (df[null_col].dtype == 'O'):\n",
    "        '''what to do if the mode is 'None' '''\n",
    "        if max(value_dict, key=value_dict.get)=='None':\n",
    "            '''Get the next most common value'''\n",
    "            my_mode = list(value_dict.items())[1][0]\n",
    "        else:\n",
    "            my_mode = df[null_col].mode()[0]\n",
    "    else:\n",
    "        my_mode = df[null_col].median()\n",
    "    \n",
    "    if group is not None:\n",
    "        '''get the list of values in the feature for grouping'''\n",
    "        group_list = list(df[group].unique())\n",
    "        n_dict = {}\n",
    "        if df[null_col].dtype == 'O':\n",
    "            '''creating a dictionary {feature value: mode}'''\n",
    "            for n in group_list:\n",
    "                n_dict[n] = df[null_col].loc[df[df[group]==n].index].mode()[0]\n",
    "        else:\n",
    "            '''creating a dictionary {feature value: median}'''\n",
    "            for n in group_list:\n",
    "                n_dict[n] = df[null_col].loc[df[df[group]==n].index].median()\n",
    "    # Outputs:\n",
    "    '''if both cond_col and group are unspecified'''\n",
    "    if (cond_col is None) & (group is None):\n",
    "        df.loc[df[null_col].isnull(), null_col]=my_mode\n",
    "        \n",
    "    elif (cond_col is None) & (group is not None):\n",
    "        '''replace null values in LotFrontage with the median values based on neighborhood'''\n",
    "        df.loc[df[null_col].isnull(), null_col] = df[group].map(n_dict)\n",
    "    \n",
    "    elif (cond_col is not None) & (group is None):\n",
    "        '''if null_col is not None, then fill the corresponding null_col with mode'''\n",
    "        df.loc[((df[null_col].isnull()) &\\\n",
    "                ((df[cond_col] != 0) & (df[cond_col] != 1) & (df[cond_col].notnull()))),\n",
    "               null_col] = my_mode\n",
    "    else:\n",
    "        '''dictionary to hold the mode for each group'''\n",
    "        n_dict = {}\n",
    "        if df[null_col].dtype == 'O':\n",
    "            '''creating a dictionary {feature value: mode}'''\n",
    "            for n in group_list:\n",
    "                value_dict = dict(df[null_col].loc[df[df[group]==n].index].value_counts())\n",
    "                '''what to do if the mode is 'None' and there are no other values'''\n",
    "                if (max(value_dict, key=value_dict.get)=='None') &\\\n",
    "                (min(value_dict, key=value_dict.get)=='None'):\n",
    "                    '''will use the mode for the entire column instead'''\n",
    "                    value_dict = dict(df[null_col].value_counts())\n",
    "                    '''if the mode for the entire column is also 'None' '''\n",
    "                    if max(value_dict, key=value_dict.get)=='None':\n",
    "                        '''will use the next most common value'''\n",
    "                        my_mode = list(value_dict.items())[1][0]\n",
    "                    else:\n",
    "                        my_mode = df[null_col].mode()[0]\n",
    "                    n_dict[n] = my_mode\n",
    "                '''what to do if the mode is 'None' '''\n",
    "                if max(value_dict, key=value_dict.get)=='None':\n",
    "                    my_mode = list(value_dict.items())[1][0]\n",
    "                    n_dict[n] = my_mode\n",
    "                else:\n",
    "                    n_dict[n] = df[null_col].loc[df[df[group]==n].index].mode()[0]\n",
    "        else:\n",
    "            for n in group_list:\n",
    "                n_dict[n] = df[null_col].loc[df[df[group]==n].index].median()  \n",
    "        '''replace null values in LotFrontage with the median values based on neighborhood'''\n",
    "        df.loc[((df[null_col].isnull()) &\\\n",
    "                ((df[cond_col] != 1) & (df[cond_col] != 0) & (df[cond_col].notnull()))),\n",
    "               null_col] = df[group].map(n_dict)\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "class Dataset_Editor():\n",
    "    \n",
    "    def __init__(self, conditions_edit, nbrhd_rank)\n",
    "    \n",
    "'''\n",
    "the following functions are used for feature engineering and dropping columns\n",
    "'''\n",
    "\n",
    "def conditions_edit(df):\n",
    "    # combine Railroad Adjacent and Railroad Within 200 for both railroads and both\n",
    "    # condition1 and 2\n",
    "    df.loc[df['Condition1'].str.contains('RRN', na=False), 'Condition1'] = 'RRN'\n",
    "    df.loc[df['Condition1'].str.contains('RRA', na=False), 'Condition1'] = 'RRA'\n",
    "\n",
    "    df.loc[df['Condition2'].str.contains('RRN', na=False), 'Condition2'] = 'RRN'\n",
    "    df.loc[df['Condition2'].str.contains('RRA', na=False), 'Condition2'] = 'RRA'\n",
    "\n",
    "def nbrhd_rank(df):\n",
    "    # creating a dictionary of the neigborhoods and ranks based on median value\n",
    "    list_neigborhoods = list(df['Neighborhood'].unique())\n",
    "    n_dict_median = {}\n",
    "    \n",
    "    # get the median price for the neighborhoods\n",
    "    for n in list_neigborhoods:\n",
    "        n_dict_median[n] = hp_saleprice.loc[hp_train\\\n",
    "                                            [hp_train['Neighborhood']==n].index].median()[0]\n",
    "    \n",
    "    # sorting the dictionary\n",
    "    n_dict_median_sort = dict(sorted(n_dict_median.items(), key=lambda item:item[1],\n",
    "                                     reverse=True))\n",
    "    n_list_median_sort = list(n_dict_median_sort.keys())\n",
    "    n_dict_median_ranking = {}\n",
    "    \n",
    "    # assigning ranking\n",
    "    for i, n in enumerate(n_list_median_sort):\n",
    "        n_dict_median_ranking[n] = i + 1\n",
    "        \n",
    "    # creating a column replacing the neighborhood name with rank\n",
    "    df['NbMedianRank'] = df['Neighborhood'].replace(n_dict_median_ranking)\n",
    "\n",
    "def drop_nbrhd(df):\n",
    "    if 'Neighborhood' in df.columns:\n",
    "        df.drop('Neighborhood', axis=1, inplace = True)\n",
    "\n",
    "def totallivsf_add(df):\n",
    "    # creating a column with total square footage\n",
    "    df.insert(0, 'TotalLivSF', df['GrLivArea'] + df['BsmtFinSF1'] + df['BsmtFinSF2'])\n",
    "\n",
    "def drop_grlivarea(df):\n",
    "    if 'GrLivArea' in df.columns:\n",
    "        df.drop(['GrLivArea'], axis=1, inplace = True)\n",
    "\n",
    "def bsmtfin_add(df):\n",
    "    # adding the column for whether basement is finished\n",
    "    df.insert(0, 'BsmtFin', df['BsmtFinSF1'] + df['BsmtFinSF2'])\n",
    "    df.loc[df['BsmtFin'] > 0, 'BsmtFin'] = 1 \n",
    "\n",
    "def drop_bsmtfinsf12(df):\n",
    "    if set(['BsmtFinSF1', 'BsmtFinSF2']).issubset(df.columns):\n",
    "        df.drop(['BsmtFinSF1', 'BsmtFinSF2'], axis=1, inplace = True)\n",
    "\n",
    "def totalporchsf_add(df):\n",
    "    # adding the total SF of porches\n",
    "    df.insert(0, 'TotalPorchSF', df['OpenPorchSF'] + df['EnclosedPorch'] +\\\n",
    "              df['3SsnPorch'] + df['ScreenPorch'])\n",
    "\n",
    "def drop_porches(df):\n",
    "    if set(['OpenPorchSF', 'EnclosedPorch',\n",
    "            '3SsnPorch', 'ScreenPorch']).issubset(df.columns):\n",
    "        df.drop(['OpenPorchSF', 'EnclosedPorch',\n",
    "                 '3SsnPorch', 'ScreenPorch'], axis=1, inplace = True)\n",
    "\n",
    "def yrsremodtosold_add(df):\n",
    "    # calculating the time between remodeling and selling the house\n",
    "    df.insert(0, 'YrsRemodToSold', df['YrSold'] - df['YearRemodAdd'])\n",
    "\n",
    "def drop_yrsoldremod(df):\n",
    "    if set(['YrSold', 'YearRemodAdd']).issubset(df.columns):\n",
    "        df.drop(['YrSold', 'YearRemodAdd'], axis=1, inplace = True)\n",
    "\n",
    "def fireplace_yes(df):\n",
    "    # Replacing number of fireplaces with either yes (1) or no (0)\n",
    "    df.insert(0, 'Fireplace', [1 if x > 0 else 0 for x in df['Fireplaces']])\n",
    "\n",
    "def drop_fireplaces(df):\n",
    "    if 'Fireplaces' in df.columns:\n",
    "        df.drop(['Fireplaces'], axis=1, inplace = True)\n",
    "\n",
    "def pool_yes(df): \n",
    "    # Replacing pool area with either yes (1) or no (0)\n",
    "    df.insert(0, 'Pool', [1 if x > 0 else 0 for x in df['PoolArea']])\n",
    "\n",
    "def drop_poolarea(df):\n",
    "    if 'PoolArea' in df.columns:\n",
    "        df.drop(['PoolArea'], axis=1, inplace = True)\n",
    "\n",
    "def totalbaths_add(df):\n",
    "    # adding the number of bathrooms\n",
    "    df.insert(0, 'TotalBaths', df['FullBath'] + df['HalfBath'] * 0.5)\n",
    "\n",
    "def drop_baths(df):\n",
    "    if set(['FullBath', 'HalfBath']).issubset(df.columns):\n",
    "        df.drop(['FullBath', 'HalfBath'], axis=1, inplace = True)\n",
    "    \n",
    "def totalbsmtbaths_add(df):\n",
    "    # adding the number of bathrooms\n",
    "    df.insert(0, 'TotalBsmtBaths', df['BsmtFullBath'] + df['BsmtHalfBath'] * 0.5)\n",
    "    \n",
    "def drop_bsmtbaths(df):\n",
    "    if set(['BsmtFullBath', 'BsmtHalfBath']).issubset(df.columns):\n",
    "        df.drop(['BsmtFullBath', 'BsmtHalfBath'], axis=1, inplace = True)\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "def dum_scale(df):\n",
    "    '''\n",
    "    dummify and scale the input dataset\n",
    "    '''\n",
    "    # dummify the variables\n",
    "    hp_dum = pd.get_dummies(df, drop_first=True)\n",
    "    hp_dum_cols = list(hp_dum.columns)\n",
    "    # scale the dataset\n",
    "    for col in hp_dum_cols:\n",
    "        hp_dum[col] = MinMaxScaler().fit_transform(np.array(hp_dum[col]).reshape(-1,1))\n",
    "        hp_dum.columns = hp_dum_cols\n",
    "    hp_dum_scale = copy.deepcopy(hp_dum)\n",
    "    return hp_dum_scale\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "def mlr_model(x_trn, y_trn, x_tst, y_tst):\n",
    "    lm = LinearRegression()\n",
    "    # parameters to use\n",
    "    fitintercept = [True, False]\n",
    "    gparam_mlr_1 = {'fit_intercept': fitintercept}\n",
    "    # setting parameters\n",
    "    gs_mlr_1 = ms.GridSearchCV(lm, gparam_mlr_1, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                               scoring='neg_root_mean_squared_error',\n",
    "                               return_train_score=True)\n",
    "    gs_mlr_1.fit(x_trn, y_trn)\n",
    "    mlr_model.test_rmse = rmse(gs_mlr_1, y_tst, x_tst)    \n",
    "    \n",
    "def lasso_model(x_trn, y_trn, x_tst, y_tst):\n",
    "    lasso = Lasso(max_iter=1000000)\n",
    "    # parameters to use\n",
    "    alphas = [0.0001, 0.000112, 0.000124, 0.000136, 0.000148]\n",
    "    fitintercept = [True, False]\n",
    "    selec = ['cyclic', 'random']\n",
    "    gparam_lasso_1 = {'alpha': alphas,\n",
    "                      'fit_intercept':fitintercept,\n",
    "                      'selection': selec}\n",
    "    # setting parameters\n",
    "    gs_lasso_1 = ms.GridSearchCV(lasso, gparam_lasso_1, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                                 scoring='neg_root_mean_squared_error',\n",
    "                                 return_train_score=True)\n",
    "    gs_lasso_1.fit(x_trn, y_trn)\n",
    "    lasso_model.test_rmse = rmse(gs_lasso_1, y_tst, x_tst)\n",
    "\n",
    "def ridge_model(x_trn, y_trn, x_tst, y_tst):\n",
    "    ridge = Ridge(random_state=state, max_iter=10000, fit_intercept=True)\n",
    "    # parameters to use\n",
    "    solvers = ['auto', 'svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga', 'lbfgs']\n",
    "    fitintercept = [True, False]\n",
    "    gparam_ridge_1 = {'solver': solvers,\n",
    "                      'fit_intercept': fitintercept, \n",
    "                      'random_state': [state]}\n",
    "    # setting parameters\n",
    "    gs_ridge_1 = ms.GridSearchCV(ridge, gparam_ridge_1, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                                 scoring='neg_root_mean_squared_error',\n",
    "                                 return_train_score=True)\n",
    "    gs_ridge_1.fit(x_trn, y_trn)\n",
    "    ridge_model.test_rmse = rmse(gs_ridge_1, y_tst, x_tst)\n",
    "    \n",
    "def enet_model(x_trn, y_trn, x_tst, y_tst):\n",
    "    enet = ElasticNet(max_iter=10000000, selection='random')\n",
    "    # parameters to use\n",
    "    alphas = [1e-4, 2.5e-4, 5e-4]\n",
    "    l1ratio = [0.5, 0.7, 1]\n",
    "    precomputes = [True, False]\n",
    "    warmstart = [True, False]\n",
    "    fitintercept = [True, False]\n",
    "    gparam_enet_1 = {'alpha': alphas,\n",
    "                     'l1_ratio': l1ratio,\n",
    "                     'precompute': precomputes, \n",
    "                     'warm_start': warmstart,\n",
    "                     'fit_intercept': fitintercept}\n",
    "    # setting parameters\n",
    "    gs_enet_1 = ms.GridSearchCV(enet, gparam_enet_1, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                                scoring='neg_root_mean_squared_error', \n",
    "                                return_train_score=True)\n",
    "    gs_enet_1.fit(x_trn, y_trn)\n",
    "    enet_model.test_rmse = rmse(gs_enet_1, y_tst, x_tst)\n",
    "    \n",
    "def rfr_model(x_trn, y_trn, x_tst, y_tst):\n",
    "    rfr = RandomForestRegressor()\n",
    "    gparam_rfr = {}\n",
    "    # setting parameters\n",
    "    gs_rfr = ms.GridSearchCV(rfr, gparam_rfr, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                             scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    gs_rfr.fit(x_trn, y_trn)\n",
    "    rfr_model.test_rmse = rmse(gs_rfr, y_tst, x_tst)\n",
    "    \n",
    "def gbm_model(x_trn, y_trn, x_tst, y_tst):\n",
    "    gbm = GradientBoostingRegressor()\n",
    "    gparam_gbm = {}\n",
    "    gs_gbm = ms.GridSearchCV(gbm, gparam_gbm, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                          scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    gs_gbm.fit(x_trn, y_trn)\n",
    "    gbm_model.test_rmse = rmse(gs_gbm, y_tst, x_tst)\n",
    "    \n",
    "def xgb_model(x_trn, y_trn, x_tst, y_tst):\n",
    "    xgb_t = xgboost.XGBRegressor()\n",
    "    gparam_xgb_t = {}\n",
    "    gs_xgb_t = ms.GridSearchCV(xgb_t, gparam_xgb_t, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                            scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    gs_xgb_t.fit(x_trn, y_trn)\n",
    "    xgb_model.test_rmse = rmse(gs_xgb_t, y_tst, x_tst)\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "def use_og_data(df):\n",
    "    '''\n",
    "    this is a placeholder function for the function below.\n",
    "    It aims to use the name of the function for a dict key\n",
    "    '''\n",
    "    return df\n",
    "\n",
    "def comb_list_generator(list_add, list_drop):\n",
    "    '''\n",
    "    This function generates a powerset for each list \n",
    "    and then generates all permutations of the 2 lists\n",
    "    '''\n",
    "    subset_list = []\n",
    "    comb_list = []\n",
    "    #https://stackoverflow.com/questions/464864/how-to-get-all-possible-combinations-of\n",
    "    #-a-list-s-elements\n",
    "    #https://stackoverflow.com/questions/61313027/python-executing-all-permutations-of\n",
    "    #-list-of-functions\n",
    "\n",
    "    # getting all the combinations of a list for both add and drop\n",
    "    for r_a, r_b in zip(range(len(list_add)+1), range(len(list_add)+1)):\n",
    "        com_list_a = list(itertools.combinations(list_add, r_a))\n",
    "        com_list_b = list(itertools.combinations(list_drop, r_b))\n",
    "\n",
    "        # getting all permutations of the 2 lists\n",
    "        for subset in itertools.product(com_list_a, com_list_b):\n",
    "\n",
    "            # adding the names of the functions we run thru to a list to track\n",
    "            if subset == ((), ()):\n",
    "                funcs_used_list = ['No changes to dataset']\n",
    "            else:\n",
    "                funcs_used_list = str(subset).split()[1::4]\n",
    "\n",
    "            # if the list of functions is not in the list, run the functions\n",
    "                # this is mostly to keep track of each permutation\n",
    "            if funcs_used_list not in subset_list:\n",
    "                # getting the functions used to add to dictionary\n",
    "                funcs_used = ', '.join(str(subset).split()[1::4])\n",
    "\n",
    "                # making the dict key sensible\n",
    "                if funcs_used == '())':\n",
    "                    funcs_used = 'use_og_data'\n",
    "                comb_list.append([funcs_used])\n",
    "            else:\n",
    "                continue\n",
    "                \n",
    "    return comb_list\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "def apply_comb_list(com_list, df_nonedit, target, model_list, df_index):\n",
    "    '''\n",
    "    This function utilizes the comb_list_generator func to apply \n",
    "    the specified functions in the permutation of the powerlists\n",
    "    to the dataset. This function will then run a list of specified\n",
    "    regression models and add the scores as values to a key of\n",
    "    whichever feature engineering functions were used. Regression\n",
    "    models will be tracked via their position in the model_list\n",
    "    that is passed into the function\n",
    "    '''\n",
    "    state = 0\n",
    "    counter = 0\n",
    "    \n",
    "    # numbers will be indices later down\n",
    "    funcs_scores = {1: {}, 2: {}, 3: {}, 4: {}, 5: {}, 6: {}}\n",
    "    \n",
    "    for cb_list in com_list:\n",
    "\n",
    "        df = copy.deepcopy(df_nonedit)\n",
    "\n",
    "        # converting any numbers to numeric type\n",
    "        for col in df.columns: \n",
    "            df[col] = df[col].astype('float', errors='ignore')\n",
    "\n",
    "        # converting these 2 cols to string bc difficulties wi/ OneHotEncoding using Dask\n",
    "        mo_yr = ['MoSold', 'YrSold']\n",
    "        for my in mo_yr:\n",
    "            if my in df.columns:\n",
    "                df[[my]] = df[[my]].astype(str)\n",
    "        \n",
    "        # converting the single string within the list to a list of strings\n",
    "        cb_split = re.sub(\"['']\", \"\", str(cb_list)).strip('][').split(', ')\n",
    "        # converting the list of strings to a single string\n",
    "        funcs_used = ', '.join(cb_split)\n",
    "        \n",
    "        # calling all of the functions within the list\n",
    "        for cb in cb_split:\n",
    "            eval(cb)(df)\n",
    "        \n",
    "        # getting the train dataset for modeling after dummification\n",
    "        hp_touse = dum_scale(df).loc[df_index]\n",
    "\n",
    "        # setting up train and test sets\n",
    "        xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_touse, target,\n",
    "                                                           test_size=0.2, \n",
    "                                                           random_state=state)\n",
    "        # Must flatten to fit\n",
    "        ytrain = ytrain.values.flatten()\n",
    "\n",
    "        # run the models\n",
    "        for i, model in enumerate(model_list):\n",
    "            model(xtrain, ytrain, xtest, ytest)\n",
    "            # each model has a designated number (the order in the list: models)\n",
    "            i+=1\n",
    "            # add scores to the dictionary of dictionaries\n",
    "            funcs_scores[i][funcs_used] = model.test_rmse\n",
    "        \n",
    "        # keeping track of rounds\n",
    "        counter += 1\n",
    "#         if counter % 5 == 0:\n",
    "        print(f'Completed round {counter}')\n",
    "    return funcs_scores\n",
    "\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "def rmse(model, actual, features):\n",
    "    '''\n",
    "    Calculates the root mean squared error using sklearn's mean_squared_error\n",
    "    '''\n",
    "    return np.sqrt(mean_squared_error(actual, model.best_estimator_.predict(features)))\n",
    "\n",
    "#########################################################################################\n",
    "\n",
    "\n",
    "\n",
    "# conversion = [\n",
    "#     # MSZoning\n",
    "#     {'A': 1, 'C': 2, 'FV': 3, 'I': 4, 'RH': 5, 'RL': 6, 'RP': 7, 'RM': 8}, \n",
    "    \n",
    "#     # Street\n",
    "#     {'Grvl': 1, 'Pave': 2},\n",
    "    \n",
    "#     # Alley\n",
    "#     {'Grvl': 1, 'Pave': 2, 'DNE': 0},\n",
    "    \n",
    "#     # LotShape\n",
    "#     {'Reg': 1, 'IR1': 2, 'IR2': 3, 'IR3': 4}, \n",
    "    \n",
    "#     # LandContour\n",
    "#     {'Lvl': 1, 'Bnk': 2, 'HLS': 3, 'Low': 4},\n",
    "    \n",
    "#     # Utilities\n",
    "#     {'AllPub': 1, 'NoSewr': 2, 'NoSeWa': 3, 'ELO': 4},\n",
    "    \n",
    "#     # LotConfig\n",
    "#     {'Inside': 1, 'Corner': 2, 'CulDSac': 3, 'FR2': 4, 'FR3': 5}, \n",
    "    \n",
    "#     # LandSlope\n",
    "#     {'Gtl': 1, 'Mod': 2, 'Sev': 3},\n",
    "    \n",
    "#     # Neighborhood\n",
    "#     {'Blmngtn': 1, 'Blueste': 2, 'BrDale': 3, 'BrkSide': 4, 'ClearCr': 5, 'CollgCr': 6,\n",
    "#      'Crawfor': 7, 'Edwards': 8, 'Gilbert': 9, 'IDOTRR': 10, 'MeadowV': 11, 'Mitchel': 12,\n",
    "#      'NAmes': 13, 'NoRidge': 14, 'NPkVill': 15, 'NridgHt': 16, 'NWAmes': 17, 'OldTown': 18,\n",
    "#      'SWISU': 19, 'Sawyer': 20, 'SawyerW': 21, 'Somerst': 22, 'StoneBr': 23, 'Timber': 24,\n",
    "#      'Veenker': 25},\n",
    "    \n",
    "#     # Condition1\n",
    "#     {'Artery': 1, 'Feedr': 2, 'Norm': 3, 'RRNn': 4, 'RRAn': 5,\n",
    "#      'PosN': 6, 'PosA': 7, 'RRNe': 8, 'RRAe': 9},\n",
    "    \n",
    "#     # Condition2\n",
    "#     {'Artery': 1, 'Feedr': 2, 'Norm': 3, 'RRNn': 4, 'RRAn': 5,\n",
    "#      'PosN': 6, 'PosA': 7, 'RRNe': 8, 'RRAe': 9},\n",
    "    \n",
    "#     # BldgType\n",
    "#     {'1Fam': 1, '2FmCon': 2, 'Duplx': 3, 'TwnhsE': 4, 'TwnhsI': 5},\n",
    "    \n",
    "#     # HouseStyle\n",
    "#     {'1Story': 1, '1.5Fin': 2, '1.5Unf': 3, '2Story': 4, '2.5Fin': 5, '2.5Unf': 6, 'SFoyer': 7,\n",
    "#      'SLvl': 8},\n",
    "    \n",
    "#     # RoofStyle \n",
    "#     {'Flat': 1, 'Gable': 2, 'Gambrel': 3, 'Hip': 4, 'Mansard': 5, 'Shed' :6},\n",
    "    \n",
    "#     # RoofMatl\n",
    "#     {'ClyTile': 1, 'CompShg': 2, 'Membran': 3, 'Metal': 4, 'Roll': 5, 'Tar&Grv': 6, 'WdShake': 7,\n",
    "#      'WdShngl': 8},\n",
    "    \n",
    "#     # Exterior1st\n",
    "#     {'AsbShng': 1, 'AsphShn': 2, 'BrkComm': 3, 'BrkFace': 4, 'CBlock': 5, 'CemntBd': 6, 'HdBoard': 7,\n",
    "#      'ImStucc': 8, 'MetalSd': 9, 'Other': 10, 'Plywood': 11, 'PreCast': 12, 'Stone': 13, 'Stucco': 14,\n",
    "#      'VinylSd': 15, 'Wd Sdng': 16, 'WdShing': 17},\n",
    "    \n",
    "#     # Exterior2nd\n",
    "#     {'AsbShng': 1, 'AsphShn': 2, 'BrkComm': 3, 'BrkFace': 4, 'CBlock': 5, 'CemntBd': 6, 'HdBoard': 7,\n",
    "#      'ImStucc': 8, 'MetalSd': 9, 'Other': 10, 'Plywood': 11, 'PreCast': 12, 'Stone': 13, 'Stucco': 14,\n",
    "#      'VinylSd': 15, 'Wd Sdng': 16, 'WdShing': 17},\n",
    "    \n",
    "#     # MasVnrType\n",
    "#     {'BrkCmn': 1, 'BrkFace': 2, 'CBlock': 3, 'None': 4, 'Stone': 5, 'DNE': 0},\n",
    "    \n",
    "#     # ExterQual\n",
    "#     {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4, 'Po': 5},\n",
    "    \n",
    "#     # ExterCond\n",
    "#     {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4, 'Po': 5},\n",
    "    \n",
    "#     # Foundation\n",
    "#     {'BrkTil': 1, 'CBlock': 2, 'PConc': 3, 'Slab': 4, 'Stone': 5, 'Wood': 6},\n",
    "    \n",
    "#     # BsmtQual\n",
    "#     {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4, 'Po': 5, 'DNE': 0},\n",
    "    \n",
    "#     # BsmtCond\n",
    "#     {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4, 'Po': 5, 'DNE': 0},\n",
    "    \n",
    "#     # BsmtExposure\n",
    "#     {'Gd': 1, 'Av': 2, 'Mn': 3, 'No': 4,'DNE': 0},\n",
    "    \n",
    "#     # BsmtFinType1\n",
    "#     {'GLQ': 1, 'ALQ': 2, 'BLQ': 3, 'Rec': 4, 'LwQ': 5, 'Unf': 6, 'DNE': 0},\n",
    "    \n",
    "#     # BsmtFinType2\n",
    "#     {'GLQ': 1, 'ALQ': 2, 'BLQ': 3, 'Rec': 4, 'LwQ': 5, 'Unf': 6, 'DNE': 0},\n",
    "    \n",
    "#     # Heating\n",
    "#     {'Floor': 1, 'GasA': 2, 'GasW': 3, 'Grav': 4, 'OthW': 5, 'Wall': 6},\n",
    "    \n",
    "#     # HeatingQC\n",
    "#     {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4, 'Po': 5},\n",
    "    \n",
    "#     # CentralAir\n",
    "#     {'N': 0, 'Y': 1},\n",
    "    \n",
    "#     # Electrical\n",
    "#     {'SBrkr': 1, 'FuseA': 2, 'FuseF': 3, 'FuseP': 4, 'Mix': 5, 'DNE': 0},\n",
    "    \n",
    "#     # KitchenQual\n",
    "#     {'Ex': 1,'Gd': 2,'TA': 3,'Fa': 4,'Po': 5},\n",
    "    \n",
    "#     # Functional \n",
    "#     {'Typ': 1, 'Min1': 2, 'Min2': 3, 'Mod': 4, 'Maj1': 5, 'Maj2': 6, 'Sev': 7, 'Sal': 8}, \n",
    "    \n",
    "#     # FireplaceQU\n",
    "#     {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4, 'Po': 5, 'DNE': 0},\n",
    "    \n",
    "#     # GarageType\n",
    "#     {'2Types': 1, 'Attchd': 2, 'Basment': 3, 'BuiltIn': 4, 'CarPort': 5, 'Detchd': 6, 'DNE': 0}, \n",
    "    \n",
    "#     # GarageFinish\n",
    "#     {'Fin': 1, 'RFn': 2, 'Unf': 3, 'DNE': 4},\n",
    "    \n",
    "#     # GarageQual\n",
    "#     {'Ex': 1,'Gd': 2,'TA': 3,'Fa': 4,'Po': 5, 'DNE':0},\n",
    "    \n",
    "#     # GarageCond\n",
    "#     {'Ex': 1,'Gd': 2,'TA': 3,'Fa': 4,'Po': 5, 'DNE':0},\n",
    "    \n",
    "#     # PavedDrive\n",
    "#     {'Y': 1, 'P': 2, 'N': 0}, \n",
    "    \n",
    "#     # PoolQC\n",
    "#     {'Ex': 1, 'Gd': 2, 'TA': 3, 'Fa': 4, 'DNE': 0},\n",
    "    \n",
    "#     # Fence\n",
    "#     {'GdPrv': 1, 'MnPrv': 2, 'GdWo': 3, 'MnWw': 4, 'DNE': 0},\n",
    "\n",
    "#     # MiscFeature\n",
    "#     {'Elev': 1, 'Gar2': 2, 'Othr': 3, 'Shed': 4, 'TenC': 5, 'DNE': 0},\n",
    "    \n",
    "#     # SaleType\n",
    "#     {'WD': 1, 'CWD': 2, 'VWD': 3, 'New': 4, 'COD': 5, 'Con': 6, 'ConLw': 7,\n",
    "#      'ConLI': 8, 'ConLD': 9, 'Oth': 10},\n",
    "    \n",
    "#     # SaleCondition\n",
    "#     {'Normal': 1, 'Abnorml': 2, 'AdjLand': 3, 'Alloca': 4, 'Family': 5, 'Partial': 6}]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
