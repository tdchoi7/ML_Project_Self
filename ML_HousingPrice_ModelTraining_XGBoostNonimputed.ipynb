{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost Using Nonimputed Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from Self_Written_Functions_Sheet.ipynb\n",
      "importing Jupyter notebook from ML_HousingPrice_EDA_and_Basic_Imputation.ipynb\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     1452 non-null   object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1460 entries, 0 to 1459\n",
      "Data columns (total 81 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   Id             1460 non-null   int64  \n",
      " 1   MSSubClass     1460 non-null   int64  \n",
      " 2   MSZoning       1460 non-null   object \n",
      " 3   LotFrontage    1201 non-null   float64\n",
      " 4   LotArea        1460 non-null   int64  \n",
      " 5   Street         1460 non-null   object \n",
      " 6   Alley          91 non-null     object \n",
      " 7   LotShape       1460 non-null   object \n",
      " 8   LandContour    1460 non-null   object \n",
      " 9   Utilities      1460 non-null   object \n",
      " 10  LotConfig      1460 non-null   object \n",
      " 11  LandSlope      1460 non-null   object \n",
      " 12  Neighborhood   1460 non-null   object \n",
      " 13  Condition1     1460 non-null   object \n",
      " 14  Condition2     1460 non-null   object \n",
      " 15  BldgType       1460 non-null   object \n",
      " 16  HouseStyle     1460 non-null   object \n",
      " 17  OverallQual    1460 non-null   int64  \n",
      " 18  OverallCond    1460 non-null   int64  \n",
      " 19  YearBuilt      1460 non-null   int64  \n",
      " 20  YearRemodAdd   1460 non-null   int64  \n",
      " 21  RoofStyle      1460 non-null   object \n",
      " 22  RoofMatl       1460 non-null   object \n",
      " 23  Exterior1st    1460 non-null   object \n",
      " 24  Exterior2nd    1460 non-null   object \n",
      " 25  MasVnrType     1452 non-null   object \n",
      " 26  MasVnrArea     1452 non-null   float64\n",
      " 27  ExterQual      1460 non-null   object \n",
      " 28  ExterCond      1460 non-null   object \n",
      " 29  Foundation     1460 non-null   object \n",
      " 30  BsmtQual       1423 non-null   object \n",
      " 31  BsmtCond       1423 non-null   object \n",
      " 32  BsmtExposure   1422 non-null   object \n",
      " 33  BsmtFinType1   1423 non-null   object \n",
      " 34  BsmtFinSF1     1460 non-null   int64  \n",
      " 35  BsmtFinType2   1422 non-null   object \n",
      " 36  BsmtFinSF2     1460 non-null   int64  \n",
      " 37  BsmtUnfSF      1460 non-null   int64  \n",
      " 38  TotalBsmtSF    1460 non-null   int64  \n",
      " 39  Heating        1460 non-null   object \n",
      " 40  HeatingQC      1460 non-null   object \n",
      " 41  CentralAir     1460 non-null   object \n",
      " 42  Electrical     1459 non-null   object \n",
      " 43  1stFlrSF       1460 non-null   int64  \n",
      " 44  2ndFlrSF       1460 non-null   int64  \n",
      " 45  LowQualFinSF   1460 non-null   int64  \n",
      " 46  GrLivArea      1460 non-null   int64  \n",
      " 47  BsmtFullBath   1460 non-null   int64  \n",
      " 48  BsmtHalfBath   1460 non-null   int64  \n",
      " 49  FullBath       1460 non-null   int64  \n",
      " 50  HalfBath       1460 non-null   int64  \n",
      " 51  BedroomAbvGr   1460 non-null   int64  \n",
      " 52  KitchenAbvGr   1460 non-null   int64  \n",
      " 53  KitchenQual    1460 non-null   object \n",
      " 54  TotRmsAbvGrd   1460 non-null   int64  \n",
      " 55  Functional     1460 non-null   object \n",
      " 56  Fireplaces     1460 non-null   int64  \n",
      " 57  FireplaceQu    770 non-null    object \n",
      " 58  GarageType     1379 non-null   object \n",
      " 59  GarageYrBlt    1379 non-null   float64\n",
      " 60  GarageFinish   1379 non-null   object \n",
      " 61  GarageCars     1460 non-null   int64  \n",
      " 62  GarageArea     1460 non-null   int64  \n",
      " 63  GarageQual     1379 non-null   object \n",
      " 64  GarageCond     1379 non-null   object \n",
      " 65  PavedDrive     1460 non-null   object \n",
      " 66  WoodDeckSF     1460 non-null   int64  \n",
      " 67  OpenPorchSF    1460 non-null   int64  \n",
      " 68  EnclosedPorch  1460 non-null   int64  \n",
      " 69  3SsnPorch      1460 non-null   int64  \n",
      " 70  ScreenPorch    1460 non-null   int64  \n",
      " 71  PoolArea       1460 non-null   int64  \n",
      " 72  PoolQC         7 non-null      object \n",
      " 73  Fence          281 non-null    object \n",
      " 74  MiscFeature    54 non-null     object \n",
      " 75  MiscVal        1460 non-null   int64  \n",
      " 76  MoSold         1460 non-null   int64  \n",
      " 77  YrSold         1460 non-null   int64  \n",
      " 78  SaleType       1460 non-null   object \n",
      " 79  SaleCondition  1460 non-null   object \n",
      " 80  SalePrice      1460 non-null   int64  \n",
      "dtypes: float64(3), int64(35), object(43)\n",
      "memory usage: 924.0+ KB\n",
      "['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n",
      "\n",
      "\n",
      "['Id', 'MSSubClass', 'LotFrontage', 'LotArea', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces', 'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold', 'YrSold', 'SalePrice']\n",
      "['MSZoning', 'Street', 'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'PoolQC', 'Fence', 'MiscFeature', 'SaleType', 'SaleCondition']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "8\n",
      "259.38169642857144\n",
      "203.5\n",
      "0    120.0\n",
      "1    180.0\n",
      "dtype: float64\n",
      "[0, 1]\n",
      "150.0\n",
      "259.71651785714283\n",
      "203.5\n",
      "0    120.0\n",
      "1    180.0\n",
      "dtype: float64\n",
      "240.234375\n",
      "206.5\n",
      "0     50.0\n",
      "1     72.0\n",
      "2    100.0\n",
      "3    106.0\n",
      "4    186.0\n",
      "5    200.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 792x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.feature_selection import chi2, f_regression, f_classif, mutual_info_classif, mutual_info_regression \n",
    "from sklearn.feature_selection import SelectKBest, SelectFromModel, VarianceThreshold\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import linear_model as lm\n",
    "from sklearn import neighbors\n",
    "from sklearn import ensemble\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn import model_selection as ms\n",
    "from sklearn.model_selection import cross_val_score, cross_val_predict\n",
    "\n",
    "sklearn.set_config(print_changed_only=False)\n",
    "\n",
    "import xgboost as xgbst\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import import_ipynb\n",
    "from Self_Written_Functions_Sheet import col_obj_list, nan_col_list, cols_nan_unique, nan_col_obj_list_fillna, fill_masvnrtype\n",
    "from Self_Written_Functions_Sheet import rmse\n",
    "\n",
    "from ML_HousingPrice_EDA_and_Basic_Imputation import hp, hp_saleprice, hp_logsaleprice, hp_nooutliers\n",
    "\n",
    "from ML_HousingPrice_LotFrontage_Imputation_DataSetManagement import hp_nonimpute\n",
    "\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>Condition2</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>RoofStyle</th>\n",
       "      <th>RoofMatl</th>\n",
       "      <th>Exterior1st</th>\n",
       "      <th>Exterior2nd</th>\n",
       "      <th>MasVnrType</th>\n",
       "      <th>MasVnrArea</th>\n",
       "      <th>ExterQual</th>\n",
       "      <th>ExterCond</th>\n",
       "      <th>Foundation</th>\n",
       "      <th>BsmtQual</th>\n",
       "      <th>BsmtCond</th>\n",
       "      <th>BsmtExposure</th>\n",
       "      <th>BsmtFinType1</th>\n",
       "      <th>BsmtFinSF1</th>\n",
       "      <th>BsmtFinType2</th>\n",
       "      <th>BsmtFinSF2</th>\n",
       "      <th>BsmtUnfSF</th>\n",
       "      <th>TotalBsmtSF</th>\n",
       "      <th>Heating</th>\n",
       "      <th>HeatingQC</th>\n",
       "      <th>CentralAir</th>\n",
       "      <th>Electrical</th>\n",
       "      <th>1stFlrSF</th>\n",
       "      <th>2ndFlrSF</th>\n",
       "      <th>LowQualFinSF</th>\n",
       "      <th>GrLivArea</th>\n",
       "      <th>BsmtFullBath</th>\n",
       "      <th>BsmtHalfBath</th>\n",
       "      <th>FullBath</th>\n",
       "      <th>HalfBath</th>\n",
       "      <th>BedroomAbvGr</th>\n",
       "      <th>KitchenAbvGr</th>\n",
       "      <th>KitchenQual</th>\n",
       "      <th>TotRmsAbvGrd</th>\n",
       "      <th>Functional</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>FireplaceQu</th>\n",
       "      <th>GarageType</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>GarageYrModified</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>196.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>706</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "      <td>856</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>856</td>\n",
       "      <td>854</td>\n",
       "      <td>0</td>\n",
       "      <td>1710</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>548</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>25</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>978</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>284</td>\n",
       "      <td>1262</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1262</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>162.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>486</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>434</td>\n",
       "      <td>920</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>920</td>\n",
       "      <td>866</td>\n",
       "      <td>0</td>\n",
       "      <td>1786</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>608</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>216</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>540</td>\n",
       "      <td>756</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>961</td>\n",
       "      <td>756</td>\n",
       "      <td>0</td>\n",
       "      <td>1717</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>642</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>2</td>\n",
       "      <td>350.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>655</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>490</td>\n",
       "      <td>1145</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1145</td>\n",
       "      <td>1053</td>\n",
       "      <td>0</td>\n",
       "      <td>2198</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>836</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>60</td>\n",
       "      <td>6</td>\n",
       "      <td>62.0</td>\n",
       "      <td>7917</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1999</td>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>953</td>\n",
       "      <td>953</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>953</td>\n",
       "      <td>694</td>\n",
       "      <td>0</td>\n",
       "      <td>1647</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>2007</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>85.0</td>\n",
       "      <td>13175</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1978</td>\n",
       "      <td>1988</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>119.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>790</td>\n",
       "      <td>4</td>\n",
       "      <td>163</td>\n",
       "      <td>589</td>\n",
       "      <td>1542</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2073</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2073</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1978.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>500</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>66.0</td>\n",
       "      <td>9042</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "      <td>1941</td>\n",
       "      <td>2006</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>275</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>877</td>\n",
       "      <td>1152</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1188</td>\n",
       "      <td>1152</td>\n",
       "      <td>0</td>\n",
       "      <td>2340</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1941.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>252</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2500</td>\n",
       "      <td>5</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>68.0</td>\n",
       "      <td>9717</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1950</td>\n",
       "      <td>1996</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>49</td>\n",
       "      <td>4</td>\n",
       "      <td>1029</td>\n",
       "      <td>0</td>\n",
       "      <td>1078</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1078</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1078</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1950.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>240</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>366</td>\n",
       "      <td>0</td>\n",
       "      <td>112</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>20</td>\n",
       "      <td>6</td>\n",
       "      <td>75.0</td>\n",
       "      <td>9937</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>1965</td>\n",
       "      <td>1965</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>830</td>\n",
       "      <td>5</td>\n",
       "      <td>290</td>\n",
       "      <td>136</td>\n",
       "      <td>1256</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1256</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1256</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1965.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>276</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>736</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>2008</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows  80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      MSSubClass  MSZoning  LotFrontage  LotArea  Street  Alley  LotShape  \\\n",
       "0             60         6         65.0     8450       2      0         1   \n",
       "1             20         6         80.0     9600       2      0         1   \n",
       "2             60         6         68.0    11250       2      0         2   \n",
       "3             70         6         60.0     9550       2      0         2   \n",
       "4             60         6         84.0    14260       2      0         2   \n",
       "...          ...       ...          ...      ...     ...    ...       ...   \n",
       "1455          60         6         62.0     7917       2      0         1   \n",
       "1456          20         6         85.0    13175       2      0         1   \n",
       "1457          70         6         66.0     9042       2      0         1   \n",
       "1458          20         6         68.0     9717       2      0         1   \n",
       "1459          20         6         75.0     9937       2      0         1   \n",
       "\n",
       "      LandContour  Utilities  LotConfig  LandSlope  Neighborhood  Condition1  \\\n",
       "0               1          1          1          1             6           3   \n",
       "1               1          1          4          1            25           2   \n",
       "2               1          1          1          1             6           3   \n",
       "3               1          1          2          1             7           3   \n",
       "4               1          1          4          1            14           3   \n",
       "...           ...        ...        ...        ...           ...         ...   \n",
       "1455            1          1          1          1             9           3   \n",
       "1456            1          1          1          1            17           3   \n",
       "1457            1          1          1          1             7           3   \n",
       "1458            1          1          1          1            13           3   \n",
       "1459            1          1          1          1             8           3   \n",
       "\n",
       "      Condition2  BldgType  HouseStyle  OverallQual  OverallCond  YearBuilt  \\\n",
       "0              3         1           4            7            5       2003   \n",
       "1              3         1           1            6            8       1976   \n",
       "2              3         1           4            7            5       2001   \n",
       "3              3         1           4            7            5       1915   \n",
       "4              3         1           4            8            5       2000   \n",
       "...          ...       ...         ...          ...          ...        ...   \n",
       "1455           3         1           4            6            5       1999   \n",
       "1456           3         1           1            6            6       1978   \n",
       "1457           3         1           4            7            9       1941   \n",
       "1458           3         1           1            5            6       1950   \n",
       "1459           3         1           1            5            6       1965   \n",
       "\n",
       "      YearRemodAdd  RoofStyle  RoofMatl  Exterior1st  Exterior2nd  MasVnrType  \\\n",
       "0             2003          2         2           15           15           2   \n",
       "1             1976          2         2            9            9           4   \n",
       "2             2002          2         2           15           15           2   \n",
       "3             1970          2         2           16           17           4   \n",
       "4             2000          2         2           15           15           2   \n",
       "...            ...        ...       ...          ...          ...         ...   \n",
       "1455          2000          2         2           15           15           4   \n",
       "1456          1988          2         2           11           11           5   \n",
       "1457          2006          2         2            6            6           4   \n",
       "1458          1996          4         2            9            9           4   \n",
       "1459          1965          2         2            7            7           4   \n",
       "\n",
       "      MasVnrArea  ExterQual  ExterCond  Foundation  BsmtQual  BsmtCond  \\\n",
       "0          196.0          2          3           3         2         3   \n",
       "1            0.0          3          3           2         2         3   \n",
       "2          162.0          2          3           3         2         3   \n",
       "3            0.0          3          3           1         3         2   \n",
       "4          350.0          2          3           3         2         3   \n",
       "...          ...        ...        ...         ...       ...       ...   \n",
       "1455         0.0          3          3           3         2         3   \n",
       "1456       119.0          3          3           2         2         3   \n",
       "1457         0.0          1          2           5         3         2   \n",
       "1458         0.0          3          3           2         3         3   \n",
       "1459         0.0          2          3           2         3         3   \n",
       "\n",
       "      BsmtExposure  BsmtFinType1  BsmtFinSF1  BsmtFinType2  BsmtFinSF2  \\\n",
       "0                4             1         706             6           0   \n",
       "1                1             2         978             6           0   \n",
       "2                3             1         486             6           0   \n",
       "3                4             2         216             6           0   \n",
       "4                2             1         655             6           0   \n",
       "...            ...           ...         ...           ...         ...   \n",
       "1455             4             6           0             6           0   \n",
       "1456             4             2         790             4         163   \n",
       "1457             4             1         275             6           0   \n",
       "1458             3             1          49             4        1029   \n",
       "1459             4             3         830             5         290   \n",
       "\n",
       "      BsmtUnfSF  TotalBsmtSF  Heating  HeatingQC  CentralAir  Electrical  \\\n",
       "0           150          856        2          1           1           1   \n",
       "1           284         1262        2          1           1           1   \n",
       "2           434          920        2          1           1           1   \n",
       "3           540          756        2          2           1           1   \n",
       "4           490         1145        2          1           1           1   \n",
       "...         ...          ...      ...        ...         ...         ...   \n",
       "1455        953          953        2          1           1           1   \n",
       "1456        589         1542        2          3           1           1   \n",
       "1457        877         1152        2          1           1           1   \n",
       "1458          0         1078        2          2           1           2   \n",
       "1459        136         1256        2          2           1           1   \n",
       "\n",
       "      1stFlrSF  2ndFlrSF  LowQualFinSF  GrLivArea  BsmtFullBath  BsmtHalfBath  \\\n",
       "0          856       854             0       1710             1             0   \n",
       "1         1262         0             0       1262             0             1   \n",
       "2          920       866             0       1786             1             0   \n",
       "3          961       756             0       1717             1             0   \n",
       "4         1145      1053             0       2198             1             0   \n",
       "...        ...       ...           ...        ...           ...           ...   \n",
       "1455       953       694             0       1647             0             0   \n",
       "1456      2073         0             0       2073             1             0   \n",
       "1457      1188      1152             0       2340             0             0   \n",
       "1458      1078         0             0       1078             1             0   \n",
       "1459      1256         0             0       1256             1             0   \n",
       "\n",
       "      FullBath  HalfBath  BedroomAbvGr  KitchenAbvGr  KitchenQual  \\\n",
       "0            2         1             3             1            2   \n",
       "1            2         0             3             1            3   \n",
       "2            2         1             3             1            2   \n",
       "3            1         0             3             1            2   \n",
       "4            2         1             4             1            2   \n",
       "...        ...       ...           ...           ...          ...   \n",
       "1455         2         1             3             1            3   \n",
       "1456         2         0             3             1            3   \n",
       "1457         2         0             4             1            2   \n",
       "1458         1         0             2             1            2   \n",
       "1459         1         1             3             1            3   \n",
       "\n",
       "      TotRmsAbvGrd  Functional  Fireplaces  FireplaceQu  GarageType  \\\n",
       "0                8           1           0            0           2   \n",
       "1                6           1           1            3           2   \n",
       "2                6           1           1            3           2   \n",
       "3                7           1           1            2           6   \n",
       "4                9           1           1            3           2   \n",
       "...            ...         ...         ...          ...         ...   \n",
       "1455             7           1           1            3           2   \n",
       "1456             7           2           2            3           2   \n",
       "1457             9           1           2            2           2   \n",
       "1458             5           1           0            0           2   \n",
       "1459             6           1           0            0           2   \n",
       "\n",
       "      GarageYrBlt  GarageFinish  GarageCars  GarageArea  GarageQual  \\\n",
       "0          2003.0             2           2         548           3   \n",
       "1          1976.0             2           2         460           3   \n",
       "2          2001.0             2           2         608           3   \n",
       "3          1998.0             3           3         642           3   \n",
       "4          2000.0             2           3         836           3   \n",
       "...           ...           ...         ...         ...         ...   \n",
       "1455       1999.0             2           2         460           3   \n",
       "1456       1978.0             3           2         500           3   \n",
       "1457       1941.0             2           1         252           3   \n",
       "1458       1950.0             3           1         240           3   \n",
       "1459       1965.0             1           1         276           3   \n",
       "\n",
       "      GarageCond  PavedDrive  WoodDeckSF  OpenPorchSF  EnclosedPorch  \\\n",
       "0              3           1           0           61              0   \n",
       "1              3           1         298            0              0   \n",
       "2              3           1           0           42              0   \n",
       "3              3           1           0           35            272   \n",
       "4              3           1         192           84              0   \n",
       "...          ...         ...         ...          ...            ...   \n",
       "1455           3           1           0           40              0   \n",
       "1456           3           1         349            0              0   \n",
       "1457           3           1           0           60              0   \n",
       "1458           3           1         366            0            112   \n",
       "1459           3           1         736           68              0   \n",
       "\n",
       "      3SsnPorch  ScreenPorch  PoolArea  PoolQC  Fence  MiscFeature  MiscVal  \\\n",
       "0             0            0         0       0      0            0        0   \n",
       "1             0            0         0       0      0            0        0   \n",
       "2             0            0         0       0      0            0        0   \n",
       "3             0            0         0       0      0            0        0   \n",
       "4             0            0         0       0      0            0        0   \n",
       "...         ...          ...       ...     ...    ...          ...      ...   \n",
       "1455          0            0         0       0      0            0        0   \n",
       "1456          0            0         0       0      2            0        0   \n",
       "1457          0            0         0       0      1            4     2500   \n",
       "1458          0            0         0       0      0            0        0   \n",
       "1459          0            0         0       0      0            0        0   \n",
       "\n",
       "      MoSold  YrSold  SaleType  SaleCondition  GarageYrModified  \n",
       "0          2    2008         1              1              2003  \n",
       "1          5    2007         1              1              1976  \n",
       "2          9    2008         1              1              2001  \n",
       "3          2    2006         1              2              1915  \n",
       "4         12    2008         1              1              2000  \n",
       "...      ...     ...       ...            ...               ...  \n",
       "1455       8    2007         1              1              1999  \n",
       "1456       2    2010         1              1              1978  \n",
       "1457       5    2010         1              1              1941  \n",
       "1458       4    2010         1              1              1950  \n",
       "1459       6    2008         1              1              1965  \n",
       "\n",
       "[1460 rows x 80 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_nonimpute = pd.read_csv('../hp_tree_fullyimputed.csv')\n",
    "\n",
    "hp_nonimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12.247694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>12.109011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.317167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.849398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.429216</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>12.072541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>12.254863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>12.493130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>11.864462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>11.901583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1460 rows  1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      SalePrice\n",
       "0     12.247694\n",
       "1     12.109011\n",
       "2     12.317167\n",
       "3     11.849398\n",
       "4     12.429216\n",
       "...         ...\n",
       "1455  12.072541\n",
       "1456  12.254863\n",
       "1457  12.493130\n",
       "1458  11.864462\n",
       "1459  11.901583\n",
       "\n",
       "[1460 rows x 1 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hp_logsaleprice = pd.read_csv('../hp_logsaleprice.csv')\n",
    "\n",
    "hp_logsaleprice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete the outliers in the features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp_tree_noOutliers = hp_nonimpute.drop([523, 825, 1298])\n",
    "\n",
    "# hp_linear_fullyimputed_noOutliers = hp_linear_fullyimputed.drop([523, 825, 1298])\n",
    "\n",
    "hp_logsaleprice_noOutliers = hp_logsaleprice.drop([523, 825, 1298])\n",
    "\n",
    "# hp_linear_selected_noOutliers = hp_linear_selected.drop([523, 825, 1298])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice, test_size=0.2, random_state=0)\n",
    "\n",
    "ytrain = ytrain.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_gb_out, xtest_gb_out, ytrain_gb_out, ytest_gb_out = ms.train_test_split(hp_tree_noOutliers, \n",
    "                                                               hp_logsaleprice_noOutliers, test_size=0.2, random_state=0)\n",
    "\n",
    "ytrain_gb_out = ytrain_gb_out.values.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE COMPLETELY NONIMPUTED DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 10s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tdcho\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\tdcho\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\tdcho\\anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.958581</td>\n",
       "      <td>0.081778</td>\n",
       "      <td>0.143096</td>\n",
       "      <td>-0.061318</td>\n",
       "      <td>0.128947</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.958581   0.081778  0.143096 -0.061318  0.128947      0          3   \n",
       "\n",
       "   subsample  \n",
       "0        0.5  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 6)\n",
    "eta_ = np.linspace(0.05, 0.2, 20)\n",
    "gamma_ = [0]\n",
    "subsample_ = np.linspace(0.5, 1, 10)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 41s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.939519</td>\n",
       "      <td>0.09882</td>\n",
       "      <td>0.14091</td>\n",
       "      <td>-0.04209</td>\n",
       "      <td>0.160526</td>\n",
       "      <td>0.2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.939519    0.09882   0.14091  -0.04209  0.160526    0.2          5   \n",
       "\n",
       "   subsample  \n",
       "0   0.566667  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 6)\n",
    "eta_ = np.linspace(0.05, 0.2, 20)\n",
    "gamma_ = [0.2]\n",
    "subsample_ = np.linspace(0.4, 0.9, 10)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 43s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.905059</td>\n",
       "      <td>0.123812</td>\n",
       "      <td>0.149567</td>\n",
       "      <td>-0.025755</td>\n",
       "      <td>0.160526</td>\n",
       "      <td>0.6</td>\n",
       "      <td>4</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.905059   0.123812  0.149567 -0.025755  0.160526    0.6          4   \n",
       "\n",
       "   subsample  \n",
       "0   0.566667  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(3, 7)\n",
    "eta_ = np.linspace(0.05, 0.2, 20)\n",
    "gamma_ = [0.6]\n",
    "subsample_ = np.linspace(0.4, 0.9, 10)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 8min 14s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.888994</td>\n",
       "      <td>0.133878</td>\n",
       "      <td>0.149049</td>\n",
       "      <td>-0.015171</td>\n",
       "      <td>0.128947</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.490909</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.888994   0.133878  0.149049 -0.015171  0.128947    0.8          3   \n",
       "\n",
       "   subsample  \n",
       "0   0.490909  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(3, 7)\n",
    "eta_ = np.linspace(0.05, 0.2, 20)\n",
    "gamma_ = [0.8]\n",
    "subsample_ = np.linspace(0.4, 0.9, 12)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 31s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.887774</td>\n",
       "      <td>0.134612</td>\n",
       "      <td>0.150701</td>\n",
       "      <td>-0.016089</td>\n",
       "      <td>0.136842</td>\n",
       "      <td>0.8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.887774   0.134612  0.150701 -0.016089  0.136842    0.8          3   \n",
       "\n",
       "   subsample  \n",
       "0        0.5  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 7)\n",
    "eta_ = np.linspace(0.05, 0.2, 20)\n",
    "gamma_ = [0.8]\n",
    "subsample_ = np.linspace(0.4, 0.6, 11)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 3s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.880243</td>\n",
       "      <td>0.139055</td>\n",
       "      <td>0.147787</td>\n",
       "      <td>-0.008731</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.52</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.880243   0.139055  0.147787 -0.008731  0.184211      1          3   \n",
       "\n",
       "   subsample  \n",
       "0       0.52  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 7)\n",
    "eta_ = np.linspace(0.05, 0.2, 20)\n",
    "gamma_ = [1]\n",
    "subsample_ = np.linspace(0.4, 0.6, 11)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 34s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.884548</td>\n",
       "      <td>0.136533</td>\n",
       "      <td>0.15314</td>\n",
       "      <td>-0.016607</td>\n",
       "      <td>0.168421</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.554737</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.884548   0.136533   0.15314 -0.016607  0.168421      1          3   \n",
       "\n",
       "   subsample  \n",
       "0   0.554737  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 7)\n",
    "eta_ = np.linspace(0.14, 0.2, 20)\n",
    "gamma_ = [1]\n",
    "subsample_ = np.linspace(0.46, 0.58, 20)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 39s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.873029</td>\n",
       "      <td>0.143182</td>\n",
       "      <td>0.153099</td>\n",
       "      <td>-0.009917</td>\n",
       "      <td>0.165263</td>\n",
       "      <td>1.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.567368</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.873029   0.143182  0.153099 -0.009917  0.165263    1.2          2   \n",
       "\n",
       "   subsample  \n",
       "0   0.567368  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 7)\n",
    "eta_ = np.linspace(0.14, 0.2, 20)\n",
    "gamma_ = [1.2]\n",
    "subsample_ = np.linspace(0.5, 0.58, 20)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 53s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.86741</td>\n",
       "      <td>0.146316</td>\n",
       "      <td>0.1536</td>\n",
       "      <td>-0.007284</td>\n",
       "      <td>0.176667</td>\n",
       "      <td>1.4</td>\n",
       "      <td>3</td>\n",
       "      <td>0.546842</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.86741   0.146316    0.1536 -0.007284  0.176667    1.4          3   \n",
       "\n",
       "   subsample  \n",
       "0   0.546842  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 7)\n",
    "eta_ = np.linspace(0.15, 0.18, 10)\n",
    "gamma_ = [1.4]\n",
    "subsample_ = np.linspace(0.53, 0.57, 20)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 54s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858692</td>\n",
       "      <td>0.15105</td>\n",
       "      <td>0.156644</td>\n",
       "      <td>-0.005594</td>\n",
       "      <td>0.176667</td>\n",
       "      <td>1.6</td>\n",
       "      <td>3</td>\n",
       "      <td>0.546316</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.858692    0.15105  0.156644 -0.005594  0.176667    1.6          3   \n",
       "\n",
       "   subsample  \n",
       "0   0.546316  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 7)\n",
    "eta_ = np.linspace(0.15, 0.19, 10)\n",
    "gamma_ = [1.6]\n",
    "subsample_ = np.linspace(0.52, 0.57, 20)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 54s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.853731</td>\n",
       "      <td>0.153678</td>\n",
       "      <td>0.158404</td>\n",
       "      <td>-0.004725</td>\n",
       "      <td>0.158889</td>\n",
       "      <td>1.8</td>\n",
       "      <td>3</td>\n",
       "      <td>0.527895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.853731   0.153678  0.158404 -0.004725  0.158889    1.8          3   \n",
       "\n",
       "   subsample  \n",
       "0   0.527895  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 7)\n",
    "eta_ = np.linspace(0.15, 0.19, 10)\n",
    "gamma_ = [1.8]\n",
    "subsample_ = np.linspace(0.52, 0.57, 20)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10min 22s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.853776</td>\n",
       "      <td>0.153655</td>\n",
       "      <td>0.158718</td>\n",
       "      <td>-0.005063</td>\n",
       "      <td>0.167778</td>\n",
       "      <td>1.8</td>\n",
       "      <td>2</td>\n",
       "      <td>0.557586</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.853776   0.153655  0.158718 -0.005063  0.167778    1.8          2   \n",
       "\n",
       "   subsample  \n",
       "0   0.557586  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 7)\n",
    "eta_ = np.linspace(0.15, 0.19, 10)\n",
    "gamma_ = [1.8]\n",
    "subsample_ = np.linspace(0.51, 0.57, 30)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 13min 48s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.852465</td>\n",
       "      <td>0.154342</td>\n",
       "      <td>0.159632</td>\n",
       "      <td>-0.00529</td>\n",
       "      <td>0.158889</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.559655</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.852465   0.154342  0.159632  -0.00529  0.158889      2          2   \n",
       "\n",
       "   subsample  \n",
       "0   0.559655  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(1, 7)\n",
    "eta_ = np.linspace(0.15, 0.19, 10)\n",
    "gamma_ = [2]\n",
    "subsample_ = np.linspace(0.51, 0.57, 30)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 36s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.841555</td>\n",
       "      <td>0.159947</td>\n",
       "      <td>0.162634</td>\n",
       "      <td>-0.002687</td>\n",
       "      <td>0.176667</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.526897</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.841555   0.159947  0.162634 -0.002687  0.176667    2.2          2   \n",
       "\n",
       "   subsample  \n",
       "0   0.526897  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(1, 7)\n",
    "eta_ = np.linspace(0.15, 0.19, 10)\n",
    "gamma_ = [2.2]\n",
    "subsample_ = np.linspace(0.52, 0.57, 30)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 14min 46s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.838468</td>\n",
       "      <td>0.161498</td>\n",
       "      <td>0.16291</td>\n",
       "      <td>-0.001412</td>\n",
       "      <td>0.185556</td>\n",
       "      <td>2.4</td>\n",
       "      <td>2</td>\n",
       "      <td>0.545862</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.838468   0.161498   0.16291 -0.001412  0.185556    2.4          2   \n",
       "\n",
       "   subsample  \n",
       "0   0.545862  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(1, 7)\n",
    "eta_ = np.linspace(0.15, 0.19, 10)\n",
    "gamma_ = [2.4]\n",
    "subsample_ = np.linspace(0.52, 0.57, 30)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 58s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.82274</td>\n",
       "      <td>0.169178</td>\n",
       "      <td>0.169273</td>\n",
       "      <td>-0.000096</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>0.561379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Scores  TrainRMSE  TestRMSE  DiffRMSE  eta  gamma  max_depth  subsample\n",
       "0  0.82274   0.169178  0.169273 -0.000096  0.2    2.6          2   0.561379"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(1, 7)\n",
    "eta_ = np.linspace(0.16, 0.20, 10)\n",
    "gamma_ = [2.6]\n",
    "subsample_ = np.linspace(0.52, 0.57, 30)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12min 15s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.832319</td>\n",
       "      <td>0.164543</td>\n",
       "      <td>0.164733</td>\n",
       "      <td>-0.00019</td>\n",
       "      <td>0.191111</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2</td>\n",
       "      <td>0.538966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.832319   0.164543  0.164733  -0.00019  0.191111    2.5          2   \n",
       "\n",
       "   subsample  \n",
       "0   0.538966  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(1, 7)\n",
    "eta_ = np.linspace(0.16, 0.20, 10)\n",
    "gamma_ = [2.5]\n",
    "subsample_ = np.linspace(0.52, 0.57, 30)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds,\n",
    "                         refit=True, n_jobs=-1,\n",
    "                         scoring='neg_root_mean_squared_error',\n",
    "                         return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11min 54s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.822816</td>\n",
       "      <td>0.169141</td>\n",
       "      <td>0.167898</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.195556</td>\n",
       "      <td>2.8</td>\n",
       "      <td>1</td>\n",
       "      <td>0.563103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.822816   0.169141  0.167898  0.001243  0.195556    2.8          1   \n",
       "\n",
       "   subsample  \n",
       "0   0.563103  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(1, 7)\n",
    "eta_ = np.linspace(0.16, 0.20, 10)\n",
    "gamma_ = [2.8]\n",
    "subsample_ = np.linspace(0.52, 0.57, 30)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Based on the results above, we will use gamma of 2.6 and tune from there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 52s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.829084</td>\n",
       "      <td>0.166123</td>\n",
       "      <td>0.165502</td>\n",
       "      <td>0.00062</td>\n",
       "      <td>0.194737</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1</td>\n",
       "      <td>0.593333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.829084   0.166123  0.165502   0.00062  0.194737    2.6          1   \n",
       "\n",
       "   subsample  \n",
       "0   0.593333  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(1, 7)\n",
    "eta_ = np.linspace(0.1, 0.3, 20)\n",
    "gamma_ = [2.6]\n",
    "subsample_ = np.linspace(0.54, 0.6, 10)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(1, 7)\n",
    "eta_ = np.linspace(0.15, 0.25, 11)\n",
    "gamma_ = [2.6]\n",
    "subsample_ = np.linspace(0.54, 0.7, 14)\n",
    "treemethod = ['gpu_hist']\n",
    "predictor_ = ['gpu_predictor']\n",
    "booster_ = ['gbtree', 'dart']\n",
    "samplingmethod = ['uniform', 'gradient_based']\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_,\n",
    "              'tree_method': treemethod,\n",
    "              'predictor': predictor_,\n",
    "              'sampling_method': samplingmethod,\n",
    "              'booster': booster_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 1s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>booster</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>predictor</th>\n",
       "      <th>sampling_method</th>\n",
       "      <th>subsample</th>\n",
       "      <th>tree_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.855518</td>\n",
       "      <td>0.152737</td>\n",
       "      <td>0.158991</td>\n",
       "      <td>-0.006254</td>\n",
       "      <td>dart</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>3</td>\n",
       "      <td>gpu_predictor</td>\n",
       "      <td>gradient_based</td>\n",
       "      <td>0.565263</td>\n",
       "      <td>gpu_hist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE booster  eta  gamma  max_depth  \\\n",
       "0  0.855518   0.152737  0.158991 -0.006254    dart  0.2    2.6          3   \n",
       "\n",
       "       predictor sampling_method  subsample tree_method  \n",
       "0  gpu_predictor  gradient_based   0.565263    gpu_hist  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(1, 7)\n",
    "eta_ = [0.19, 0.2, 0.21]\n",
    "gamma_ = [2.6]\n",
    "subsample_ = np.linspace(0.54, 0.6, 20)\n",
    "treemethod = ['gpu_hist']\n",
    "predictor_ = ['gpu_predictor']\n",
    "booster_ = ['dart']\n",
    "samplingmethod = ['gradient_based']\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_,\n",
    "              'tree_method': treemethod,\n",
    "              'predictor': predictor_,\n",
    "              'sampling_method': samplingmethod,\n",
    "              'booster': booster_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 3s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>booster</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>predictor</th>\n",
       "      <th>sampling_method</th>\n",
       "      <th>subsample</th>\n",
       "      <th>tree_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.858364</td>\n",
       "      <td>0.151225</td>\n",
       "      <td>0.159313</td>\n",
       "      <td>-0.008088</td>\n",
       "      <td>dart</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2.6</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>gpu_predictor</td>\n",
       "      <td>gradient_based</td>\n",
       "      <td>0.549474</td>\n",
       "      <td>gpu_hist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE booster  colsample_bytree  eta  \\\n",
       "0  0.858364   0.151225  0.159313 -0.008088    dart               0.8  0.2   \n",
       "\n",
       "   gamma  max_depth  min_child_weight      predictor sampling_method  \\\n",
       "0    2.6          2                 1  gpu_predictor  gradient_based   \n",
       "\n",
       "   subsample tree_method  \n",
       "0   0.549474    gpu_hist  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(1, 7)\n",
    "eta_ = [0.2]\n",
    "gamma_ = [2.6]\n",
    "subsample_ = np.linspace(0.54, 0.6, 20)\n",
    "treemethod = ['gpu_hist']\n",
    "predictor_ = ['gpu_predictor']\n",
    "booster_ = ['dart']\n",
    "samplingmethod = ['gradient_based']\n",
    "minchildwt = [1]\n",
    "colsamptree = [0.8]\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_,\n",
    "              'tree_method': treemethod,\n",
    "              'predictor': predictor_,\n",
    "              'sampling_method': samplingmethod,\n",
    "              'booster': booster_,\n",
    "              'min_child_weight': minchildwt,\n",
    "              'colsample_bytree': colsamptree}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 31s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>booster</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>predictor</th>\n",
       "      <th>sampling_method</th>\n",
       "      <th>subsample</th>\n",
       "      <th>tree_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.920203</td>\n",
       "      <td>0.111986</td>\n",
       "      <td>0.13028</td>\n",
       "      <td>-0.018294</td>\n",
       "      <td>dart</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>gpu_predictor</td>\n",
       "      <td>gradient_based</td>\n",
       "      <td>0.552632</td>\n",
       "      <td>gpu_hist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE booster  colsample_bytree  eta  \\\n",
       "0  0.920203   0.111986   0.13028 -0.018294    dart               0.8  0.1   \n",
       "\n",
       "   gamma  max_depth  min_child_weight      predictor sampling_method  \\\n",
       "0    0.5          3                 1  gpu_predictor  gradient_based   \n",
       "\n",
       "   subsample tree_method  \n",
       "0   0.552632    gpu_hist  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_tree_noOutliers,\n",
    "                                                   hp_logsaleprice_noOutliers,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(1, 7)\n",
    "eta_ = [0.1]\n",
    "gamma_ = [0.5]\n",
    "subsample_ = np.linspace(0.54, 0.6, 20)\n",
    "treemethod = ['gpu_hist']\n",
    "predictor_ = ['gpu_predictor']\n",
    "booster_ = ['dart']\n",
    "samplingmethod = ['gradient_based']\n",
    "minchildwt = [1]\n",
    "colsamptree = [0.8]\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_,\n",
    "              'tree_method': treemethod,\n",
    "              'predictor': predictor_,\n",
    "              'sampling_method': samplingmethod,\n",
    "              'booster': booster_,\n",
    "              'min_child_weight': minchildwt,\n",
    "              'colsample_bytree': colsamptree}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7min 43s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>booster</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>predictor</th>\n",
       "      <th>sampling_method</th>\n",
       "      <th>subsample</th>\n",
       "      <th>tree_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.922725</td>\n",
       "      <td>0.110203</td>\n",
       "      <td>0.134312</td>\n",
       "      <td>-0.02411</td>\n",
       "      <td>dart</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>gpu_predictor</td>\n",
       "      <td>gradient_based</td>\n",
       "      <td>0.562105</td>\n",
       "      <td>gpu_hist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE booster  colsample_bytree   eta  \\\n",
       "0  0.922725   0.110203  0.134312  -0.02411    dart               0.8  0.12   \n",
       "\n",
       "   gamma  max_depth  min_child_weight      predictor sampling_method  \\\n",
       "0    0.5          3                 1  gpu_predictor  gradient_based   \n",
       "\n",
       "   subsample tree_method  \n",
       "0   0.562105    gpu_hist  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_tree_noOutliers,\n",
    "                                                   hp_logsaleprice_noOutliers,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(1, 5)\n",
    "eta_ = [0.08, 0.09, 0.1, 0.11, 0.12]\n",
    "gamma_ = [0.5]\n",
    "subsample_ = np.linspace(0.54, 0.6, 20)\n",
    "treemethod = ['gpu_hist']\n",
    "predictor_ = ['gpu_predictor']\n",
    "booster_ = ['dart']\n",
    "samplingmethod = ['gradient_based']\n",
    "minchildwt = [1]\n",
    "colsamptree = [0.8]\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_,\n",
    "              'tree_method': treemethod,\n",
    "              'predictor': predictor_,\n",
    "              'sampling_method': samplingmethod,\n",
    "              'booster': booster_,\n",
    "              'min_child_weight': minchildwt,\n",
    "              'colsample_bytree': colsamptree}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([2.21967235, 2.29927797, 2.23953352, 2.28589964, 2.24983058,\n",
       "        2.24613128, 2.32409611, 2.22649755, 2.27042584, 2.22993031,\n",
       "        2.19658427, 2.1979321 , 2.23103323, 2.24335489, 2.29696174,\n",
       "        2.32826724, 2.36012173, 2.28431826, 2.3104846 , 2.37971501,\n",
       "        2.52555108, 2.53547873, 2.63745918, 2.65594492, 2.56871409,\n",
       "        2.57215075, 2.64179754, 2.68351927, 2.63158855, 2.67058187,\n",
       "        2.63331904, 2.60025606, 2.65859456, 2.56026616, 2.61606765,\n",
       "        2.61782675, 2.59211078, 2.62923169, 2.69637189, 2.67770348,\n",
       "        2.82959652, 2.80220857, 2.84794683, 2.93921132, 2.96832528,\n",
       "        2.92520208, 2.88855453, 2.89280405, 2.858708  , 2.94984717,\n",
       "        2.91438074, 2.94475207, 2.80861044, 2.92831268, 2.92862425,\n",
       "        2.94652705, 2.94449639, 2.87500291, 2.889007  , 2.8769196 ,\n",
       "        3.04343162, 3.10380654, 3.03064957, 3.03601284, 3.02790904,\n",
       "        3.08359084, 3.0744338 , 3.06849704, 3.12095685, 3.09558516,\n",
       "        3.12479901, 3.12473054, 3.03517833, 3.11931028, 3.06780014,\n",
       "        3.13823023, 3.13779573, 3.01548886, 3.04685321, 2.71369181,\n",
       "        2.37747908, 2.29148803, 2.23524508, 2.19329185, 2.16768045,\n",
       "        2.23892293, 2.19671202, 2.23197298, 2.22759504, 2.12354617,\n",
       "        2.13218317, 2.19003792, 2.26240501, 2.21175179, 2.16087241,\n",
       "        2.22907677, 2.24529953, 2.30247993, 2.31501937, 2.36303849,\n",
       "        2.50727921, 2.49718976, 2.52227073, 2.49163418, 2.4928576 ,\n",
       "        2.55753899, 2.49318738, 2.57297869, 2.5443594 , 2.59140258,\n",
       "        2.60192194, 2.62241554, 2.58890181, 2.55077524, 2.57047367,\n",
       "        2.57207608, 2.55145626, 2.5400425 , 2.52880373, 2.55295253,\n",
       "        2.68463993, 2.76758127, 2.70320892, 2.73126388, 2.76313419,\n",
       "        2.8228425 , 2.78498297, 2.82246547, 2.97990031, 2.8820199 ,\n",
       "        2.72129588, 2.72947645, 2.75888376, 2.80936842, 2.7755197 ,\n",
       "        2.81226592, 2.89901328, 2.84583416, 2.89314003, 2.92711701,\n",
       "        2.96050701, 2.93394666, 2.97396579, 3.01755209, 2.9532136 ,\n",
       "        2.97667689, 2.96736655, 2.97783337, 3.0932898 , 2.99866934,\n",
       "        3.07723455, 3.08669062, 3.08682661, 3.02186894, 2.93370514,\n",
       "        2.95139856, 2.94932909, 2.98087544, 2.94431734, 2.64329305,\n",
       "        2.30498595, 2.2552433 , 2.24294105, 2.21889839, 2.22862339,\n",
       "        2.2398922 , 2.22520614, 2.23041048, 2.15389662, 2.0860527 ,\n",
       "        2.13633065, 2.20659795, 2.20574737, 2.22267995, 2.23162303,\n",
       "        2.20568871, 2.20549822, 2.25419407, 2.27469883, 2.35659862,\n",
       "        2.47285523, 2.52972155, 2.53450823, 2.5163084 , 2.46791434,\n",
       "        2.45425987, 2.47074862, 2.50793166, 2.49430647, 2.49127584,\n",
       "        2.45799642, 2.51779985, 2.81297827, 2.88979225, 2.70559139,\n",
       "        2.58425336, 2.57874446, 2.55978723, 2.5695796 , 2.64453192,\n",
       "        2.82083969, 2.77035408, 2.73905864, 2.76108775, 2.77155747,\n",
       "        2.85230751, 3.04119296, 3.03013387, 2.92762642, 2.81222463,\n",
       "        2.77755871, 2.73685722, 2.72775121, 2.76141558, 2.70654392,\n",
       "        2.74809613, 2.70982652, 2.67378807, 2.71310835, 2.71360068,\n",
       "        2.81894193, 2.90363245, 2.93490934, 2.95701861, 2.88494811,\n",
       "        2.87468481, 2.93090935, 2.86697593, 2.89192095, 2.93179121,\n",
       "        2.87542667, 2.92799172, 2.99011102, 2.98763924, 2.94576368,\n",
       "        2.86679769, 2.91787705, 2.94335628, 2.90987196, 2.64252853,\n",
       "        2.26602302, 2.20011916, 2.20261755, 2.16010747, 2.18220067,\n",
       "        2.19669714, 2.17828112, 2.21851406, 2.23652234, 2.24732847,\n",
       "        2.23941865, 2.26849661, 2.2615458 , 2.22494707, 2.18764691,\n",
       "        2.23019662, 2.21661029, 2.19018192, 2.24988651, 2.361724  ,\n",
       "        2.49008551, 2.452775  , 2.51171336, 2.53279805, 2.56268287,\n",
       "        2.53499179, 2.46124487, 2.53411627, 2.49329276, 2.50148964,\n",
       "        2.47039976, 2.460604  , 2.47021937, 2.50516033, 2.51835418,\n",
       "        2.49548569, 2.50340838, 2.52429409, 2.54460878, 2.5812397 ,\n",
       "        2.7244319 , 2.72899728, 2.72879839, 2.67296262, 2.73867898,\n",
       "        2.67772207, 2.72003951, 2.67793632, 2.66492844, 2.73401141,\n",
       "        2.72720051, 2.77699265, 2.74216356, 2.75559969, 2.74560456,\n",
       "        2.69138193, 2.736479  , 2.66028705, 2.67294683, 2.76284637,\n",
       "        2.78678703, 2.80909553, 2.83840065, 2.87134228, 2.87708354,\n",
       "        2.84842663, 2.8540338 , 2.86302414, 2.90728827, 2.78304524,\n",
       "        2.78820577, 2.85041203, 2.89722843, 2.81763616, 2.81567454,\n",
       "        2.88636746, 2.86858516, 2.83551226, 2.82643633, 2.65150018,\n",
       "        2.27396231, 2.2958406 , 2.26009951, 2.22160234, 2.25938783,\n",
       "        2.22909966, 2.20730252, 2.23048306, 2.1794466 , 2.15252604,\n",
       "        2.19155931, 2.18638887, 2.20951977, 2.22109585, 2.2094739 ,\n",
       "        2.24932423, 2.22889075, 2.26149426, 2.20635982, 2.24977355,\n",
       "        2.49238782, 2.47132244, 2.48767319, 2.45879679, 2.56661243,\n",
       "        2.73260117, 2.75934749, 2.7593204 , 2.76100655, 2.79658141,\n",
       "        2.83263965, 2.76483202, 2.71882172, 2.65975556, 2.7485888 ,\n",
       "        2.74120092, 2.66074867, 2.68762894, 2.68037667, 3.14835911,\n",
       "        3.56412182, 3.63412485, 3.34502897, 3.38381643, 3.17831345,\n",
       "        2.94049764, 2.80499039, 2.8174562 , 2.76657419, 2.74436512,\n",
       "        2.71026778, 2.86684356, 2.86808186, 2.79905109, 2.80658932,\n",
       "        2.83342686, 2.80719619, 2.69451222, 2.72976422, 2.79619994,\n",
       "        2.84448037, 2.94601474, 2.83959031, 2.88457394, 2.89564748,\n",
       "        3.23941517, 3.32975516, 3.1182972 , 2.94195132, 2.92599773,\n",
       "        2.98990006, 2.90808954, 2.94241261, 2.93869991, 2.90818329,\n",
       "        2.97407136, 3.02619777, 2.94980979, 2.8183331 , 2.1784533 ]),\n",
       " 'std_fit_time': array([0.08626912, 0.03257228, 0.06329357, 0.00852926, 0.03638972,\n",
       "        0.03088501, 0.02439509, 0.07594532, 0.04240212, 0.05211655,\n",
       "        0.02688425, 0.04544238, 0.01248317, 0.02017219, 0.05320925,\n",
       "        0.03065968, 0.02888279, 0.05392973, 0.03252431, 0.09334848,\n",
       "        0.0279516 , 0.04449622, 0.02827742, 0.02668966, 0.04402893,\n",
       "        0.03206247, 0.08882049, 0.02407187, 0.04162208, 0.02274128,\n",
       "        0.03095654, 0.05021371, 0.03540415, 0.07959271, 0.03430943,\n",
       "        0.05549086, 0.03600378, 0.01336825, 0.06997991, 0.02689942,\n",
       "        0.03444361, 0.03492145, 0.04728155, 0.04321627, 0.04256927,\n",
       "        0.06616289, 0.04491016, 0.02516018, 0.05365804, 0.03268047,\n",
       "        0.05023017, 0.04259796, 0.06998845, 0.04751249, 0.05514696,\n",
       "        0.08186928, 0.02674881, 0.08242359, 0.03492859, 0.07516128,\n",
       "        0.05644524, 0.04821008, 0.03167479, 0.03582056, 0.04945144,\n",
       "        0.04686463, 0.04397748, 0.04216199, 0.02031413, 0.06072412,\n",
       "        0.01970934, 0.0735478 , 0.02297078, 0.04513664, 0.04108469,\n",
       "        0.03173939, 0.04779963, 0.05274552, 0.06351358, 0.17295844,\n",
       "        0.02568811, 0.09138259, 0.04894096, 0.02887574, 0.04936879,\n",
       "        0.07589963, 0.04332518, 0.03610392, 0.05390365, 0.07605433,\n",
       "        0.05914682, 0.05386981, 0.02456037, 0.0420765 , 0.03855185,\n",
       "        0.02444894, 0.02837268, 0.03718634, 0.01552367, 0.03427246,\n",
       "        0.03534743, 0.04467039, 0.0193433 , 0.03062475, 0.02138644,\n",
       "        0.04865295, 0.05928815, 0.0616391 , 0.06139792, 0.02676925,\n",
       "        0.06190987, 0.03276061, 0.03538032, 0.06967096, 0.04072938,\n",
       "        0.06300481, 0.03909177, 0.06666166, 0.07288728, 0.04270378,\n",
       "        0.04907291, 0.06359608, 0.0408026 , 0.02471406, 0.04675367,\n",
       "        0.07655044, 0.0489053 , 0.04881285, 0.08875263, 0.09250652,\n",
       "        0.05973175, 0.03870737, 0.05679958, 0.06525355, 0.06382165,\n",
       "        0.05032889, 0.08056381, 0.06496868, 0.04021806, 0.03970138,\n",
       "        0.04597014, 0.03709212, 0.04230716, 0.04386053, 0.04962007,\n",
       "        0.03342563, 0.02989392, 0.0610762 , 0.04840723, 0.03355003,\n",
       "        0.05743956, 0.08716053, 0.05242028, 0.05986363, 0.0547671 ,\n",
       "        0.04855378, 0.02120779, 0.05753391, 0.06846403, 0.06193293,\n",
       "        0.03792774, 0.03269411, 0.02663674, 0.0342259 , 0.03638501,\n",
       "        0.03929076, 0.04720841, 0.05951634, 0.05273193, 0.03428183,\n",
       "        0.03008129, 0.03747917, 0.00967543, 0.02469108, 0.05185481,\n",
       "        0.07683491, 0.06362444, 0.04750199, 0.01579867, 0.02671665,\n",
       "        0.04176182, 0.04022746, 0.01958049, 0.05084057, 0.04716772,\n",
       "        0.02111508, 0.02266964, 0.05947356, 0.04666674, 0.04895144,\n",
       "        0.05357759, 0.0355055 , 0.05003719, 0.02550328, 0.13074355,\n",
       "        0.04470407, 0.03725609, 0.08082015, 0.0521228 , 0.04947837,\n",
       "        0.05420268, 0.06315508, 0.02709769, 0.078612  , 0.08100393,\n",
       "        0.08424938, 0.08524201, 0.0655539 , 0.06221258, 0.0309053 ,\n",
       "        0.04744574, 0.0427674 , 0.05708229, 0.02604733, 0.05950499,\n",
       "        0.02638207, 0.04102278, 0.0391544 , 0.02928712, 0.03063916,\n",
       "        0.07274163, 0.03102423, 0.05387655, 0.02525748, 0.06423474,\n",
       "        0.06012554, 0.06041859, 0.06207959, 0.04031034, 0.03866183,\n",
       "        0.04631002, 0.05582722, 0.04527262, 0.01539768, 0.04469939,\n",
       "        0.05586703, 0.05787454, 0.06069779, 0.06118092, 0.07766892,\n",
       "        0.06220056, 0.025431  , 0.03266933, 0.06742108, 0.0349221 ,\n",
       "        0.02193196, 0.03012427, 0.03603604, 0.03053879, 0.03393242,\n",
       "        0.0297819 , 0.03857282, 0.0556076 , 0.03037788, 0.055244  ,\n",
       "        0.04230104, 0.01273747, 0.05926128, 0.04907452, 0.04353951,\n",
       "        0.04115155, 0.04030054, 0.05079298, 0.04521058, 0.02561585,\n",
       "        0.0529557 , 0.02396013, 0.01408165, 0.07079353, 0.07220695,\n",
       "        0.04201015, 0.06000057, 0.02276154, 0.04274249, 0.05170241,\n",
       "        0.03019961, 0.02692421, 0.04658747, 0.03418616, 0.05800273,\n",
       "        0.04311135, 0.05609052, 0.06319645, 0.04444587, 0.04571539,\n",
       "        0.05641144, 0.08622512, 0.08092778, 0.05003818, 0.04769965,\n",
       "        0.04912825, 0.04952772, 0.01465151, 0.04312322, 0.10920146,\n",
       "        0.06534555, 0.01891456, 0.02914928, 0.01318818, 0.06167147,\n",
       "        0.05888597, 0.05963255, 0.06189996, 0.03612547, 0.04186363,\n",
       "        0.0338086 , 0.06374229, 0.04913508, 0.04019356, 0.05008425,\n",
       "        0.0325783 , 0.05669585, 0.03868586, 0.0568242 , 0.02763708,\n",
       "        0.04923825, 0.05442053, 0.04216709, 0.05393326, 0.05879213,\n",
       "        0.03595275, 0.06906487, 0.03528711, 0.05359627, 0.0311675 ,\n",
       "        0.04120697, 0.05448581, 0.06122436, 0.0558708 , 0.01115594,\n",
       "        0.05075576, 0.03650711, 0.02944555, 0.01486528, 0.03805236,\n",
       "        0.0432641 , 0.05069844, 0.02699404, 0.01819935, 0.08432926,\n",
       "        0.03454362, 0.02667061, 0.05141535, 0.02726619, 0.08683443,\n",
       "        0.03574886, 0.05100379, 0.04788029, 0.05874791, 0.08558321,\n",
       "        0.04981042, 0.04825497, 0.06101509, 0.04207918, 0.03146526,\n",
       "        0.05479449, 0.04452121, 0.04688394, 0.08858469, 0.20093706,\n",
       "        0.04424079, 0.14064444, 0.06512636, 0.0350546 , 0.0941196 ,\n",
       "        0.09849163, 0.04229501, 0.05144891, 0.04262425, 0.05528356,\n",
       "        0.04678813, 0.07146706, 0.01049534, 0.01902836, 0.03378569,\n",
       "        0.0664329 , 0.05787746, 0.04492994, 0.1070411 , 0.06008617,\n",
       "        0.06777574, 0.0149136 , 0.08550485, 0.04659827, 0.05608752,\n",
       "        0.09352464, 0.04506185, 0.12789042, 0.02539206, 0.0463754 ,\n",
       "        0.02519308, 0.01760377, 0.02791018, 0.04231886, 0.04073476,\n",
       "        0.05647626, 0.04399526, 0.04543309, 0.03438471, 0.46847928]),\n",
       " 'mean_score_time': array([0.04482694, 0.03296361, 0.04424639, 0.03322115, 0.03428855,\n",
       "        0.04457169, 0.04250898, 0.04494286, 0.03601651, 0.05052466,\n",
       "        0.04619322, 0.03697662, 0.04808087, 0.04016533, 0.05857835,\n",
       "        0.06271734, 0.05468793, 0.04870563, 0.0461453 , 0.05948792,\n",
       "        0.04676652, 0.05576625, 0.05232348, 0.04940743, 0.05743389,\n",
       "        0.05275278, 0.07131433, 0.05121036, 0.06116962, 0.04754229,\n",
       "        0.04312487, 0.05892854, 0.04580278, 0.05300012, 0.05564179,\n",
       "        0.05238018, 0.06185212, 0.05072608, 0.06105919, 0.04831786,\n",
       "        0.06355433, 0.05707049, 0.06741939, 0.07257032, 0.05489635,\n",
       "        0.06265616, 0.04574142, 0.05733743, 0.05479689, 0.05611405,\n",
       "        0.06277285, 0.05589809, 0.0609961 , 0.06168857, 0.06117477,\n",
       "        0.0648664 , 0.06042848, 0.05339675, 0.05606742, 0.06171517,\n",
       "        0.0691606 , 0.05976524, 0.06676898, 0.05365562, 0.06577926,\n",
       "        0.06191001, 0.05907302, 0.06970091, 0.05188851, 0.07956553,\n",
       "        0.04790015, 0.06603832, 0.06891565, 0.05673976, 0.06568527,\n",
       "        0.05629702, 0.06380854, 0.06389003, 0.05524755, 0.06282821,\n",
       "        0.05593433, 0.05699182, 0.04979749, 0.05350642, 0.05643754,\n",
       "        0.04513464, 0.05624127, 0.05419598, 0.04315248, 0.05513592,\n",
       "        0.04537745, 0.05961561, 0.05109196, 0.05411286, 0.05787077,\n",
       "        0.05518603, 0.05751619, 0.04856243, 0.06013474, 0.05704899,\n",
       "        0.06090693, 0.06192341, 0.05832162, 0.06119337, 0.06113482,\n",
       "        0.06730251, 0.05908465, 0.05860896, 0.06357279, 0.06455956,\n",
       "        0.06638699, 0.06369014, 0.05459037, 0.05970011, 0.05855126,\n",
       "        0.06831236, 0.05479918, 0.06240253, 0.05771956, 0.06603131,\n",
       "        0.0679749 , 0.05804877, 0.06738205, 0.06779504, 0.07693338,\n",
       "        0.06750598, 0.05819707, 0.07151136, 0.06763906, 0.05577559,\n",
       "        0.06716814, 0.07043285, 0.07307787, 0.06729879, 0.06290975,\n",
       "        0.06864457, 0.05446248, 0.07308607, 0.06458831, 0.07933488,\n",
       "        0.06264753, 0.06849041, 0.06942725, 0.06145344, 0.06159253,\n",
       "        0.07177062, 0.07056732, 0.06418638, 0.05942178, 0.07347488,\n",
       "        0.0606698 , 0.06701355, 0.07591572, 0.05973711, 0.07979717,\n",
       "        0.06288071, 0.07096343, 0.07057061, 0.05671425, 0.0500905 ,\n",
       "        0.04801702, 0.06121087, 0.05309596, 0.05506358, 0.05202131,\n",
       "        0.05843468, 0.05120044, 0.05061207, 0.05903792, 0.05068173,\n",
       "        0.05203838, 0.05716958, 0.05401359, 0.05480042, 0.0549758 ,\n",
       "        0.05299835, 0.05296144, 0.0568881 , 0.06198225, 0.05386286,\n",
       "        0.06832952, 0.06669569, 0.05688968, 0.05659447, 0.05838575,\n",
       "        0.06148162, 0.05901928, 0.05916867, 0.06067924, 0.06526427,\n",
       "        0.06475983, 0.06917853, 0.0763514 , 0.0634933 , 0.06437163,\n",
       "        0.06218247, 0.05040345, 0.0670269 , 0.06212697, 0.0697185 ,\n",
       "        0.06318865, 0.06447768, 0.0636097 , 0.0600091 , 0.07186041,\n",
       "        0.06626792, 0.07510991, 0.06423922, 0.058952  , 0.06206698,\n",
       "        0.05972772, 0.05900674, 0.06788855, 0.05581865, 0.06806483,\n",
       "        0.06299181, 0.06576715, 0.06449466, 0.06631899, 0.06509929,\n",
       "        0.06874971, 0.06931419, 0.06687126, 0.0546782 , 0.05794573,\n",
       "        0.07038569, 0.05525393, 0.08300505, 0.0707819 , 0.06641488,\n",
       "        0.06389508, 0.06932478, 0.06912484, 0.07303228, 0.05803876,\n",
       "        0.07147965, 0.06571169, 0.06896834, 0.05460215, 0.05400896,\n",
       "        0.04925256, 0.05110173, 0.05090756, 0.05069647, 0.0538106 ,\n",
       "        0.06366587, 0.0499351 , 0.05500045, 0.04648423, 0.06116939,\n",
       "        0.05074544, 0.04910984, 0.05474715, 0.04205499, 0.05534477,\n",
       "        0.04701762, 0.05174189, 0.04900327, 0.04441199, 0.05737605,\n",
       "        0.0550941 , 0.06209259, 0.06138577, 0.06547608, 0.05744739,\n",
       "        0.04989705, 0.06298389, 0.05480232, 0.06098075, 0.05247812,\n",
       "        0.05927253, 0.06360025, 0.05035644, 0.06324053, 0.05752206,\n",
       "        0.05847697, 0.06604667, 0.05285354, 0.06550326, 0.06630168,\n",
       "        0.06310034, 0.05987124, 0.07328558, 0.07128153, 0.07050195,\n",
       "        0.0562396 , 0.0679565 , 0.05774956, 0.06371722, 0.06225471,\n",
       "        0.06821871, 0.06730318, 0.05788279, 0.05801125, 0.06797996,\n",
       "        0.06501498, 0.06716056, 0.07730918, 0.070051  , 0.07814279,\n",
       "        0.05823436, 0.06162238, 0.06604438, 0.06848559, 0.06650648,\n",
       "        0.05345507, 0.0597096 , 0.080793  , 0.05892348, 0.061731  ,\n",
       "        0.05902205, 0.06730437, 0.06057849, 0.06556396, 0.06029234,\n",
       "        0.06070046, 0.0747097 , 0.07250175, 0.05519037, 0.05675001,\n",
       "        0.04920816, 0.05598478, 0.0506074 , 0.05184031, 0.05587306,\n",
       "        0.04871697, 0.05821438, 0.05863276, 0.0561758 , 0.0517972 ,\n",
       "        0.05074854, 0.05620031, 0.05891771, 0.06181011, 0.05823407,\n",
       "        0.05347476, 0.05831523, 0.04999552, 0.05280323, 0.06247993,\n",
       "        0.06048703, 0.06349149, 0.06204619, 0.06038146, 0.06879725,\n",
       "        0.07559924, 0.07944307, 0.06710806, 0.06819854, 0.07992268,\n",
       "        0.06653981, 0.06621003, 0.06589518, 0.07310886, 0.06209378,\n",
       "        0.0623961 , 0.06734171, 0.06715207, 0.07954469, 0.1224144 ,\n",
       "        0.07230587, 0.08363237, 0.07555499, 0.06469636, 0.05368237,\n",
       "        0.06807728, 0.06352096, 0.07249794, 0.07333989, 0.05642619,\n",
       "        0.07271471, 0.05775943, 0.06217456, 0.07305536, 0.0528316 ,\n",
       "        0.07305651, 0.05984812, 0.06289306, 0.06321011, 0.05887413,\n",
       "        0.07725   , 0.05695963, 0.0695869 , 0.05764818, 0.06191545,\n",
       "        0.06598535, 0.06094298, 0.06961722, 0.06975298, 0.08010135,\n",
       "        0.06097393, 0.05013599, 0.05005856, 0.05865235, 0.0610271 ,\n",
       "        0.06059799, 0.05408564, 0.06375718, 0.04020534, 0.022082  ]),\n",
       " 'std_score_time': array([0.0082077 , 0.00759935, 0.01482963, 0.01076677, 0.01742847,\n",
       "        0.00527635, 0.00815553, 0.0069214 , 0.00392872, 0.00789576,\n",
       "        0.01014806, 0.00741945, 0.0077705 , 0.00940597, 0.00711524,\n",
       "        0.01037228, 0.0032126 , 0.00722574, 0.00511112, 0.00565365,\n",
       "        0.00262402, 0.00731347, 0.0056216 , 0.01064539, 0.01101365,\n",
       "        0.00635151, 0.00889628, 0.01149275, 0.02066095, 0.01001601,\n",
       "        0.00804117, 0.00452675, 0.00541839, 0.0077004 , 0.00614883,\n",
       "        0.00467791, 0.02080534, 0.00774766, 0.01041329, 0.0056968 ,\n",
       "        0.01178963, 0.00923443, 0.01325619, 0.0156985 , 0.00550389,\n",
       "        0.0117017 , 0.00605613, 0.00348934, 0.00849797, 0.01081203,\n",
       "        0.01007753, 0.00342366, 0.00424295, 0.00367392, 0.00230076,\n",
       "        0.01372219, 0.00638948, 0.00624435, 0.00429779, 0.00669462,\n",
       "        0.02165601, 0.00652282, 0.01510772, 0.00732724, 0.01027087,\n",
       "        0.00915857, 0.00575224, 0.01219785, 0.00396649, 0.01148744,\n",
       "        0.00631984, 0.0141545 , 0.00966763, 0.00670948, 0.00441004,\n",
       "        0.00255089, 0.01050818, 0.00360239, 0.00593741, 0.01475022,\n",
       "        0.00698509, 0.00510961, 0.00466215, 0.00419268, 0.00577658,\n",
       "        0.00351642, 0.00121802, 0.00701387, 0.00306451, 0.00467566,\n",
       "        0.00224495, 0.00965437, 0.00896837, 0.00982917, 0.01068929,\n",
       "        0.00324488, 0.0041578 , 0.00673954, 0.01039322, 0.00586802,\n",
       "        0.00760679, 0.00675255, 0.0047285 , 0.00367925, 0.00510166,\n",
       "        0.01269399, 0.00287736, 0.00532424, 0.00309936, 0.00980301,\n",
       "        0.01170755, 0.00514894, 0.00668696, 0.00447869, 0.00556224,\n",
       "        0.00518279, 0.00440126, 0.00613583, 0.00222032, 0.00753291,\n",
       "        0.00717706, 0.00938654, 0.00934748, 0.01035696, 0.0173693 ,\n",
       "        0.01151249, 0.00493958, 0.01440278, 0.01911155, 0.01236919,\n",
       "        0.00492569, 0.01794876, 0.01430803, 0.00807172, 0.00643418,\n",
       "        0.00880507, 0.00558927, 0.01284558, 0.01528251, 0.00704302,\n",
       "        0.01203167, 0.01021847, 0.00939329, 0.00914868, 0.01142977,\n",
       "        0.0123068 , 0.01671379, 0.01346281, 0.01318807, 0.0153749 ,\n",
       "        0.0115192 , 0.02187372, 0.01599223, 0.00774635, 0.01059332,\n",
       "        0.00957922, 0.01412653, 0.00885063, 0.0065198 , 0.00584801,\n",
       "        0.00574529, 0.00395968, 0.00516097, 0.00704585, 0.00413148,\n",
       "        0.00623267, 0.00443961, 0.00833171, 0.00918683, 0.004748  ,\n",
       "        0.00599074, 0.00460657, 0.00334871, 0.00428169, 0.00347059,\n",
       "        0.00474488, 0.00720174, 0.00727265, 0.00488975, 0.00566789,\n",
       "        0.00470522, 0.00935645, 0.00255058, 0.00463975, 0.00759629,\n",
       "        0.00490115, 0.00711033, 0.0036    , 0.00821239, 0.01253571,\n",
       "        0.01335248, 0.00917986, 0.00732219, 0.00477494, 0.00653885,\n",
       "        0.007469  , 0.00378831, 0.00731117, 0.0070414 , 0.00745011,\n",
       "        0.01018685, 0.0131147 , 0.00835347, 0.01087732, 0.01307486,\n",
       "        0.01473129, 0.02274205, 0.00860362, 0.0132899 , 0.00310582,\n",
       "        0.01461941, 0.00656948, 0.00401874, 0.01219893, 0.01421353,\n",
       "        0.00629117, 0.00967197, 0.01185794, 0.01399357, 0.01239976,\n",
       "        0.01111278, 0.01164912, 0.01289434, 0.00651168, 0.01124122,\n",
       "        0.00378647, 0.01239888, 0.01147105, 0.01301614, 0.00540182,\n",
       "        0.02216477, 0.00988649, 0.00956901, 0.00688241, 0.01031409,\n",
       "        0.0175672 , 0.00907641, 0.00513093, 0.01039202, 0.01224912,\n",
       "        0.00245956, 0.00539098, 0.00500124, 0.00335057, 0.00743882,\n",
       "        0.01241571, 0.00448916, 0.00694112, 0.01066747, 0.00762863,\n",
       "        0.0062183 , 0.00686247, 0.00186912, 0.00879707, 0.00244506,\n",
       "        0.00930433, 0.00792108, 0.00891188, 0.00915224, 0.00460643,\n",
       "        0.0065879 , 0.00779137, 0.01458619, 0.01396555, 0.00525196,\n",
       "        0.00793153, 0.00562154, 0.0073707 , 0.00599669, 0.00324569,\n",
       "        0.00760462, 0.00969402, 0.00317334, 0.00798764, 0.00624112,\n",
       "        0.00689954, 0.01010966, 0.00612041, 0.00224677, 0.01604288,\n",
       "        0.00636716, 0.00713167, 0.01429079, 0.01542942, 0.0080753 ,\n",
       "        0.0066234 , 0.01383386, 0.01231175, 0.0046897 , 0.00846467,\n",
       "        0.00976263, 0.00555154, 0.00529   , 0.00841383, 0.01326455,\n",
       "        0.0103722 , 0.0102863 , 0.01168628, 0.01242503, 0.01753262,\n",
       "        0.01202479, 0.01242021, 0.01641582, 0.00536564, 0.00894482,\n",
       "        0.00909774, 0.01015969, 0.00986874, 0.01412939, 0.0138794 ,\n",
       "        0.0074358 , 0.00732293, 0.00371492, 0.00817277, 0.01477169,\n",
       "        0.01247496, 0.01283337, 0.01727213, 0.01256879, 0.00661067,\n",
       "        0.0026986 , 0.00317066, 0.0037801 , 0.00882759, 0.01113957,\n",
       "        0.00634213, 0.00689392, 0.01745743, 0.00920224, 0.00671323,\n",
       "        0.00293929, 0.00465254, 0.00886083, 0.00365328, 0.00575628,\n",
       "        0.00268727, 0.0030682 , 0.00214096, 0.00414298, 0.00891089,\n",
       "        0.00397608, 0.00494201, 0.01199383, 0.00498956, 0.0064086 ,\n",
       "        0.00741177, 0.01280888, 0.00756985, 0.00389263, 0.00603222,\n",
       "        0.00975135, 0.00768442, 0.00793056, 0.01109823, 0.00894727,\n",
       "        0.00296986, 0.00472454, 0.00678343, 0.01190485, 0.03564875,\n",
       "        0.01706572, 0.02287744, 0.03035306, 0.01279203, 0.00921002,\n",
       "        0.01158214, 0.01127853, 0.02747507, 0.01190468, 0.00932037,\n",
       "        0.01549401, 0.0074673 , 0.01896174, 0.01503269, 0.00957851,\n",
       "        0.00564402, 0.01383327, 0.01550441, 0.01365216, 0.01224746,\n",
       "        0.00783179, 0.00775437, 0.01957559, 0.00712541, 0.00647672,\n",
       "        0.02017982, 0.00214763, 0.00752768, 0.00983319, 0.02962951,\n",
       "        0.01034247, 0.00382679, 0.00959259, 0.00581152, 0.00821514,\n",
       "        0.01975406, 0.00425795, 0.01326303, 0.01264682, 0.00640797]),\n",
       " 'param_booster': masked_array(data=['dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart', 'dart', 'dart', 'dart', 'dart', 'dart', 'dart',\n",
       "                    'dart'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_colsample_bytree': masked_array(data=[0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8,\n",
       "                    0.8, 0.8, 0.8, 0.8],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_eta': masked_array(data=[0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08,\n",
       "                    0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08,\n",
       "                    0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08,\n",
       "                    0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08,\n",
       "                    0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08,\n",
       "                    0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08,\n",
       "                    0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08,\n",
       "                    0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08,\n",
       "                    0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.08, 0.09,\n",
       "                    0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09,\n",
       "                    0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09,\n",
       "                    0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09,\n",
       "                    0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09,\n",
       "                    0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09,\n",
       "                    0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09,\n",
       "                    0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09,\n",
       "                    0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09,\n",
       "                    0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.09, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1,\n",
       "                    0.1, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11,\n",
       "                    0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11,\n",
       "                    0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11,\n",
       "                    0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11,\n",
       "                    0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11,\n",
       "                    0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11,\n",
       "                    0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11,\n",
       "                    0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11,\n",
       "                    0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11, 0.11,\n",
       "                    0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12,\n",
       "                    0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12,\n",
       "                    0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12,\n",
       "                    0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12,\n",
       "                    0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12,\n",
       "                    0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12,\n",
       "                    0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12,\n",
       "                    0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12,\n",
       "                    0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12, 0.12],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_gamma': masked_array(data=[0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5,\n",
       "                    0.5, 0.5, 0.5, 0.5],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_max_depth': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "                    2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "                    3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "                    3, 3, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4,\n",
       "                    4, 4, 4, 4],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_min_child_weight': masked_array(data=[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "                    1, 1, 1, 1],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_predictor': masked_array(data=['gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor', 'gpu_predictor', 'gpu_predictor',\n",
       "                    'gpu_predictor'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_sampling_method': masked_array(data=['gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based', 'gradient_based', 'gradient_based',\n",
       "                    'gradient_based'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_subsample': masked_array(data=[0.54, 0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6, 0.54,\n",
       "                    0.5431578947368422, 0.5463157894736842,\n",
       "                    0.5494736842105263, 0.5526315789473685,\n",
       "                    0.5557894736842105, 0.5589473684210526,\n",
       "                    0.5621052631578948, 0.5652631578947368,\n",
       "                    0.5684210526315789, 0.5715789473684211,\n",
       "                    0.5747368421052632, 0.5778947368421052,\n",
       "                    0.5810526315789474, 0.5842105263157895,\n",
       "                    0.5873684210526315, 0.5905263157894737,\n",
       "                    0.5936842105263158, 0.5968421052631578, 0.6],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_tree_method': masked_array(data=['gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist',\n",
       "                    'gpu_hist', 'gpu_hist', 'gpu_hist', 'gpu_hist'],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.08,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.09,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.1,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.11,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 1,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 2,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 3,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.54,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5431578947368422,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5463157894736842,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5494736842105263,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5526315789473685,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5557894736842105,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5589473684210526,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5621052631578948,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5652631578947368,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5684210526315789,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5715789473684211,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5747368421052632,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5778947368421052,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5810526315789474,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5842105263157895,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5873684210526315,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5905263157894737,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5936842105263158,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.5968421052631578,\n",
       "   'tree_method': 'gpu_hist'},\n",
       "  {'booster': 'dart',\n",
       "   'colsample_bytree': 0.8,\n",
       "   'eta': 0.12,\n",
       "   'gamma': 0.5,\n",
       "   'max_depth': 4,\n",
       "   'min_child_weight': 1,\n",
       "   'predictor': 'gpu_predictor',\n",
       "   'sampling_method': 'gradient_based',\n",
       "   'subsample': 0.6,\n",
       "   'tree_method': 'gpu_hist'}],\n",
       " 'split0_test_score': array([-0.15540279, -0.15488056, -0.15472784, -0.15683516, -0.15500417,\n",
       "        -0.15500414, -0.15466984, -0.15433939, -0.1556782 , -0.15495023,\n",
       "        -0.15408596, -0.15423126, -0.15375403, -0.15667625, -0.1530032 ,\n",
       "        -0.1564157 , -0.15557672, -0.15509253, -0.15680318, -0.15734111,\n",
       "        -0.13374645, -0.1331944 , -0.13353364, -0.13494937, -0.13268974,\n",
       "        -0.13289779, -0.13511231, -0.13443249, -0.13275599, -0.13315307,\n",
       "        -0.13564976, -0.13537066, -0.13643702, -0.13598172, -0.13287287,\n",
       "        -0.13618929, -0.13571638, -0.13493045, -0.13589057, -0.13541432,\n",
       "        -0.13166592, -0.12995419, -0.1291636 , -0.13281162, -0.13223017,\n",
       "        -0.12716402, -0.13198106, -0.13271138, -0.13182816, -0.1322201 ,\n",
       "        -0.13095429, -0.13135265, -0.1342467 , -0.13437774, -0.13191982,\n",
       "        -0.13428843, -0.13391445, -0.13514962, -0.13343661, -0.13291356,\n",
       "        -0.12906023, -0.12981695, -0.1301208 , -0.12990719, -0.13025309,\n",
       "        -0.1293898 , -0.13457973, -0.13087433, -0.12873143, -0.13151029,\n",
       "        -0.13129282, -0.13134324, -0.13479997, -0.13270765, -0.13315526,\n",
       "        -0.13409401, -0.13268657, -0.13392729, -0.13359283, -0.13522472,\n",
       "        -0.14877001, -0.14975035, -0.15157893, -0.15076289, -0.14872647,\n",
       "        -0.14939118, -0.15062034, -0.15080189, -0.14736308, -0.15242847,\n",
       "        -0.14587298, -0.14952196, -0.14738755, -0.14806205, -0.1523244 ,\n",
       "        -0.15087705, -0.14930622, -0.15182478, -0.14986418, -0.14986007,\n",
       "        -0.13438962, -0.13312842, -0.13442929, -0.13748953, -0.13316645,\n",
       "        -0.13347276, -0.13312953, -0.1339056 , -0.13061802, -0.13306557,\n",
       "        -0.13312121, -0.13592454, -0.13157045, -0.13204608, -0.13216925,\n",
       "        -0.13551695, -0.12933663, -0.13283665, -0.13307714, -0.13077943,\n",
       "        -0.12918552, -0.13274946, -0.13520995, -0.13260593, -0.13085257,\n",
       "        -0.1305397 , -0.1304112 , -0.12892225, -0.13111489, -0.13421037,\n",
       "        -0.13126393, -0.13046101, -0.1276104 , -0.12646476, -0.13427572,\n",
       "        -0.13157994, -0.12899888, -0.13196274, -0.13226144, -0.13270373,\n",
       "        -0.12818834, -0.13040188, -0.1321288 , -0.12912753, -0.13156158,\n",
       "        -0.13055149, -0.12988422, -0.12878279, -0.13005646, -0.13064221,\n",
       "        -0.13012132, -0.13248406, -0.12792288, -0.12781955, -0.13224813,\n",
       "        -0.13073536, -0.12878014, -0.1321342 , -0.13310639, -0.13161271,\n",
       "        -0.14782965, -0.14519659, -0.14613062, -0.14322143, -0.1459473 ,\n",
       "        -0.14555566, -0.14458821, -0.14559332, -0.14465639, -0.14548332,\n",
       "        -0.14570929, -0.14492219, -0.14474362, -0.14743585, -0.14591111,\n",
       "        -0.14708047, -0.14830869, -0.14808207, -0.14868166, -0.14675926,\n",
       "        -0.13432406, -0.12852845, -0.13249293, -0.1297134 , -0.13145256,\n",
       "        -0.13279635, -0.12870082, -0.12825184, -0.13266951, -0.12963236,\n",
       "        -0.13140402, -0.13009758, -0.13078834, -0.1293268 , -0.12702518,\n",
       "        -0.1312121 , -0.13084702, -0.13370355, -0.13355404, -0.13423529,\n",
       "        -0.13202332, -0.12990001, -0.13154844, -0.12982146, -0.12946704,\n",
       "        -0.12742705, -0.12784825, -0.12888714, -0.12976209, -0.13194165,\n",
       "        -0.13006532, -0.12937603, -0.1284076 , -0.13299655, -0.12765074,\n",
       "        -0.1319431 , -0.12859066, -0.13010466, -0.13087649, -0.13057526,\n",
       "        -0.1336169 , -0.13098422, -0.1323299 , -0.13057517, -0.1314539 ,\n",
       "        -0.12939998, -0.12924614, -0.1279753 , -0.12956252, -0.12837714,\n",
       "        -0.12797423, -0.12770634, -0.12933311, -0.12900566, -0.1303102 ,\n",
       "        -0.13024535, -0.12774143, -0.13017164, -0.13101687, -0.13022963,\n",
       "        -0.14039027, -0.14241343, -0.14258371, -0.14208785, -0.14132928,\n",
       "        -0.14273245, -0.14221465, -0.1422755 , -0.14185381, -0.14300076,\n",
       "        -0.14212356, -0.14123668, -0.1413064 , -0.1418795 , -0.14318498,\n",
       "        -0.1444525 , -0.1426607 , -0.14109605, -0.14165337, -0.14581375,\n",
       "        -0.13230383, -0.13408513, -0.13068853, -0.13314358, -0.13138406,\n",
       "        -0.13113098, -0.13013391, -0.13369717, -0.12921284, -0.13262452,\n",
       "        -0.13459376, -0.13664882, -0.13592617, -0.13484369, -0.13623654,\n",
       "        -0.13719653, -0.13703684, -0.13682125, -0.13547239, -0.1328547 ,\n",
       "        -0.13385495, -0.13209585, -0.13313929, -0.12993142, -0.13387876,\n",
       "        -0.13486356, -0.13025211, -0.12921262, -0.13263577, -0.13203308,\n",
       "        -0.13366788, -0.12918345, -0.13279566, -0.1288703 , -0.13494255,\n",
       "        -0.13344831, -0.1352586 , -0.13197281, -0.13211103, -0.13123039,\n",
       "        -0.13254275, -0.13314021, -0.13554698, -0.13126302, -0.13288033,\n",
       "        -0.13374108, -0.13208486, -0.13078591, -0.12900084, -0.13239429,\n",
       "        -0.13211872, -0.13247532, -0.13223081, -0.13215479, -0.13296709,\n",
       "        -0.13289379, -0.13223149, -0.13306576, -0.13183793, -0.13547277,\n",
       "        -0.14058884, -0.14302197, -0.14294789, -0.14351335, -0.14235801,\n",
       "        -0.14092288, -0.14115043, -0.13935311, -0.1415579 , -0.13872238,\n",
       "        -0.14036656, -0.14133504, -0.13713626, -0.13994052, -0.13756619,\n",
       "        -0.13893804, -0.14114049, -0.1401231 , -0.14142473, -0.14245696,\n",
       "        -0.13518989, -0.13210406, -0.12989271, -0.12986284, -0.13054291,\n",
       "        -0.13664936, -0.13682734, -0.13675471, -0.13622959, -0.1296564 ,\n",
       "        -0.13283713, -0.13532149, -0.13266603, -0.13399953, -0.13371169,\n",
       "        -0.13458679, -0.13471637, -0.1351999 , -0.13489014, -0.1333508 ,\n",
       "        -0.13657305, -0.13156782, -0.13355245, -0.1330996 , -0.13677638,\n",
       "        -0.13259196, -0.13069294, -0.12855788, -0.13012778, -0.12609032,\n",
       "        -0.13505932, -0.13152356, -0.13251153, -0.13107123, -0.13149155,\n",
       "        -0.13411668, -0.13475579, -0.13266172, -0.1325467 , -0.13045753,\n",
       "        -0.13938505, -0.13346934, -0.13275765, -0.13281051, -0.13381912,\n",
       "        -0.13231423, -0.13137255, -0.13024057, -0.13062543, -0.12710213,\n",
       "        -0.13603119, -0.13387117, -0.13035535, -0.13545847, -0.13350554,\n",
       "        -0.13703591, -0.13191726, -0.13170705, -0.13348109, -0.13241141]),\n",
       " 'split1_test_score': array([-0.14346363, -0.14622681, -0.14480755, -0.14512244, -0.14441925,\n",
       "        -0.14595   , -0.14670092, -0.14612846, -0.14533047, -0.14545841,\n",
       "        -0.14360074, -0.14397557, -0.14430449, -0.14488938, -0.14434085,\n",
       "        -0.1466687 , -0.14662893, -0.1463974 , -0.14454844, -0.14471377,\n",
       "        -0.13134096, -0.13294569, -0.13529425, -0.13245209, -0.13197778,\n",
       "        -0.13437866, -0.1331921 , -0.13168438, -0.13414847, -0.13279954,\n",
       "        -0.13458453, -0.13345084, -0.13478219, -0.13266143, -0.13328765,\n",
       "        -0.13273966, -0.1338866 , -0.13373955, -0.13266011, -0.13201812,\n",
       "        -0.12626039, -0.1319508 , -0.1298294 , -0.13360947, -0.13054683,\n",
       "        -0.134576  , -0.13373568, -0.13715264, -0.1303983 , -0.13178375,\n",
       "        -0.13388737, -0.13227177, -0.13742095, -0.12972403, -0.13328953,\n",
       "        -0.13128025, -0.13231595, -0.13397627, -0.13021564, -0.13322801,\n",
       "        -0.12626979, -0.13247717, -0.13126447, -0.1347929 , -0.12962954,\n",
       "        -0.13488043, -0.13403318, -0.13628307, -0.13142869, -0.13226548,\n",
       "        -0.13476743, -0.13501925, -0.13566693, -0.12833613, -0.13115728,\n",
       "        -0.13213164, -0.13310832, -0.13382855, -0.13354098, -0.13407768,\n",
       "        -0.14419026, -0.1426132 , -0.14258716, -0.14266704, -0.1421289 ,\n",
       "        -0.13947804, -0.1422273 , -0.14232077, -0.1415182 , -0.14294153,\n",
       "        -0.14348674, -0.14291336, -0.14139739, -0.14192363, -0.14174409,\n",
       "        -0.14240884, -0.14055072, -0.13984303, -0.1389164 , -0.13923746,\n",
       "        -0.13725583, -0.13872446, -0.13834482, -0.13682862, -0.13975487,\n",
       "        -0.13477471, -0.13583872, -0.13601742, -0.13455521, -0.13920896,\n",
       "        -0.13706918, -0.13721829, -0.13557569, -0.13468141, -0.13424827,\n",
       "        -0.13415446, -0.13424697, -0.13327877, -0.13537779, -0.13343649,\n",
       "        -0.13455333, -0.13723576, -0.13875972, -0.13571151, -0.13855887,\n",
       "        -0.1376864 , -0.13528757, -0.13691174, -0.13501023, -0.1366652 ,\n",
       "        -0.13807552, -0.13393058, -0.13620167, -0.13844585, -0.13709973,\n",
       "        -0.13339094, -0.13022079, -0.13204722, -0.13361409, -0.13188908,\n",
       "        -0.13357063, -0.13588976, -0.13928116, -0.13713741, -0.13636879,\n",
       "        -0.1377389 , -0.13716889, -0.1368813 , -0.13664594, -0.13637381,\n",
       "        -0.13251731, -0.13718138, -0.13371172, -0.13891264, -0.13441238,\n",
       "        -0.13037345, -0.13153305, -0.12973237, -0.1320064 , -0.13256513,\n",
       "        -0.13637757, -0.13659821, -0.13769933, -0.13666141, -0.13738067,\n",
       "        -0.13649793, -0.13710112, -0.13565659, -0.13709234, -0.13592383,\n",
       "        -0.13499068, -0.13657396, -0.13644038, -0.13748969, -0.13735934,\n",
       "        -0.13713268, -0.13743373, -0.13780362, -0.13652663, -0.13593333,\n",
       "        -0.1353332 , -0.13312031, -0.13324693, -0.13676752, -0.13651306,\n",
       "        -0.13567034, -0.13698859, -0.13818081, -0.13904357, -0.13269796,\n",
       "        -0.13597886, -0.13469832, -0.13339339, -0.1353875 , -0.13343576,\n",
       "        -0.13449354, -0.13396226, -0.13633574, -0.13727501, -0.13285786,\n",
       "        -0.1364756 , -0.13413751, -0.13131669, -0.13821812, -0.13409094,\n",
       "        -0.1343854 , -0.14019235, -0.13823565, -0.13933362, -0.13513537,\n",
       "        -0.13689006, -0.1314007 , -0.13191457, -0.13049735, -0.13523978,\n",
       "        -0.13396257, -0.13431919, -0.13669702, -0.13511055, -0.13344409,\n",
       "        -0.13620512, -0.13553846, -0.13244837, -0.13662114, -0.13400924,\n",
       "        -0.13579956, -0.13766197, -0.1352333 , -0.13893605, -0.13588693,\n",
       "        -0.13497467, -0.13408952, -0.13295397, -0.13305546, -0.13333181,\n",
       "        -0.1335183 , -0.1357312 , -0.13633265, -0.1368231 , -0.13380965,\n",
       "        -0.13722118, -0.13268576, -0.13718957, -0.13503736, -0.13656315,\n",
       "        -0.13792694, -0.13302973, -0.13327049, -0.13407792, -0.13545891,\n",
       "        -0.1326735 , -0.13620456, -0.13724473, -0.13282102, -0.13320899,\n",
       "        -0.13263812, -0.13594484, -0.13629287, -0.13773696, -0.13793979,\n",
       "        -0.13316316, -0.13385272, -0.13052661, -0.13338564, -0.13472308,\n",
       "        -0.1333003 , -0.13446037, -0.13635956, -0.13727937, -0.13589093,\n",
       "        -0.1354521 , -0.13476527, -0.13344634, -0.13246655, -0.13326147,\n",
       "        -0.13504541, -0.13434643, -0.13413743, -0.13732118, -0.13654231,\n",
       "        -0.13714221, -0.13602202, -0.13431826, -0.13329678, -0.13681832,\n",
       "        -0.13634933, -0.13277213, -0.13157191, -0.13320185, -0.13640842,\n",
       "        -0.13451889, -0.13679341, -0.13560196, -0.13390206, -0.14057084,\n",
       "        -0.13400647, -0.13418616, -0.13746234, -0.13793994, -0.13529164,\n",
       "        -0.13845954, -0.13378155, -0.13398429, -0.13144501, -0.13265289,\n",
       "        -0.13381721, -0.13303881, -0.13184219, -0.13547785, -0.13475794,\n",
       "        -0.13555989, -0.13475415, -0.13585743, -0.13285311, -0.1404605 ,\n",
       "        -0.13509767, -0.13772079, -0.13706044, -0.13890333, -0.13473148,\n",
       "        -0.13370829, -0.13497656, -0.13510534, -0.14139275, -0.13624553,\n",
       "        -0.13251195, -0.13275134, -0.13232648, -0.13445939, -0.13436741,\n",
       "        -0.13499728, -0.13459579, -0.13302016, -0.13525806, -0.13477329,\n",
       "        -0.13218642, -0.13410762, -0.13428279, -0.1349379 , -0.13523442,\n",
       "        -0.13278464, -0.13234558, -0.13441804, -0.13867476, -0.13380959,\n",
       "        -0.13352539, -0.13190484, -0.1335449 , -0.13564461, -0.1384777 ,\n",
       "        -0.1349162 , -0.13343458, -0.13649562, -0.13377545, -0.13255146,\n",
       "        -0.13327411, -0.13648908, -0.13580963, -0.13684691, -0.13442772,\n",
       "        -0.13355328, -0.13130107, -0.1321704 , -0.13555948, -0.13332017,\n",
       "        -0.13272025, -0.13464555, -0.13290868, -0.13802776, -0.13654692,\n",
       "        -0.13639257, -0.13564384, -0.13317412, -0.13372675, -0.13176279,\n",
       "        -0.1313986 , -0.13798707, -0.13487291, -0.13204471, -0.13242083,\n",
       "        -0.13435145, -0.1327215 , -0.13270639, -0.13509182, -0.13162149,\n",
       "        -0.13262691, -0.13388246, -0.13373138, -0.13922241, -0.13628444,\n",
       "        -0.13440465, -0.13675756, -0.13467745, -0.1348005 , -0.12937852,\n",
       "        -0.13628235, -0.13465591, -0.13725355, -0.13206818, -0.13338025]),\n",
       " 'split2_test_score': array([-0.16470066, -0.1618573 , -0.16385091, -0.16357784, -0.16175687,\n",
       "        -0.15943732, -0.16281606, -0.16259683, -0.16240771, -0.16252379,\n",
       "        -0.16103276, -0.16224544, -0.15993372, -0.16150755, -0.16090914,\n",
       "        -0.16199478, -0.16560405, -0.16345109, -0.16431435, -0.1614362 ,\n",
       "        -0.13643481, -0.13646481, -0.1346653 , -0.13557577, -0.13286519,\n",
       "        -0.13547611, -0.13567515, -0.13251954, -0.13621796, -0.1380769 ,\n",
       "        -0.13476417, -0.13403469, -0.13489026, -0.13521438, -0.13448492,\n",
       "        -0.1353428 , -0.13539163, -0.1333325 , -0.13388274, -0.13497318,\n",
       "        -0.13401942, -0.13437867, -0.13536766, -0.13359066, -0.1345156 ,\n",
       "        -0.13515487, -0.13500452, -0.13685427, -0.13496947, -0.1362079 ,\n",
       "        -0.13884574, -0.13255692, -0.13441041, -0.13502921, -0.13545875,\n",
       "        -0.13568461, -0.13292026, -0.13454089, -0.13366072, -0.13403607,\n",
       "        -0.1335811 , -0.13410753, -0.1362071 , -0.13531313, -0.13507764,\n",
       "        -0.1351156 , -0.13531447, -0.13719713, -0.13663925, -0.13509833,\n",
       "        -0.13603267, -0.13393959, -0.13482661, -0.13605916, -0.13384839,\n",
       "        -0.13380688, -0.1347737 , -0.13555477, -0.13588598, -0.13687006,\n",
       "        -0.15375585, -0.15242546, -0.15460184, -0.15255455, -0.15115113,\n",
       "        -0.15518098, -0.15483152, -0.15417025, -0.15612022, -0.15405741,\n",
       "        -0.15560452, -0.15597033, -0.15467636, -0.15385391, -0.15512913,\n",
       "        -0.15419682, -0.15256716, -0.15411279, -0.1523778 , -0.15567662,\n",
       "        -0.1371116 , -0.13390893, -0.13768085, -0.13682797, -0.13772391,\n",
       "        -0.13346831, -0.13575019, -0.13694496, -0.1357083 , -0.13284878,\n",
       "        -0.13689069, -0.1321878 , -0.13555286, -0.13676421, -0.13840753,\n",
       "        -0.13767844, -0.13108364, -0.13491918, -0.13549597, -0.13161819,\n",
       "        -0.13240155, -0.13299896, -0.13530409, -0.13502042, -0.13529694,\n",
       "        -0.13366808, -0.13354496, -0.13264587, -0.13428226, -0.13582926,\n",
       "        -0.13665983, -0.13402683, -0.1342989 , -0.13141836, -0.13525597,\n",
       "        -0.13280331, -0.13422106, -0.13262177, -0.13442074, -0.13754905,\n",
       "        -0.13320816, -0.1327255 , -0.13262756, -0.13343121, -0.13510372,\n",
       "        -0.13462344, -0.13387917, -0.1377633 , -0.13499905, -0.13401652,\n",
       "        -0.13523082, -0.1305701 , -0.13312435, -0.13601661, -0.13516813,\n",
       "        -0.13026882, -0.13143155, -0.13221353, -0.13214586, -0.13765419,\n",
       "        -0.14475373, -0.1453726 , -0.14518739, -0.14639695, -0.14360663,\n",
       "        -0.14616224, -0.14560302, -0.14646528, -0.14685146, -0.14596796,\n",
       "        -0.14651788, -0.14521022, -0.14822339, -0.14798729, -0.14734281,\n",
       "        -0.14787311, -0.1487191 , -0.14799677, -0.14756875, -0.14892697,\n",
       "        -0.1306983 , -0.12911959, -0.12565123, -0.12846021, -0.12993155,\n",
       "        -0.12785214, -0.1241577 , -0.12519638, -0.12542608, -0.13337011,\n",
       "        -0.13247217, -0.1293419 , -0.13312593, -0.13314198, -0.13856226,\n",
       "        -0.13899207, -0.13278388, -0.1315349 , -0.13354508, -0.13533629,\n",
       "        -0.12964844, -0.12598427, -0.12734314, -0.12552327, -0.1256355 ,\n",
       "        -0.12839692, -0.12363271, -0.12800149, -0.12648125, -0.13297397,\n",
       "        -0.13349614, -0.13011446, -0.13528352, -0.13212007, -0.12859992,\n",
       "        -0.12918024, -0.13361749, -0.13292038, -0.13256958, -0.13139823,\n",
       "        -0.13164121, -0.12780425, -0.12644763, -0.12412515, -0.12486211,\n",
       "        -0.12799031, -0.12532341, -0.12728138, -0.12699705, -0.13113929,\n",
       "        -0.13319739, -0.13166788, -0.13363623, -0.13328395, -0.1284659 ,\n",
       "        -0.12994795, -0.13313621, -0.13355932, -0.13272474, -0.13232155,\n",
       "        -0.14387276, -0.13924846, -0.13953324, -0.14129556, -0.13800984,\n",
       "        -0.14076825, -0.13997885, -0.14014207, -0.1399693 , -0.14150987,\n",
       "        -0.14039679, -0.14218444, -0.14225001, -0.14107739, -0.14038416,\n",
       "        -0.14330394, -0.14408292, -0.14196166, -0.14110196, -0.14200964,\n",
       "        -0.12843983, -0.1301295 , -0.12878235, -0.13055324, -0.13229902,\n",
       "        -0.1303147 , -0.12867536, -0.13095967, -0.13028211, -0.13061518,\n",
       "        -0.12982471, -0.12941081, -0.13372566, -0.13317819, -0.1287865 ,\n",
       "        -0.13576298, -0.13577032, -0.13602305, -0.1334639 , -0.13452513,\n",
       "        -0.13109614, -0.13183027, -0.12921051, -0.13336729, -0.12946955,\n",
       "        -0.13322535, -0.13522398, -0.1346699 , -0.13290096, -0.13146197,\n",
       "        -0.13217728, -0.13548447, -0.13357034, -0.13315899, -0.13361122,\n",
       "        -0.13328911, -0.13112078, -0.13089115, -0.1325546 , -0.13184437,\n",
       "        -0.13222209, -0.13209791, -0.1344284 , -0.13545068, -0.1273644 ,\n",
       "        -0.1342556 , -0.13554341, -0.13379143, -0.13342493, -0.13297806,\n",
       "        -0.13471774, -0.13260202, -0.13518645, -0.13142112, -0.12931721,\n",
       "        -0.13657449, -0.1314799 , -0.13619949, -0.13492259, -0.13647876,\n",
       "        -0.1450952 , -0.14272426, -0.1410483 , -0.14167073, -0.14143179,\n",
       "        -0.13740181, -0.14071436, -0.14072125, -0.13907573, -0.13958693,\n",
       "        -0.14167293, -0.14079469, -0.13867663, -0.13939956, -0.14117209,\n",
       "        -0.13886237, -0.13878963, -0.14120529, -0.14085951, -0.13956403,\n",
       "        -0.13125715, -0.12985616, -0.12755975, -0.12794489, -0.1305416 ,\n",
       "        -0.12877828, -0.13168856, -0.13213995, -0.1315344 , -0.1293051 ,\n",
       "        -0.13098783, -0.13776748, -0.1315417 , -0.13452213, -0.13476615,\n",
       "        -0.1309612 , -0.13208656, -0.1347268 , -0.13475251, -0.13462748,\n",
       "        -0.12926008, -0.13176195, -0.12646115, -0.12954071, -0.12923953,\n",
       "        -0.1291103 , -0.13061016, -0.12862461, -0.13130382, -0.1298272 ,\n",
       "        -0.12848587, -0.13183054, -0.1373561 , -0.13032495, -0.13390736,\n",
       "        -0.13165078, -0.12589959, -0.12772358, -0.13123276, -0.13342494,\n",
       "        -0.13175517, -0.13175863, -0.12421892, -0.1326038 , -0.13160977,\n",
       "        -0.12998191, -0.12915445, -0.13252559, -0.1327487 , -0.13244025,\n",
       "        -0.12971399, -0.13131444, -0.1361421 , -0.13223332, -0.13447278,\n",
       "        -0.12982114, -0.1293306 , -0.12911575, -0.13327142, -0.13118431]),\n",
       " 'split3_test_score': array([-0.1604796 , -0.16210398, -0.16042949, -0.15931122, -0.15931372,\n",
       "        -0.15872419, -0.1599892 , -0.15953714, -0.15949712, -0.1592578 ,\n",
       "        -0.16004926, -0.16160846, -0.1603062 , -0.15965884, -0.161568  ,\n",
       "        -0.16016586, -0.16113476, -0.16408372, -0.1595963 , -0.16224818,\n",
       "        -0.1395809 , -0.13979803, -0.14200668, -0.14047644, -0.13882288,\n",
       "        -0.1387316 , -0.14234573, -0.14067805, -0.13954476, -0.14285531,\n",
       "        -0.1434615 , -0.14211057, -0.14091937, -0.14197066, -0.14332115,\n",
       "        -0.14322453, -0.14281873, -0.14461635, -0.14489583, -0.14480815,\n",
       "        -0.14082835, -0.14105921, -0.14067592, -0.14207324, -0.14346656,\n",
       "        -0.14116902, -0.1441077 , -0.14175968, -0.14174798, -0.14529642,\n",
       "        -0.14287563, -0.14552746, -0.14130652, -0.14483437, -0.14225457,\n",
       "        -0.14249563, -0.14351998, -0.14473478, -0.14474861, -0.14478093,\n",
       "        -0.14054684, -0.14024629, -0.14180769, -0.14431399, -0.1410724 ,\n",
       "        -0.14241772, -0.14270785, -0.1412562 , -0.14295382, -0.14523523,\n",
       "        -0.14253207, -0.14314382, -0.14091544, -0.14406154, -0.14399574,\n",
       "        -0.14411813, -0.14307449, -0.14023979, -0.14586549, -0.1453087 ,\n",
       "        -0.15855363, -0.15910126, -0.15570379, -0.15805738, -0.15933872,\n",
       "        -0.15898372, -0.16046517, -0.15822193, -0.15690322, -0.15435993,\n",
       "        -0.15620012, -0.15533549, -0.15720224, -0.15386777, -0.15579498,\n",
       "        -0.15483994, -0.15582771, -0.15609241, -0.15826717, -0.1569426 ,\n",
       "        -0.14271297, -0.14086502, -0.14235921, -0.14066545, -0.14379768,\n",
       "        -0.14218425, -0.14244955, -0.14366368, -0.14323486, -0.14369009,\n",
       "        -0.1432272 , -0.14427519, -0.14314441, -0.14019174, -0.14142609,\n",
       "        -0.1441512 , -0.14252563, -0.1411736 , -0.14033092, -0.14389183,\n",
       "        -0.14347746, -0.14130419, -0.14216642, -0.14389919, -0.14321966,\n",
       "        -0.14093332, -0.14487646, -0.14394937, -0.14400908, -0.14690653,\n",
       "        -0.14153951, -0.14299677, -0.14511365, -0.14131093, -0.14011625,\n",
       "        -0.14416221, -0.14225537, -0.14456036, -0.14232396, -0.1408837 ,\n",
       "        -0.14367717, -0.14509326, -0.14066395, -0.14405603, -0.14284599,\n",
       "        -0.14580244, -0.14267097, -0.14555612, -0.14534348, -0.14446157,\n",
       "        -0.14262899, -0.14499441, -0.14220721, -0.14618178, -0.14162516,\n",
       "        -0.14547339, -0.14653538, -0.14423251, -0.14266647, -0.14012777,\n",
       "        -0.15202224, -0.1515221 , -0.15223728, -0.15313248, -0.15220867,\n",
       "        -0.1517731 , -0.15174466, -0.14908317, -0.15122782, -0.15159428,\n",
       "        -0.15206649, -0.14929473, -0.14960902, -0.15078146, -0.15129108,\n",
       "        -0.15288508, -0.15295829, -0.14773653, -0.14942079, -0.14982831,\n",
       "        -0.14472938, -0.14632047, -0.14064538, -0.14532078, -0.14316572,\n",
       "        -0.14520969, -0.1432116 , -0.14093403, -0.14319074, -0.14271569,\n",
       "        -0.14176009, -0.14287223, -0.1417261 , -0.14442821, -0.14658734,\n",
       "        -0.14406261, -0.14407491, -0.14259303, -0.14801399, -0.14462908,\n",
       "        -0.14233099, -0.14581455, -0.1460218 , -0.14426688, -0.14202861,\n",
       "        -0.14818256, -0.14521526, -0.14192491, -0.14323262, -0.14156034,\n",
       "        -0.14209313, -0.14468895, -0.14289261, -0.14575109, -0.14702186,\n",
       "        -0.14418236, -0.14676269, -0.13974264, -0.14581029, -0.14803455,\n",
       "        -0.14410031, -0.14547982, -0.14630009, -0.14814854, -0.14348985,\n",
       "        -0.14962219, -0.14688416, -0.14538103, -0.14745666, -0.14202981,\n",
       "        -0.14174431, -0.13981574, -0.14422035, -0.14949885, -0.14545197,\n",
       "        -0.14555758, -0.14853219, -0.1416724 , -0.14575155, -0.14705012,\n",
       "        -0.14729695, -0.14700806, -0.1472108 , -0.14650344, -0.14760484,\n",
       "        -0.14753975, -0.1478354 , -0.14652659, -0.14642951, -0.1484452 ,\n",
       "        -0.14668091, -0.14913733, -0.15046246, -0.1453413 , -0.15013303,\n",
       "        -0.14861978, -0.14867729, -0.14961729, -0.15047297, -0.14859227,\n",
       "        -0.13796985, -0.14160344, -0.13992083, -0.14495646, -0.14224186,\n",
       "        -0.14425913, -0.14470311, -0.14271038, -0.14062777, -0.14202658,\n",
       "        -0.14595145, -0.14193894, -0.14272291, -0.13966462, -0.14643131,\n",
       "        -0.143034  , -0.14541689, -0.14479895, -0.14428135, -0.14408209,\n",
       "        -0.14213468, -0.1440401 , -0.14642256, -0.1428267 , -0.14125948,\n",
       "        -0.14359574, -0.14122945, -0.14328235, -0.1478274 , -0.14195934,\n",
       "        -0.14516923, -0.1414749 , -0.14328705, -0.14096745, -0.13816346,\n",
       "        -0.14671389, -0.14497223, -0.14518224, -0.146152  , -0.14467298,\n",
       "        -0.14355548, -0.14592333, -0.1421407 , -0.14135193, -0.14095118,\n",
       "        -0.14234715, -0.1432102 , -0.14551066, -0.14841248, -0.14342546,\n",
       "        -0.14335246, -0.14187451, -0.14105312, -0.14178904, -0.14006903,\n",
       "        -0.14572685, -0.14532773, -0.14463472, -0.14488083, -0.14279973,\n",
       "        -0.14370977, -0.14345387, -0.14514208, -0.14329684, -0.14279409,\n",
       "        -0.14357124, -0.14334156, -0.14214496, -0.14288622, -0.14339729,\n",
       "        -0.1440477 , -0.14433311, -0.14410269, -0.14362919, -0.14318517,\n",
       "        -0.14304652, -0.14245258, -0.14531615, -0.14466055, -0.14326881,\n",
       "        -0.14079474, -0.13892974, -0.13814371, -0.13890701, -0.13915865,\n",
       "        -0.13879298, -0.14148006, -0.14071173, -0.14288219, -0.14355021,\n",
       "        -0.14174367, -0.13907159, -0.14148374, -0.14466213, -0.14270096,\n",
       "        -0.14227239, -0.14237441, -0.14029132, -0.14218126, -0.14283401,\n",
       "        -0.14410686, -0.14466081, -0.14491715, -0.14529599, -0.14281077,\n",
       "        -0.14008816, -0.14271663, -0.13817986, -0.13666026, -0.1405861 ,\n",
       "        -0.14204829, -0.14174546, -0.1417015 , -0.14141425, -0.13924439,\n",
       "        -0.14436241, -0.14304204, -0.14077319, -0.140286  , -0.14327584,\n",
       "        -0.13951696, -0.14192669, -0.14610679, -0.13905094, -0.14098181,\n",
       "        -0.14264123, -0.14209778, -0.14146792, -0.14046017, -0.14002359,\n",
       "        -0.14186989, -0.14234104, -0.14132258, -0.14116169, -0.14238742,\n",
       "        -0.1445802 , -0.1422995 , -0.13902649, -0.13935346, -0.14202388]),\n",
       " 'split4_test_score': array([-0.14738196, -0.1466121 , -0.14528146, -0.1448933 , -0.14628417,\n",
       "        -0.14617454, -0.14742828, -0.14840116, -0.14894537, -0.14747849,\n",
       "        -0.14810401, -0.14687102, -0.14693052, -0.14607229, -0.14565924,\n",
       "        -0.14438704, -0.14507994, -0.14853594, -0.14840729, -0.14755258,\n",
       "        -0.13482956, -0.13401179, -0.13405236, -0.13084339, -0.13588179,\n",
       "        -0.135611  , -0.13616967, -0.13506881, -0.13726889, -0.1349917 ,\n",
       "        -0.13715983, -0.13550692, -0.13233223, -0.1329442 , -0.13132445,\n",
       "        -0.13425305, -0.13461173, -0.13420088, -0.13358842, -0.13368916,\n",
       "        -0.13425822, -0.13413688, -0.13232779, -0.13147907, -0.13480165,\n",
       "        -0.13413824, -0.1340003 , -0.13424394, -0.13671494, -0.13718424,\n",
       "        -0.13547741, -0.13412746, -0.13296458, -0.13263223, -0.13153651,\n",
       "        -0.13260561, -0.13455622, -0.13336371, -0.13373931, -0.13387965,\n",
       "        -0.13490693, -0.13356012, -0.1327676 , -0.13173704, -0.13511923,\n",
       "        -0.13608659, -0.13429746, -0.13398959, -0.13460213, -0.13685665,\n",
       "        -0.13626485, -0.13463396, -0.13168808, -0.1314085 , -0.13124787,\n",
       "        -0.13325659, -0.13247723, -0.13494515, -0.13352126, -0.13343995,\n",
       "        -0.1428991 , -0.14052421, -0.14093369, -0.14098392, -0.14191342,\n",
       "        -0.14259913, -0.14200947, -0.14143581, -0.14329607, -0.14415258,\n",
       "        -0.14270945, -0.14322574, -0.14223918, -0.14326798, -0.14184251,\n",
       "        -0.14190464, -0.14160954, -0.14385312, -0.14261096, -0.14252217,\n",
       "        -0.13167026, -0.13368834, -0.13470026, -0.13407987, -0.13412757,\n",
       "        -0.13106821, -0.13258717, -0.13118139, -0.13392322, -0.13285999,\n",
       "        -0.13211981, -0.13435551, -0.1340084 , -0.13542341, -0.13286992,\n",
       "        -0.13411558, -0.13356864, -0.13481667, -0.13085021, -0.13205841,\n",
       "        -0.13335717, -0.13433909, -0.13510155, -0.13467517, -0.13539133,\n",
       "        -0.13311047, -0.1315127 , -0.13055852, -0.1340041 , -0.13052314,\n",
       "        -0.13428183, -0.13231647, -0.13228304, -0.13507084, -0.13365441,\n",
       "        -0.13440589, -0.13644469, -0.13591001, -0.13174952, -0.13217247,\n",
       "        -0.1343878 , -0.13347436, -0.13477877, -0.13423641, -0.1333324 ,\n",
       "        -0.13357108, -0.13192928, -0.12966384, -0.13118809, -0.13163668,\n",
       "        -0.13275224, -0.13326692, -0.13269471, -0.13496777, -0.13426169,\n",
       "        -0.13231186, -0.13575162, -0.13424154, -0.13241233, -0.13180735,\n",
       "        -0.13761578, -0.13779915, -0.13839483, -0.14044584, -0.14103638,\n",
       "        -0.13908875, -0.1402869 , -0.13765854, -0.13834393, -0.13834989,\n",
       "        -0.13819234, -0.1381825 , -0.1389833 , -0.13875596, -0.13784684,\n",
       "        -0.13813867, -0.13776615, -0.13854923, -0.13919035, -0.13953355,\n",
       "        -0.13281983, -0.13268931, -0.13343463, -0.13448496, -0.13426354,\n",
       "        -0.134528  , -0.13145353, -0.13387629, -0.13322603, -0.13415375,\n",
       "        -0.13349351, -0.13031173, -0.13472502, -0.1332086 , -0.13358597,\n",
       "        -0.13296316, -0.13278607, -0.13221113, -0.13265265, -0.13143098,\n",
       "        -0.13353479, -0.13234845, -0.12996638, -0.13212602, -0.13055041,\n",
       "        -0.13330181, -0.13408109, -0.13375626, -0.13425739, -0.13428615,\n",
       "        -0.13335251, -0.13345649, -0.13260187, -0.13068886, -0.13394836,\n",
       "        -0.1336532 , -0.13243364, -0.13205587, -0.13229663, -0.13157754,\n",
       "        -0.13267163, -0.13070784, -0.13059181, -0.13144009, -0.1321525 ,\n",
       "        -0.13256712, -0.13308748, -0.13281599, -0.13394225, -0.13403259,\n",
       "        -0.13374423, -0.13205644, -0.1308102 , -0.13030872, -0.13395947,\n",
       "        -0.13296715, -0.13174291, -0.13331421, -0.13086619, -0.13256947,\n",
       "        -0.13671791, -0.13689872, -0.13570731, -0.13628914, -0.1376012 ,\n",
       "        -0.13840392, -0.13724372, -0.13819642, -0.13750188, -0.13720869,\n",
       "        -0.13476797, -0.13641263, -0.13655657, -0.1353342 , -0.1371105 ,\n",
       "        -0.13593355, -0.13619025, -0.13552029, -0.13411838, -0.13459513,\n",
       "        -0.1319359 , -0.13183933, -0.13160067, -0.13202568, -0.13153567,\n",
       "        -0.13364094, -0.13369538, -0.13350402, -0.1318229 , -0.13166385,\n",
       "        -0.13117487, -0.1312251 , -0.13100969, -0.13289908, -0.1339285 ,\n",
       "        -0.13585369, -0.13248897, -0.13480735, -0.13373593, -0.13455642,\n",
       "        -0.13079006, -0.13052301, -0.13148716, -0.13309013, -0.13062732,\n",
       "        -0.1332268 , -0.13206564, -0.1318389 , -0.13170098, -0.13240616,\n",
       "        -0.13150152, -0.13162078, -0.13149221, -0.13061369, -0.13453404,\n",
       "        -0.13404241, -0.13410522, -0.13394529, -0.13398179, -0.13233071,\n",
       "        -0.13277316, -0.1334375 , -0.13381034, -0.13163104, -0.13048542,\n",
       "        -0.1332638 , -0.1328173 , -0.13324642, -0.13316536, -0.13134045,\n",
       "        -0.13106193, -0.13125212, -0.13139054, -0.13378625, -0.13543248,\n",
       "        -0.13229801, -0.13359227, -0.13436248, -0.13536663, -0.13362662,\n",
       "        -0.13751554, -0.13337305, -0.13677297, -0.13872587, -0.13827979,\n",
       "        -0.13647289, -0.13655629, -0.13534341, -0.1368178 , -0.1360641 ,\n",
       "        -0.13546319, -0.13575738, -0.13572559, -0.13602534, -0.13559427,\n",
       "        -0.13604074, -0.13615415, -0.13641899, -0.13649749, -0.13477471,\n",
       "        -0.13321516, -0.13307232, -0.13050031, -0.13144998, -0.12876897,\n",
       "        -0.13082975, -0.13428502, -0.13258706, -0.13384477, -0.13302606,\n",
       "        -0.13279721, -0.13354171, -0.13328313, -0.13318962, -0.13134857,\n",
       "        -0.13167252, -0.13264962, -0.13301505, -0.13129075, -0.13048245,\n",
       "        -0.13415474, -0.13471637, -0.1302797 , -0.12955891, -0.1315236 ,\n",
       "        -0.13127382, -0.13149315, -0.13129722, -0.13264907, -0.13301998,\n",
       "        -0.13435182, -0.13341339, -0.13142752, -0.13141606, -0.13114898,\n",
       "        -0.13003575, -0.13007595, -0.13075377, -0.133569  , -0.13270278,\n",
       "        -0.13455177, -0.13418859, -0.12940334, -0.13088212, -0.12980524,\n",
       "        -0.1318328 , -0.13105403, -0.13235446, -0.1335288 , -0.1339347 ,\n",
       "        -0.13370756, -0.13383385, -0.13137272, -0.13308645, -0.13152297,\n",
       "        -0.13039195, -0.13040406, -0.13211602, -0.13342602, -0.13260225]),\n",
       " 'mean_test_score': array([-0.15428573, -0.15433615, -0.15381945, -0.15394799, -0.15335564,\n",
       "        -0.15305804, -0.15432086, -0.1542006 , -0.15437177, -0.15393375,\n",
       "        -0.15337455, -0.15378635, -0.15304579, -0.15376086, -0.15309609,\n",
       "        -0.15392642, -0.15480488, -0.15551214, -0.15473391, -0.15465837,\n",
       "        -0.13518654, -0.13528295, -0.13591044, -0.13485941, -0.13444748,\n",
       "        -0.13541903, -0.13649899, -0.13487665, -0.13598721, -0.13637531,\n",
       "        -0.13712396, -0.13609473, -0.13587221, -0.13575448, -0.13505821,\n",
       "        -0.13634987, -0.13648501, -0.13616394, -0.13618353, -0.13618058,\n",
       "        -0.13340646, -0.13429595, -0.13347287, -0.13471281, -0.13511216,\n",
       "        -0.13444043, -0.13576585, -0.13654438, -0.13513177, -0.13653848,\n",
       "        -0.13640809, -0.13516725, -0.13606983, -0.13531952, -0.13489184,\n",
       "        -0.13527091, -0.13544537, -0.13635305, -0.13516018, -0.13576764,\n",
       "        -0.13287298, -0.13404161, -0.13443353, -0.13521285, -0.13423038,\n",
       "        -0.13557803, -0.13618654, -0.13592006, -0.13487107, -0.13619319,\n",
       "        -0.13617797, -0.13561597, -0.13557941, -0.1345146 , -0.13468091,\n",
       "        -0.13548145, -0.13522406, -0.13569911, -0.13648131, -0.13698422,\n",
       "        -0.14963377, -0.14888289, -0.14908108, -0.14900516, -0.14865173,\n",
       "        -0.14912661, -0.15003076, -0.14939013, -0.14904016, -0.14958798,\n",
       "        -0.14877476, -0.14939338, -0.14858054, -0.14819507, -0.14936702,\n",
       "        -0.14884546, -0.14797227, -0.14914523, -0.1484073 , -0.14884778,\n",
       "        -0.13662806, -0.13606303, -0.13750288, -0.13717829, -0.1377141 ,\n",
       "        -0.13499365, -0.13595103, -0.13634261, -0.13560792, -0.13633468,\n",
       "        -0.13648562, -0.13679227, -0.13597037, -0.13582137, -0.13582421,\n",
       "        -0.13712333, -0.1341523 , -0.13540497, -0.13502641, -0.13435687,\n",
       "        -0.13459501, -0.13572549, -0.13730835, -0.13638244, -0.13666387,\n",
       "        -0.13518759, -0.13512658, -0.13459755, -0.13568411, -0.1368269 ,\n",
       "        -0.13636412, -0.13474633, -0.13510153, -0.13454215, -0.13608042,\n",
       "        -0.13526846, -0.13442816, -0.13542042, -0.13487395, -0.1350396 ,\n",
       "        -0.13460642, -0.13551695, -0.13589605, -0.13559772, -0.1358425 ,\n",
       "        -0.13645747, -0.13510651, -0.13572947, -0.13564661, -0.13542616,\n",
       "        -0.13465014, -0.13569937, -0.13393217, -0.13677967, -0.1355431 ,\n",
       "        -0.13383258, -0.13480635, -0.13451083, -0.13446749, -0.13475343,\n",
       "        -0.1437198 , -0.14329773, -0.14392989, -0.14397162, -0.14403593,\n",
       "        -0.14381553, -0.14386478, -0.14289138, -0.14363439, -0.14346386,\n",
       "        -0.14349533, -0.14283672, -0.14359994, -0.14449005, -0.14395023,\n",
       "        -0.144622  , -0.14503719, -0.14403364, -0.14427764, -0.14419628,\n",
       "        -0.13558095, -0.13395563, -0.13309422, -0.13494938, -0.13506528,\n",
       "        -0.13521131, -0.13290245, -0.13328787, -0.13471119, -0.13451397,\n",
       "        -0.13502173, -0.13346435, -0.13475176, -0.13509862, -0.1358393 ,\n",
       "        -0.1363447 , -0.13489083, -0.13527567, -0.13700816, -0.1356979 ,\n",
       "        -0.13480263, -0.13363696, -0.13323929, -0.13399115, -0.1323545 ,\n",
       "        -0.13433875, -0.13419393, -0.13416109, -0.1346134 , -0.1351795 ,\n",
       "        -0.13517943, -0.13380733, -0.13422003, -0.13441078, -0.13449213,\n",
       "        -0.13458429, -0.13514473, -0.13430411, -0.13533271, -0.13500593,\n",
       "        -0.13564703, -0.13410292, -0.13362356, -0.13418202, -0.13319352,\n",
       "        -0.13507583, -0.13444063, -0.1337374 , -0.13537891, -0.13429315,\n",
       "        -0.13432696, -0.13306718, -0.13419077, -0.13503053, -0.13430387,\n",
       "        -0.13444726, -0.13537679, -0.13501004, -0.13543649, -0.13519608,\n",
       "        -0.14109981, -0.13965088, -0.14044492, -0.14024267, -0.14022166,\n",
       "        -0.14147426, -0.14006047, -0.14008221, -0.13996648, -0.14112469,\n",
       "        -0.13932855, -0.14103513, -0.14156403, -0.13929068, -0.14080433,\n",
       "        -0.14098958, -0.1415112 , -0.14089763, -0.14101673, -0.14179011,\n",
       "        -0.13276251, -0.13430202, -0.1323038 , -0.13481292, -0.13443674,\n",
       "        -0.13452921, -0.13433362, -0.13544616, -0.133845  , -0.13456421,\n",
       "        -0.13539938, -0.13479779, -0.13536615, -0.13461043, -0.13572886,\n",
       "        -0.13737852, -0.13701189, -0.13731761, -0.13685495, -0.13651213,\n",
       "        -0.13500361, -0.13490225, -0.13491556, -0.13450247, -0.13441068,\n",
       "        -0.13625216, -0.13430866, -0.13411514, -0.13565339, -0.13485379,\n",
       "        -0.13540696, -0.1349114 , -0.13534944, -0.1335025 , -0.13636442,\n",
       "        -0.13630004, -0.1359286 , -0.13589077, -0.13654787, -0.13507402,\n",
       "        -0.1359106 , -0.1356761 , -0.13598214, -0.13422834, -0.13286685,\n",
       "        -0.13548497, -0.13533892, -0.13503532, -0.13589629, -0.13497924,\n",
       "        -0.13536215, -0.13459163, -0.13514367, -0.13440086, -0.13564926,\n",
       "        -0.13651816, -0.13607043, -0.13706458, -0.13718226, -0.13662187,\n",
       "        -0.14012353, -0.13950994, -0.14020332, -0.14171991, -0.14022184,\n",
       "        -0.13817615, -0.1389028 , -0.13797784, -0.13895941, -0.13842763,\n",
       "        -0.13930953, -0.1393632 , -0.13773227, -0.13885053, -0.1384582 ,\n",
       "        -0.13781482, -0.13852889, -0.13946926, -0.13967603, -0.13905979,\n",
       "        -0.13464832, -0.13326157, -0.13210291, -0.1333679 , -0.13256434,\n",
       "        -0.13371515, -0.13523716, -0.13514767, -0.13602711, -0.1348031 ,\n",
       "        -0.13465641, -0.13582737, -0.13509404, -0.13602977, -0.13501577,\n",
       "        -0.1345534 , -0.13566321, -0.13580854, -0.13599232, -0.13514449,\n",
       "        -0.1355296 , -0.1348016 , -0.13347617, -0.13461093, -0.13473409,\n",
       "        -0.1331569 , -0.13403169, -0.13191365, -0.13375374, -0.1332141 ,\n",
       "        -0.13526757, -0.13483136, -0.13523415, -0.13359065, -0.13351101,\n",
       "        -0.13431284, -0.13435209, -0.13335703, -0.13393583, -0.13445638,\n",
       "        -0.13591208, -0.13481295, -0.13303862, -0.13408784, -0.13356748,\n",
       "        -0.13387942, -0.13351225, -0.13406399, -0.1353171 , -0.13395702,\n",
       "        -0.13514545, -0.13562361, -0.13477404, -0.13534808, -0.13425345,\n",
       "        -0.13562231, -0.13372147, -0.13384377, -0.13432004, -0.13432042]),\n",
       " 'std_test_score': array([0.00791028, 0.006966  , 0.0077364 , 0.00761199, 0.00690886,\n",
       "        0.00590753, 0.00648061, 0.00628948, 0.00638347, 0.00658285,\n",
       "        0.00673813, 0.00744329, 0.00654928, 0.00694427, 0.00727718,\n",
       "        0.00712601, 0.00798381, 0.00732695, 0.00725919, 0.00721289,\n",
       "        0.00275167, 0.00257773, 0.0031047 , 0.00328621, 0.00256492,\n",
       "        0.00192105, 0.00309289, 0.00315078, 0.00237377, 0.00374083,\n",
       "        0.00329716, 0.00310786, 0.00284523, 0.00336003, 0.00425339,\n",
       "        0.00362578, 0.00322996, 0.00425941, 0.0044818 , 0.00447243,\n",
       "        0.00469641, 0.00374498, 0.00421023, 0.00376096, 0.00445862,\n",
       "        0.00444613, 0.00428321, 0.00308705, 0.00399065, 0.00486793,\n",
       "        0.00411668, 0.00525669, 0.00299895, 0.0050997 , 0.00392911,\n",
       "        0.003908  , 0.00411104, 0.00423251, 0.00497241, 0.00452543,\n",
       "        0.00493286, 0.00343597, 0.00421762, 0.00496485, 0.00413056,\n",
       "        0.00414812, 0.00328863, 0.00344741, 0.00485952, 0.00491434,\n",
       "        0.00364026, 0.00397632, 0.00299324, 0.00537803, 0.00477469,\n",
       "        0.00437019, 0.0040073 , 0.00235989, 0.00477845, 0.0043224 ,\n",
       "        0.00587008, 0.00673638, 0.00615027, 0.00635834, 0.00645626,\n",
       "        0.0073429 , 0.00717644, 0.00657404, 0.00639284, 0.00499068,\n",
       "        0.00591528, 0.00563226, 0.00639885, 0.00505622, 0.00629276,\n",
       "        0.00562678, 0.00600273, 0.00623951, 0.00691056, 0.00700793,\n",
       "        0.00366727, 0.00313157, 0.00288535, 0.00210249, 0.00386572,\n",
       "        0.0037895 , 0.00350885, 0.00416278, 0.00417195, 0.0044108 ,\n",
       "        0.00390725, 0.00410042, 0.00387313, 0.00267207, 0.00353906,\n",
       "        0.00374541, 0.00454026, 0.00299921, 0.00315348, 0.00484446,\n",
       "        0.00478551, 0.00321296, 0.00279277, 0.0038983 , 0.00409459,\n",
       "        0.00367379, 0.00515524, 0.00538784, 0.00436863, 0.00546267,\n",
       "        0.0034684 , 0.00432484, 0.00576468, 0.00522182, 0.0023302 ,\n",
       "        0.00453983, 0.00474455, 0.00479468, 0.00384402, 0.0035792 ,\n",
       "        0.00503082, 0.00509849, 0.00347319, 0.00494619, 0.00386001,\n",
       "        0.00520665, 0.00447991, 0.00611677, 0.00541434, 0.00493498,\n",
       "        0.00430489, 0.00512169, 0.00462213, 0.00595144, 0.0031909 ,\n",
       "        0.00586651, 0.00627507, 0.00506642, 0.00411695, 0.00348224,\n",
       "        0.00596754, 0.00548943, 0.0053832 , 0.00558993, 0.00497838,\n",
       "        0.00543595, 0.00498369, 0.00525616, 0.00528851, 0.00564725,\n",
       "        0.00613112, 0.00474494, 0.00512574, 0.0053362 , 0.00547602,\n",
       "        0.00605005, 0.00628782, 0.00478956, 0.00534111, 0.00548972,\n",
       "        0.0048332 , 0.0064503 , 0.00475224, 0.00601019, 0.00464248,\n",
       "        0.00566806, 0.00662116, 0.00589211, 0.00605409, 0.004378  ,\n",
       "        0.00369446, 0.00506556, 0.00371051, 0.00505646, 0.00650351,\n",
       "        0.00464284, 0.0046996 , 0.00401307, 0.00572877, 0.00465421,\n",
       "        0.00436744, 0.00667432, 0.00656391, 0.00657309, 0.00553788,\n",
       "        0.00742705, 0.00786474, 0.00534412, 0.00610517, 0.00337233,\n",
       "        0.00407556, 0.00561424, 0.00485927, 0.00574487, 0.00691913,\n",
       "        0.0050903 , 0.00613672, 0.00346145, 0.00541368, 0.00658156,\n",
       "        0.00448978, 0.00620356, 0.00669977, 0.00803345, 0.00600337,\n",
       "        0.00775673, 0.00744521, 0.00653518, 0.00727565, 0.00463487,\n",
       "        0.00441383, 0.00395744, 0.00524318, 0.00741456, 0.00592242,\n",
       "        0.00573398, 0.00706702, 0.00386075, 0.00558744, 0.00603768,\n",
       "        0.004021  , 0.00485817, 0.00410427, 0.00415779, 0.00402198,\n",
       "        0.00348947, 0.00494898, 0.00438862, 0.00414927, 0.00457589,\n",
       "        0.0050616 , 0.00472671, 0.0049689 , 0.00456102, 0.00573066,\n",
       "        0.005846  , 0.00487001, 0.00504626, 0.00544384, 0.00508134,\n",
       "        0.00306125, 0.00392516, 0.00391616, 0.00516969, 0.00408183,\n",
       "        0.00502555, 0.00561433, 0.00401404, 0.00438271, 0.00412835,\n",
       "        0.00567312, 0.0043865 , 0.00399478, 0.00265238, 0.0058707 ,\n",
       "        0.00291191, 0.00446649, 0.00385507, 0.00396252, 0.00396119,\n",
       "        0.00423716, 0.00492433, 0.00600374, 0.00435716, 0.00428286,\n",
       "        0.00385176, 0.00381006, 0.00489958, 0.0061077 , 0.00395872,\n",
       "        0.00499597, 0.00425646, 0.00418652, 0.0041426 , 0.00260318,\n",
       "        0.00521543, 0.00472678, 0.00515621, 0.0052229 , 0.00499834,\n",
       "        0.00446615, 0.00515443, 0.00313814, 0.00388653, 0.00450223,\n",
       "        0.00344549, 0.00410456, 0.00534292, 0.00660217, 0.00436612,\n",
       "        0.00431998, 0.00381199, 0.00340598, 0.00377577, 0.00424307,\n",
       "        0.00485318, 0.00510633, 0.00403354, 0.00445409, 0.00322651,\n",
       "        0.00414131, 0.00439167, 0.00375219, 0.00171912, 0.00253875,\n",
       "        0.00380093, 0.00377843, 0.00362383, 0.00306666, 0.00310484,\n",
       "        0.00353675, 0.00364328, 0.00368945, 0.00300708, 0.00323375,\n",
       "        0.00359321, 0.00308126, 0.00384268, 0.0035173 , 0.00353571,\n",
       "        0.00331961, 0.00303116, 0.00374018, 0.00456547, 0.00367745,\n",
       "        0.0036641 , 0.00363609, 0.003216  , 0.00379806, 0.0054752 ,\n",
       "        0.00375557, 0.00225808, 0.00359406, 0.0043373 , 0.00400865,\n",
       "        0.00406015, 0.00370116, 0.0024265 , 0.00357463, 0.00411975,\n",
       "        0.0048941 , 0.00508255, 0.00619734, 0.00580662, 0.00473076,\n",
       "        0.00370063, 0.00458483, 0.00354159, 0.00306872, 0.00505393,\n",
       "        0.0043386 , 0.00375292, 0.00380726, 0.00407353, 0.00302465,\n",
       "        0.00519433, 0.00597698, 0.00438847, 0.0032641 , 0.00451788,\n",
       "        0.00305361, 0.00364701, 0.00723836, 0.00281961, 0.00391943,\n",
       "        0.00447587, 0.00454884, 0.00386876, 0.0038343 , 0.0042781 ,\n",
       "        0.00395271, 0.00377499, 0.00389508, 0.00312816, 0.00442695,\n",
       "        0.00536075, 0.00464749, 0.00369844, 0.00256939, 0.00391558]),\n",
       " 'rank_test_score': array([393, 395, 388, 391, 384, 382, 394, 392, 396, 390, 385, 387, 381,\n",
       "        386, 383, 389, 399, 400, 398, 397, 165, 177, 236, 124,  84, 191,\n",
       "        275, 127, 244, 268, 293, 252, 232, 223, 145, 264, 273, 253, 256,\n",
       "        255,  21,  62,  23, 109, 153,  81, 224, 279, 155, 278, 270, 162,\n",
       "        249, 179, 129, 175, 195, 265, 161, 225,   8,  47,  79, 169,  59,\n",
       "        202, 257, 239, 125, 258, 254, 207, 203,  91, 107, 197, 170, 218,\n",
       "        272, 288, 379, 369, 372, 370, 365, 373, 380, 376, 371, 378, 366,\n",
       "        377, 364, 362, 375, 367, 361, 374, 363, 368, 282, 248, 299, 294,\n",
       "        300, 135, 241, 262, 206, 261, 274, 285, 242, 227, 228, 292,  52,\n",
       "        189, 141,  74,  98, 220, 296, 269, 283, 166, 154,  99, 216, 286,\n",
       "        266, 111, 151,  93, 251, 174,  78, 192, 126, 144, 100, 199, 234,\n",
       "        205, 231, 271, 152, 222, 210, 193, 105, 219,  41, 284, 201,  37,\n",
       "        119,  89,  86, 113, 348, 343, 351, 353, 355, 349, 350, 342, 347,\n",
       "        344, 345, 341, 346, 358, 352, 359, 360, 354, 357, 356, 204,  43,\n",
       "         12, 133, 146, 168,   9,  18, 108,  90, 140,  22, 112, 150, 230,\n",
       "        263, 128, 176, 289, 217, 117,  31,  16,  45,   4,  72,  56,  53,\n",
       "        103, 164, 163,  36,  57,  77,  87,  96, 158,  65, 180, 137, 211,\n",
       "         50,  30,  54,  14, 148,  82,  34, 187,  61,  70,  11,  55, 142,\n",
       "         64,  83, 186, 138, 194, 167, 334, 318, 328, 327, 325, 336, 321,\n",
       "        322, 320, 335, 314, 333, 338, 312, 329, 331, 337, 330, 332, 340,\n",
       "          6,  63,   3, 120,  80,  92,  71, 196,  39,  95, 188, 115, 185,\n",
       "        101, 221, 298, 290, 297, 287, 276, 136, 130, 132,  88,  76, 259,\n",
       "         66,  51, 213, 123, 190, 131, 183,  25, 267, 260, 240, 233, 280,\n",
       "        147, 237, 215, 243,  58,   7, 198, 181, 143, 235, 134, 184,  97,\n",
       "        156,  75, 212, 277, 250, 291, 295, 281, 323, 317, 324, 339, 326,\n",
       "        304, 309, 303, 310, 305, 313, 315, 301, 308, 306, 302, 307, 316,\n",
       "        319, 311, 104,  17,   2,  20,   5,  32, 172, 160, 246, 118, 106,\n",
       "        229, 149, 247, 139,  94, 214, 226, 245, 157, 200, 116,  24, 102,\n",
       "        110,  13,  46,   1,  35,  15, 173, 122, 171,  29,  26,  67,  73,\n",
       "         19,  42,  85, 238, 121,  10,  49,  28,  40,  27,  48, 178,  44,\n",
       "        159, 209, 114, 182,  60, 208,  33,  38,  68,  69]),\n",
       " 'split0_train_score': array([-0.14040897, -0.1401231 , -0.13995635, -0.1401529 , -0.13960304,\n",
       "        -0.13983168, -0.13969163, -0.13995327, -0.14054783, -0.14006255,\n",
       "        -0.13963222, -0.13957518, -0.14011134, -0.14025883, -0.14012918,\n",
       "        -0.13941127, -0.1393804 , -0.13968449, -0.13996782, -0.13984338,\n",
       "        -0.11742563, -0.11794787, -0.11781537, -0.11766076, -0.11672628,\n",
       "        -0.11683899, -0.11750539, -0.11752284, -0.11712894, -0.11691495,\n",
       "        -0.1192905 , -0.11786325, -0.11686578, -0.1172709 , -0.11714888,\n",
       "        -0.11721525, -0.11794415, -0.1175957 , -0.11809948, -0.11764358,\n",
       "        -0.1149993 , -0.11447324, -0.1143775 , -0.11409765, -0.11413163,\n",
       "        -0.11333887, -0.11476597, -0.11444282, -0.11529454, -0.11335639,\n",
       "        -0.11425139, -0.11458942, -0.11491629, -0.11396523, -0.11380301,\n",
       "        -0.11506802, -0.11573588, -0.11561216, -0.11518144, -0.11520451,\n",
       "        -0.11330971, -0.11453261, -0.11433609, -0.11304058, -0.11276241,\n",
       "        -0.11294084, -0.11432205, -0.11281386, -0.11267346, -0.11388986,\n",
       "        -0.11404607, -0.11418574, -0.11257629, -0.1128568 , -0.11353967,\n",
       "        -0.11396824, -0.11459491, -0.11545644, -0.11572732, -0.1155971 ,\n",
       "        -0.13458501, -0.13443978, -0.13369199, -0.13465681, -0.13463145,\n",
       "        -0.13424805, -0.13451632, -0.13491476, -0.13315697, -0.13578366,\n",
       "        -0.13249809, -0.13366002, -0.13226571, -0.13202026, -0.13416605,\n",
       "        -0.13422386, -0.13334978, -0.13376501, -0.13324373, -0.13332184,\n",
       "        -0.11873413, -0.11841901, -0.11811752, -0.11770181, -0.11657213,\n",
       "        -0.11724925, -0.11673818, -0.11885979, -0.11544608, -0.11771943,\n",
       "        -0.11685996, -0.11742188, -0.11644572, -0.11499042, -0.11689209,\n",
       "        -0.11604749, -0.11547683, -0.11702956, -0.11701544, -0.11690255,\n",
       "        -0.11575593, -0.11578625, -0.11475514, -0.11582593, -0.11467289,\n",
       "        -0.11483294, -0.11431549, -0.11447016, -0.11355481, -0.1158512 ,\n",
       "        -0.1137113 , -0.11338082, -0.11287265, -0.11320413, -0.11553695,\n",
       "        -0.11196248, -0.1145333 , -0.11500896, -0.11516575, -0.11575339,\n",
       "        -0.11568702, -0.11516485, -0.11328717, -0.11452112, -0.11334685,\n",
       "        -0.11400298, -0.11414773, -0.11269772, -0.11230844, -0.11437208,\n",
       "        -0.11205407, -0.1128957 , -0.11181438, -0.11240235, -0.11450699,\n",
       "        -0.11179763, -0.11356518, -0.11397334, -0.11441556, -0.11472238,\n",
       "        -0.13020091, -0.12911601, -0.12974974, -0.12848179, -0.12811845,\n",
       "        -0.1280362 , -0.12819069, -0.12860858, -0.12877982, -0.12874633,\n",
       "        -0.12885822, -0.12914888, -0.12949201, -0.12953673, -0.12880236,\n",
       "        -0.12935605, -0.12937904, -0.1294096 , -0.1293363 , -0.12967537,\n",
       "        -0.11917851, -0.11588514, -0.1173172 , -0.11671435, -0.1176735 ,\n",
       "        -0.11675859, -0.11682438, -0.11729514, -0.11732562, -0.11699325,\n",
       "        -0.11718756, -0.11604272, -0.11590445, -0.11690495, -0.11605062,\n",
       "        -0.11598871, -0.11699914, -0.11732992, -0.11830949, -0.1167853 ,\n",
       "        -0.11616116, -0.115175  , -0.11654427, -0.11522908, -0.11413912,\n",
       "        -0.11247574, -0.11375227, -0.11361596, -0.11346559, -0.11358736,\n",
       "        -0.11473145, -0.11454455, -0.1145234 , -0.11575908, -0.11419433,\n",
       "        -0.11349025, -0.11223717, -0.1147953 , -0.11566077, -0.11507516,\n",
       "        -0.11512748, -0.11343692, -0.11678145, -0.11349474, -0.11332005,\n",
       "        -0.11332578, -0.11468525, -0.11346944, -0.11326288, -0.1117465 ,\n",
       "        -0.11270425, -0.11223773, -0.11311991, -0.11379356, -0.1131624 ,\n",
       "        -0.11441476, -0.11272871, -0.11414593, -0.11375218, -0.11529041,\n",
       "        -0.12516121, -0.12581334, -0.12600368, -0.12543236, -0.12465031,\n",
       "        -0.12583066, -0.12600623, -0.12600866, -0.12637785, -0.12681907,\n",
       "        -0.126245  , -0.12574238, -0.12572911, -0.12576346, -0.12698788,\n",
       "        -0.12626367, -0.1262863 , -0.12556513, -0.12579748, -0.12725484,\n",
       "        -0.11619373, -0.11782532, -0.11695533, -0.11699446, -0.11688523,\n",
       "        -0.1166449 , -0.11771386, -0.11654521, -0.11747532, -0.11719913,\n",
       "        -0.11759083, -0.11745019, -0.11712546, -0.11739499, -0.11855785,\n",
       "        -0.11732584, -0.11876944, -0.11781542, -0.11773586, -0.1173389 ,\n",
       "        -0.11485801, -0.11474892, -0.11449853, -0.11465199, -0.1146434 ,\n",
       "        -0.11461106, -0.11489066, -0.11491631, -0.1151818 , -0.11545053,\n",
       "        -0.11668004, -0.11456504, -0.11685744, -0.11605258, -0.11554591,\n",
       "        -0.11599616, -0.11568224, -0.11587417, -0.11598815, -0.11531137,\n",
       "        -0.11375453, -0.11558479, -0.11301023, -0.11509843, -0.11221508,\n",
       "        -0.11488933, -0.11466691, -0.11387765, -0.11268704, -0.11400928,\n",
       "        -0.11407473, -0.1140567 , -0.11466441, -0.11467968, -0.11442585,\n",
       "        -0.11469424, -0.11359818, -0.11470461, -0.11451875, -0.11556857,\n",
       "        -0.1229988 , -0.12360884, -0.12512624, -0.1255292 , -0.1243915 ,\n",
       "        -0.12348004, -0.12386469, -0.12434056, -0.1245871 , -0.12517303,\n",
       "        -0.12463802, -0.12453441, -0.12525401, -0.12449845, -0.12467228,\n",
       "        -0.12508741, -0.12496687, -0.12501313, -0.12550294, -0.12503915,\n",
       "        -0.11613695, -0.11626385, -0.11739968, -0.11758685, -0.11715859,\n",
       "        -0.11906799, -0.11877165, -0.11873041, -0.11875967, -0.11793497,\n",
       "        -0.11788476, -0.11813707, -0.11743885, -0.11821132, -0.11770217,\n",
       "        -0.11808394, -0.11917804, -0.11867865, -0.11901374, -0.11842946,\n",
       "        -0.11513621, -0.11341012, -0.11422114, -0.11406106, -0.11448022,\n",
       "        -0.11493104, -0.11594559, -0.11479241, -0.11660585, -0.11430238,\n",
       "        -0.11568262, -0.11510644, -0.11557086, -0.11481067, -0.11606238,\n",
       "        -0.11553158, -0.11581726, -0.1157022 , -0.11619256, -0.11628865,\n",
       "        -0.11428831, -0.11342588, -0.11341369, -0.11315488, -0.11287052,\n",
       "        -0.11504223, -0.11539876, -0.11495853, -0.11547139, -0.11503595,\n",
       "        -0.11507254, -0.11465096, -0.11641171, -0.1133985 , -0.11411437,\n",
       "        -0.11497974, -0.1164155 , -0.11532561, -0.11630088, -0.11606435]),\n",
       " 'split1_train_score': array([-0.13949213, -0.140056  , -0.13913635, -0.13914116, -0.13917237,\n",
       "        -0.13926659, -0.13943016, -0.13914346, -0.13887601, -0.13944422,\n",
       "        -0.1388246 , -0.13883807, -0.13888583, -0.14020532, -0.14032753,\n",
       "        -0.13984159, -0.13997355, -0.13990581, -0.139768  , -0.13914854,\n",
       "        -0.11754658, -0.11785722, -0.11816504, -0.1159781 , -0.11810959,\n",
       "        -0.11727816, -0.11764506, -0.11764348, -0.11700767, -0.11753367,\n",
       "        -0.11778667, -0.11771764, -0.11831436, -0.11794343, -0.1189083 ,\n",
       "        -0.11819295, -0.11870956, -0.11822887, -0.11850793, -0.11852531,\n",
       "        -0.11506519, -0.11509494, -0.11448277, -0.11463614, -0.11516924,\n",
       "        -0.11484303, -0.11487485, -0.11352925, -0.11411887, -0.11390026,\n",
       "        -0.1159856 , -0.1153128 , -0.11540944, -0.11520999, -0.11518177,\n",
       "        -0.11369239, -0.11514873, -0.11504908, -0.1146153 , -0.11456494,\n",
       "        -0.1136558 , -0.113135  , -0.11375705, -0.11309627, -0.11437547,\n",
       "        -0.1126582 , -0.11342968, -0.11327767, -0.11350058, -0.11337446,\n",
       "        -0.1148971 , -0.11380468, -0.11406462, -0.11472444, -0.11421785,\n",
       "        -0.11304508, -0.11525801, -0.11472189, -0.11406873, -0.1139733 ,\n",
       "        -0.13415324, -0.13481548, -0.13561914, -0.13586796, -0.13565685,\n",
       "        -0.13361744, -0.13404189, -0.13426794, -0.13507073, -0.13452751,\n",
       "        -0.13451725, -0.13469764, -0.13484875, -0.13455273, -0.1346043 ,\n",
       "        -0.135758  , -0.135202  , -0.13494792, -0.13485943, -0.13529494,\n",
       "        -0.11761503, -0.11956163, -0.11925643, -0.11854053, -0.12008   ,\n",
       "        -0.1164875 , -0.11601149, -0.11764901, -0.11676631, -0.1182515 ,\n",
       "        -0.11834228, -0.11761549, -0.11798446, -0.1177442 , -0.11822837,\n",
       "        -0.11879499, -0.11843441, -0.11818827, -0.11914001, -0.11871762,\n",
       "        -0.11412005, -0.11714825, -0.11676863, -0.11563692, -0.11756   ,\n",
       "        -0.11467715, -0.11407176, -0.11507239, -0.11471096, -0.1150849 ,\n",
       "        -0.11504783, -0.11509617, -0.11492506, -0.11546765, -0.11508532,\n",
       "        -0.11520943, -0.11612206, -0.1160113 , -0.1165785 , -0.11662479,\n",
       "        -0.11421315, -0.11667504, -0.11478274, -0.11571897, -0.11658665,\n",
       "        -0.11363857, -0.11494553, -0.11330754, -0.11347436, -0.11446918,\n",
       "        -0.11324248, -0.11510378, -0.11461553, -0.11511443, -0.11544337,\n",
       "        -0.1141908 , -0.11521721, -0.1152476 , -0.11544343, -0.11521471,\n",
       "        -0.1302292 , -0.1295312 , -0.13081366, -0.12985852, -0.12993142,\n",
       "        -0.13081316, -0.13042326, -0.13041845, -0.13005669, -0.13038834,\n",
       "        -0.12968594, -0.13035991, -0.13082162, -0.13039583, -0.13009503,\n",
       "        -0.13139923, -0.13101738, -0.13058123, -0.13093035, -0.13105356,\n",
       "        -0.11676287, -0.11647484, -0.11683875, -0.11812092, -0.1166252 ,\n",
       "        -0.11735399, -0.11735394, -0.11979457, -0.11876   , -0.11869446,\n",
       "        -0.11855059, -0.11845407, -0.11903785, -0.11812502, -0.11817928,\n",
       "        -0.11908375, -0.11989828, -0.12053477, -0.12023652, -0.1198833 ,\n",
       "        -0.11392985, -0.11414583, -0.11354137, -0.11579219, -0.1145369 ,\n",
       "        -0.1136858 , -0.11527404, -0.11594707, -0.11615217, -0.11518419,\n",
       "        -0.11591502, -0.11557808, -0.11570271, -0.11399499, -0.11622998,\n",
       "        -0.11586231, -0.11594458, -0.1169032 , -0.11725011, -0.11536661,\n",
       "        -0.11356199, -0.11306731, -0.11314565, -0.11486513, -0.11390655,\n",
       "        -0.11479865, -0.11401743, -0.11619892, -0.11464234, -0.11517201,\n",
       "        -0.11505826, -0.11488603, -0.11504419, -0.11590587, -0.11469473,\n",
       "        -0.11362546, -0.11399372, -0.11564585, -0.11500058, -0.11448755,\n",
       "        -0.12871948, -0.12849852, -0.12727628, -0.12706455, -0.12862413,\n",
       "        -0.13006147, -0.12707013, -0.12712421, -0.12832424, -0.12793552,\n",
       "        -0.12751364, -0.12942649, -0.12860335, -0.12927668, -0.12754399,\n",
       "        -0.12868902, -0.12816056, -0.12830249, -0.12835385, -0.12880932,\n",
       "        -0.11739189, -0.11965152, -0.1177248 , -0.1154182 , -0.1183692 ,\n",
       "        -0.11872558, -0.11597546, -0.11779614, -0.11881383, -0.11831943,\n",
       "        -0.11695434, -0.11723052, -0.11695324, -0.11912626, -0.1170071 ,\n",
       "        -0.11733855, -0.11848623, -0.11924202, -0.11829532, -0.11886243,\n",
       "        -0.11437166, -0.11353268, -0.11408051, -0.11433164, -0.11638801,\n",
       "        -0.11542283, -0.11215381, -0.11269609, -0.1149687 , -0.11434869,\n",
       "        -0.11534324, -0.11574481, -0.11588058, -0.1162423 , -0.1150459 ,\n",
       "        -0.11533951, -0.11531341, -0.11611899, -0.11712974, -0.11691426,\n",
       "        -0.11166326, -0.11336283, -0.11266078, -0.11156729, -0.11527842,\n",
       "        -0.11401603, -0.11300937, -0.11133271, -0.11446492, -0.11389204,\n",
       "        -0.11504411, -0.11466776, -0.11465393, -0.11393916, -0.11333805,\n",
       "        -0.11524231, -0.11503642, -0.11605132, -0.11510272, -0.11436852,\n",
       "        -0.12491218, -0.12512789, -0.1263184 , -0.12670385, -0.12585826,\n",
       "        -0.12602089, -0.12589141, -0.12635349, -0.12562964, -0.12661176,\n",
       "        -0.12577968, -0.12565968, -0.12587809, -0.12445232, -0.12352463,\n",
       "        -0.1239503 , -0.12591751, -0.12559385, -0.12661974, -0.12543044,\n",
       "        -0.11789434, -0.1163069 , -0.11813444, -0.11826105, -0.11742632,\n",
       "        -0.11867742, -0.11494127, -0.11738538, -0.11759896, -0.1181674 ,\n",
       "        -0.11566941, -0.11621744, -0.11716609, -0.11744721, -0.11654904,\n",
       "        -0.11598729, -0.11733689, -0.1178172 , -0.11747939, -0.11588652,\n",
       "        -0.11365131, -0.11271063, -0.11481359, -0.11529508, -0.11470694,\n",
       "        -0.11580727, -0.11505002, -0.11296662, -0.11563276, -0.11688061,\n",
       "        -0.11448924, -0.11363883, -0.11577868, -0.11544741, -0.11473146,\n",
       "        -0.11457161, -0.11527057, -0.11430889, -0.11389921, -0.11398344,\n",
       "        -0.11344412, -0.11320868, -0.11563285, -0.11326665, -0.11328978,\n",
       "        -0.11386764, -0.11366768, -0.11168996, -0.11450738, -0.1150515 ,\n",
       "        -0.11422017, -0.11293265, -0.11454543, -0.11383333, -0.11216603,\n",
       "        -0.11273885, -0.11460963, -0.1141615 , -0.11308359, -0.11421447]),\n",
       " 'split2_train_score': array([-0.14208863, -0.14147576, -0.14247754, -0.14196734, -0.14107887,\n",
       "        -0.14107876, -0.14099895, -0.14114175, -0.14135524, -0.14208767,\n",
       "        -0.14165592, -0.14198937, -0.14222488, -0.14108993, -0.14119935,\n",
       "        -0.14131783, -0.14170517, -0.14257422, -0.14256749, -0.14206355,\n",
       "        -0.11806266, -0.11795164, -0.11815393, -0.11757355, -0.11868338,\n",
       "        -0.11801659, -0.1178301 , -0.11840774, -0.11910388, -0.11961341,\n",
       "        -0.11958909, -0.1200735 , -0.12050771, -0.11863516, -0.11859123,\n",
       "        -0.11828903, -0.12018126, -0.1197249 , -0.1214088 , -0.12096489,\n",
       "        -0.11522434, -0.11558945, -0.11605266, -0.11608756, -0.11603789,\n",
       "        -0.11532511, -0.11605792, -0.11577887, -0.11574288, -0.11708869,\n",
       "        -0.11684454, -0.11752764, -0.11866197, -0.11558139, -0.11613039,\n",
       "        -0.11590684, -0.11592009, -0.11729176, -0.11822486, -0.11683224,\n",
       "        -0.11408438, -0.11409796, -0.11389149, -0.11351603, -0.11469498,\n",
       "        -0.11546123, -0.11396036, -0.11457453, -0.11590691, -0.1157468 ,\n",
       "        -0.1143031 , -0.1161956 , -0.11626892, -0.11517868, -0.11514177,\n",
       "        -0.11498163, -0.11506365, -0.11656521, -0.11597633, -0.11688316,\n",
       "        -0.1358298 , -0.13491367, -0.13525122, -0.13509646, -0.1355356 ,\n",
       "        -0.13554538, -0.13681328, -0.1366735 , -0.13548071, -0.13553205,\n",
       "        -0.13697609, -0.13771827, -0.13698698, -0.13585882, -0.13521665,\n",
       "        -0.1351309 , -0.13525207, -0.1349061 , -0.13526219, -0.13630619,\n",
       "        -0.11742549, -0.11697248, -0.11696439, -0.11727457, -0.11665896,\n",
       "        -0.1172031 , -0.11838668, -0.11820853, -0.11755211, -0.11769858,\n",
       "        -0.12047675, -0.12085997, -0.11983281, -0.11837688, -0.11885675,\n",
       "        -0.11938567, -0.11875586, -0.1192153 , -0.12000722, -0.1185041 ,\n",
       "        -0.11535444, -0.11441593, -0.11547701, -0.11579605, -0.1152752 ,\n",
       "        -0.11456459, -0.11544002, -0.1164266 , -0.11721166, -0.11738122,\n",
       "        -0.11792164, -0.11842239, -0.117877  , -0.11645991, -0.11671562,\n",
       "        -0.11724939, -0.11762224, -0.11651427, -0.1174856 , -0.11781037,\n",
       "        -0.11435209, -0.11403564, -0.11339102, -0.11386151, -0.11390406,\n",
       "        -0.11485082, -0.11392316, -0.11512113, -0.11567275, -0.11628734,\n",
       "        -0.11765047, -0.11684856, -0.11719586, -0.11530882, -0.11617196,\n",
       "        -0.11488215, -0.1161021 , -0.11583553, -0.11610619, -0.11520467,\n",
       "        -0.12912242, -0.12974037, -0.13032951, -0.12974902, -0.12942472,\n",
       "        -0.13026471, -0.13052162, -0.13036913, -0.13042625, -0.1299372 ,\n",
       "        -0.12932264, -0.12924352, -0.12984277, -0.13029314, -0.12995553,\n",
       "        -0.12956877, -0.12989436, -0.12968281, -0.12996985, -0.13020898,\n",
       "        -0.11720149, -0.11715994, -0.11760774, -0.1169145 , -0.11715823,\n",
       "        -0.11604254, -0.11900104, -0.11868314, -0.11841918, -0.11723763,\n",
       "        -0.1167859 , -0.11693041, -0.11676715, -0.11743695, -0.11917133,\n",
       "        -0.11923606, -0.11847571, -0.11799892, -0.11765024, -0.1185339 ,\n",
       "        -0.11556223, -0.1153302 , -0.1154303 , -0.11476006, -0.11499874,\n",
       "        -0.11427975, -0.11606638, -0.11561139, -0.11500506, -0.1155453 ,\n",
       "        -0.11454999, -0.11431331, -0.11535163, -0.11519673, -0.11496304,\n",
       "        -0.11519071, -0.1160483 , -0.11755655, -0.11566616, -0.11555634,\n",
       "        -0.1130424 , -0.11446747, -0.11238762, -0.11285985, -0.11452859,\n",
       "        -0.11409532, -0.11413425, -0.11441496, -0.11584683, -0.1140728 ,\n",
       "        -0.11414139, -0.11400777, -0.11527146, -0.11583728, -0.1141263 ,\n",
       "        -0.11519753, -0.11504027, -0.11513532, -0.11486812, -0.11555342,\n",
       "        -0.12665479, -0.12628166, -0.12752735, -0.12767755, -0.12780379,\n",
       "        -0.127762  , -0.12748293, -0.12729886, -0.12593524, -0.12668378,\n",
       "        -0.12664465, -0.12610247, -0.12683278, -0.12560021, -0.12697793,\n",
       "        -0.12761176, -0.12713292, -0.1261431 , -0.12609344, -0.12662087,\n",
       "        -0.11609283, -0.11727587, -0.11773313, -0.11751105, -0.11764875,\n",
       "        -0.11798824, -0.11734561, -0.11775047, -0.11725832, -0.11774782,\n",
       "        -0.11731911, -0.11792567, -0.11819913, -0.11655573, -0.11670924,\n",
       "        -0.11835621, -0.11729683, -0.1167786 , -0.11759408, -0.11736852,\n",
       "        -0.1143174 , -0.11373204, -0.11510667, -0.11509151, -0.11650441,\n",
       "        -0.11460652, -0.11493552, -0.11542613, -0.11409592, -0.11460109,\n",
       "        -0.11362238, -0.11574617, -0.11548933, -0.11534105, -0.11485697,\n",
       "        -0.11640216, -0.11437472, -0.11447377, -0.11530413, -0.11544885,\n",
       "        -0.11422892, -0.11408216, -0.11378053, -0.11475036, -0.11462905,\n",
       "        -0.11509028, -0.11480346, -0.11536259, -0.11325763, -0.1143922 ,\n",
       "        -0.11231258, -0.1141114 , -0.11528188, -0.11371644, -0.11396662,\n",
       "        -0.11563093, -0.11334967, -0.11442694, -0.11549077, -0.11493813,\n",
       "        -0.12452327, -0.12470009, -0.12597182, -0.12651652, -0.12789584,\n",
       "        -0.12622462, -0.12717057, -0.12373026, -0.12341351, -0.12373603,\n",
       "        -0.12510157, -0.12400597, -0.12442316, -0.12537541, -0.12657676,\n",
       "        -0.12536759, -0.12458877, -0.12584437, -0.12471737, -0.12468446,\n",
       "        -0.11635979, -0.11647192, -0.11779382, -0.11864158, -0.11829835,\n",
       "        -0.11856065, -0.11855813, -0.117811  , -0.11720017, -0.1173205 ,\n",
       "        -0.11825998, -0.1174352 , -0.11646815, -0.11622948, -0.11789749,\n",
       "        -0.11828263, -0.11864189, -0.1189416 , -0.11512817, -0.1167432 ,\n",
       "        -0.11540001, -0.11471104, -0.11584257, -0.11599836, -0.11552082,\n",
       "        -0.11450486, -0.1163027 , -0.11409661, -0.11472609, -0.11601707,\n",
       "        -0.1155232 , -0.11481337, -0.11650727, -0.11302552, -0.1147294 ,\n",
       "        -0.11510046, -0.11433309, -0.11471435, -0.11468253, -0.11513665,\n",
       "        -0.11309202, -0.11465829, -0.11590888, -0.11544578, -0.11478695,\n",
       "        -0.11363623, -0.11331845, -0.11404635, -0.11413324, -0.11450204,\n",
       "        -0.11417319, -0.11304035, -0.11488006, -0.11218634, -0.11468515,\n",
       "        -0.11263894, -0.11299294, -0.11499924, -0.11387396, -0.11276508]),\n",
       " 'split3_train_score': array([-0.13849699, -0.13791858, -0.13802246, -0.13710173, -0.13778552,\n",
       "        -0.13723033, -0.13781419, -0.13768683, -0.13831921, -0.13791049,\n",
       "        -0.13803558, -0.13773634, -0.13774655, -0.13857093, -0.13840124,\n",
       "        -0.13820954, -0.13826373, -0.1393264 , -0.1385128 , -0.13872598,\n",
       "        -0.1174252 , -0.11624541, -0.11774128, -0.11801286, -0.1163975 ,\n",
       "        -0.11645015, -0.11727464, -0.11632757, -0.11761602, -0.11702273,\n",
       "        -0.11736457, -0.11735664, -0.11838115, -0.11795357, -0.11785238,\n",
       "        -0.11721915, -0.11785786, -0.11772284, -0.11929029, -0.11820174,\n",
       "        -0.11131026, -0.11250813, -0.11469386, -0.1133287 , -0.11318444,\n",
       "        -0.11387905, -0.1147099 , -0.11441191, -0.11320645, -0.11451745,\n",
       "        -0.11288113, -0.1144431 , -0.11600058, -0.11415299, -0.11352843,\n",
       "        -0.11383543, -0.1145169 , -0.11597854, -0.11454615, -0.11517505,\n",
       "        -0.11123597, -0.11208889, -0.11206775, -0.11201263, -0.11318549,\n",
       "        -0.11381612, -0.11341713, -0.11232972, -0.11283585, -0.11229003,\n",
       "        -0.11338043, -0.11424851, -0.11396086, -0.11340483, -0.1130755 ,\n",
       "        -0.11482868, -0.11392908, -0.11305903, -0.11501963, -0.11470588,\n",
       "        -0.13257622, -0.13281031, -0.13226665, -0.13240047, -0.13348377,\n",
       "        -0.13275425, -0.13374543, -0.13380451, -0.13272808, -0.13250151,\n",
       "        -0.13260831, -0.13320054, -0.13224138, -0.13252604, -0.13240507,\n",
       "        -0.13258369, -0.13278547, -0.13268892, -0.13299905, -0.1331403 ,\n",
       "        -0.1179972 , -0.11712618, -0.11623759, -0.11805698, -0.11774371,\n",
       "        -0.11793689, -0.11798657, -0.11739753, -0.11547458, -0.11731501,\n",
       "        -0.11699592, -0.11680734, -0.11619968, -0.11715641, -0.11650966,\n",
       "        -0.11623099, -0.1172354 , -0.11788598, -0.11672132, -0.11857819,\n",
       "        -0.11461857, -0.11429318, -0.11343335, -0.11412651, -0.11498577,\n",
       "        -0.11417903, -0.1139068 , -0.11327662, -0.11330409, -0.11361576,\n",
       "        -0.11373992, -0.11493755, -0.11524987, -0.11577258, -0.1132933 ,\n",
       "        -0.11427413, -0.11344611, -0.11473966, -0.11352097, -0.11531753,\n",
       "        -0.11371665, -0.11397388, -0.11166637, -0.11288531, -0.11385207,\n",
       "        -0.11429716, -0.11236101, -0.11384543, -0.11267897, -0.11252147,\n",
       "        -0.11395915, -0.11349615, -0.11368734, -0.11442774, -0.11307898,\n",
       "        -0.11281916, -0.11285466, -0.11370151, -0.11299173, -0.11424267,\n",
       "        -0.12903316, -0.12927852, -0.12897042, -0.12895123, -0.12903567,\n",
       "        -0.12917306, -0.1284389 , -0.1277652 , -0.12895958, -0.12843566,\n",
       "        -0.12960377, -0.12828307, -0.12749778, -0.12921063, -0.12878369,\n",
       "        -0.12930753, -0.12939136, -0.1277975 , -0.12747516, -0.12802001,\n",
       "        -0.11750141, -0.11825159, -0.11610934, -0.11666633, -0.11656625,\n",
       "        -0.11556528, -0.11693703, -0.11631177, -0.1171851 , -0.11686701,\n",
       "        -0.11667255, -0.11597165, -0.11687197, -0.11606118, -0.11726824,\n",
       "        -0.11783501, -0.11786889, -0.11663226, -0.11870263, -0.11894301,\n",
       "        -0.1133128 , -0.11343275, -0.11573682, -0.1136142 , -0.11404215,\n",
       "        -0.11391226, -0.11491569, -0.11568431, -0.11405346, -0.1153828 ,\n",
       "        -0.11514192, -0.11427064, -0.11414401, -0.11373604, -0.1151524 ,\n",
       "        -0.11463017, -0.11508131, -0.11349197, -0.11588414, -0.11356968,\n",
       "        -0.11290731, -0.11280637, -0.11228969, -0.11240476, -0.1137123 ,\n",
       "        -0.11377268, -0.11294533, -0.11360321, -0.11398951, -0.11463332,\n",
       "        -0.11309469, -0.11358834, -0.11269604, -0.1131284 , -0.11400295,\n",
       "        -0.11399252, -0.11407334, -0.1140433 , -0.11456564, -0.1138372 ,\n",
       "        -0.12468693, -0.12521541, -0.1246938 , -0.12486859, -0.12555793,\n",
       "        -0.12498557, -0.1257188 , -0.12599864, -0.12403414, -0.12483609,\n",
       "        -0.12624586, -0.1266434 , -0.12565782, -0.12437805, -0.12654693,\n",
       "        -0.12541801, -0.12466065, -0.12477758, -0.12507187, -0.12505976,\n",
       "        -0.11526229, -0.11639216, -0.11537655, -0.11707744, -0.11562949,\n",
       "        -0.11566833, -0.11659999, -0.1170808 , -0.11429191, -0.11700793,\n",
       "        -0.11685112, -0.11465413, -0.11641924, -0.11526861, -0.11609936,\n",
       "        -0.11789447, -0.11626435, -0.11604329, -0.1164988 , -0.11626533,\n",
       "        -0.11217988, -0.1143535 , -0.1118262 , -0.11396896, -0.11504685,\n",
       "        -0.1153326 , -0.11480561, -0.11396862, -0.11300038, -0.11331773,\n",
       "        -0.11504149, -0.11371094, -0.11501457, -0.11401941, -0.11530673,\n",
       "        -0.11566734, -0.11515027, -0.11551494, -0.11511283, -0.11579434,\n",
       "        -0.11256616, -0.1128828 , -0.11223004, -0.11399938, -0.11307206,\n",
       "        -0.11421386, -0.11438365, -0.11391146, -0.11111691, -0.11294089,\n",
       "        -0.11401537, -0.11317681, -0.11320249, -0.11355331, -0.1140879 ,\n",
       "        -0.11512958, -0.11539206, -0.11378812, -0.11591257, -0.11437207,\n",
       "        -0.12267439, -0.12231568, -0.12280226, -0.12291579, -0.122686  ,\n",
       "        -0.12339952, -0.12308422, -0.12320592, -0.1236388 , -0.12336357,\n",
       "        -0.12331469, -0.12331828, -0.12364844, -0.12328711, -0.12359398,\n",
       "        -0.12315774, -0.12351904, -0.12411206, -0.12455899, -0.12448786,\n",
       "        -0.11365333, -0.11498213, -0.11502621, -0.11454009, -0.11509482,\n",
       "        -0.11605914, -0.11746909, -0.11679559, -0.11657799, -0.11646254,\n",
       "        -0.11630984, -0.11487895, -0.11520942, -0.11612431, -0.11649187,\n",
       "        -0.11720227, -0.1165397 , -0.11662293, -0.11650547, -0.11688381,\n",
       "        -0.11346238, -0.11363357, -0.11426831, -0.11445103, -0.11397519,\n",
       "        -0.11346272, -0.11556726, -0.11427024, -0.11341161, -0.11503973,\n",
       "        -0.11404801, -0.11310409, -0.11342075, -0.11381261, -0.11521447,\n",
       "        -0.11344267, -0.11424738, -0.11433287, -0.11420362, -0.11453572,\n",
       "        -0.11224107, -0.11214379, -0.11323373, -0.11082685, -0.11177102,\n",
       "        -0.11182765, -0.11360578, -0.11219422, -0.11414061, -0.1129241 ,\n",
       "        -0.11188639, -0.11235347, -0.1120788 , -0.11385381, -0.11411318,\n",
       "        -0.11345811, -0.11406145, -0.11308292, -0.11481014, -0.11399755]),\n",
       " 'split4_train_score': array([-0.14079163, -0.14089626, -0.14068597, -0.14110057, -0.14221894,\n",
       "        -0.14175759, -0.14134509, -0.14159575, -0.14184258, -0.14133768,\n",
       "        -0.14182411, -0.14030662, -0.14116637, -0.14043146, -0.14003993,\n",
       "        -0.13979905, -0.139909  , -0.14193701, -0.1418681 , -0.14217533,\n",
       "        -0.11702344, -0.11673886, -0.1169364 , -0.11705666, -0.11707018,\n",
       "        -0.11720591, -0.11723278, -0.11716048, -0.11814706, -0.11826787,\n",
       "        -0.11752035, -0.11715134, -0.11685406, -0.11674706, -0.11689531,\n",
       "        -0.11880503, -0.11823064, -0.11760551, -0.11727774, -0.11735642,\n",
       "        -0.11261044, -0.11343358, -0.11301882, -0.11275626, -0.11427765,\n",
       "        -0.11350402, -0.1137816 , -0.11448265, -0.11493061, -0.11525272,\n",
       "        -0.11419218, -0.11273044, -0.11393792, -0.11402227, -0.11460477,\n",
       "        -0.11572932, -0.11467051, -0.11600558, -0.11447282, -0.11572721,\n",
       "        -0.11280529, -0.11222255, -0.11188692, -0.11245394, -0.11357641,\n",
       "        -0.11260395, -0.11265242, -0.11353845, -0.11342449, -0.1125171 ,\n",
       "        -0.11323599, -0.11342809, -0.11417115, -0.11391333, -0.11404626,\n",
       "        -0.11463122, -0.11483986, -0.11536682, -0.11390617, -0.11393348,\n",
       "        -0.13470522, -0.13418332, -0.13372615, -0.13394877, -0.13385455,\n",
       "        -0.13397875, -0.13445794, -0.13426365, -0.13384636, -0.13615266,\n",
       "        -0.13618993, -0.13564022, -0.13493375, -0.13493815, -0.13463268,\n",
       "        -0.13501352, -0.13422151, -0.13486572, -0.13423312, -0.1351062 ,\n",
       "        -0.11532225, -0.11776764, -0.11633226, -0.11622497, -0.11604239,\n",
       "        -0.11537337, -0.11663587, -0.11498789, -0.11601151, -0.11692423,\n",
       "        -0.11753606, -0.11739637, -0.11754097, -0.11862676, -0.11791291,\n",
       "        -0.1185874 , -0.11726234, -0.11580935, -0.11575403, -0.11715711,\n",
       "        -0.11081111, -0.11187511, -0.11213938, -0.11164091, -0.1112237 ,\n",
       "        -0.1130399 , -0.11265024, -0.11334645, -0.11386704, -0.11367975,\n",
       "        -0.11419806, -0.11446561, -0.11408791, -0.1142799 , -0.11564608,\n",
       "        -0.11503004, -0.11538323, -0.11411365, -0.11381996, -0.1157481 ,\n",
       "        -0.11122605, -0.11088137, -0.11059236, -0.11077555, -0.11220972,\n",
       "        -0.11302625, -0.11156792, -0.11165203, -0.11174013, -0.1125509 ,\n",
       "        -0.11491994, -0.11351114, -0.11322699, -0.11411473, -0.11460219,\n",
       "        -0.11405905, -0.11608055, -0.11420817, -0.11371893, -0.11539213,\n",
       "        -0.1282629 , -0.12882796, -0.12758937, -0.12941955, -0.12989953,\n",
       "        -0.12706681, -0.12798819, -0.12787511, -0.12819067, -0.12867795,\n",
       "        -0.12867653, -0.12977393, -0.1299512 , -0.12929563, -0.12873518,\n",
       "        -0.12829404, -0.12874941, -0.12879939, -0.13079551, -0.13073507,\n",
       "        -0.11564815, -0.11624173, -0.11469382, -0.11578392, -0.11590613,\n",
       "        -0.11569647, -0.11414718, -0.11559899, -0.11558038, -0.11472039,\n",
       "        -0.11466823, -0.11627851, -0.11688776, -0.1171811 , -0.116669  ,\n",
       "        -0.11688064, -0.1169152 , -0.11700218, -0.11667583, -0.11801133,\n",
       "        -0.11158853, -0.11289327, -0.11219623, -0.11231721, -0.11270963,\n",
       "        -0.11142136, -0.11209036, -0.11238508, -0.11246617, -0.11271529,\n",
       "        -0.11196231, -0.11482653, -0.11264261, -0.11322068, -0.11400482,\n",
       "        -0.11288656, -0.11275359, -0.11348575, -0.11415972, -0.11494536,\n",
       "        -0.11075673, -0.11127375, -0.11060813, -0.11140747, -0.11176854,\n",
       "        -0.10951221, -0.11131335, -0.11118414, -0.11107363, -0.11256054,\n",
       "        -0.11135809, -0.11285285, -0.11336118, -0.11303545, -0.11241063,\n",
       "        -0.11315411, -0.11287126, -0.1120357 , -0.11337143, -0.11351911,\n",
       "        -0.12583899, -0.12625968, -0.12658585, -0.12406263, -0.12403035,\n",
       "        -0.12473883, -0.12405354, -0.12490433, -0.12485611, -0.12482364,\n",
       "        -0.12510531, -0.12584333, -0.12422212, -0.12399085, -0.12471805,\n",
       "        -0.125524  , -0.12631292, -0.12579217, -0.12589158, -0.12594359,\n",
       "        -0.11692922, -0.11570996, -0.11570016, -0.1151886 , -0.11650563,\n",
       "        -0.11554186, -0.11464956, -0.11434794, -0.11528356, -0.11487492,\n",
       "        -0.11387144, -0.11408238, -0.11471309, -0.11359505, -0.11501959,\n",
       "        -0.11577298, -0.1163251 , -0.11598084, -0.1161536 , -0.11676741,\n",
       "        -0.11299826, -0.11416152, -0.11387009, -0.11127923, -0.11242738,\n",
       "        -0.11175652, -0.11201027, -0.11278533, -0.11155994, -0.11221993,\n",
       "        -0.11212106, -0.11161848, -0.11246908, -0.11155532, -0.11456369,\n",
       "        -0.11317667, -0.11411356, -0.11450575, -0.11422476, -0.1143394 ,\n",
       "        -0.11379101, -0.11229601, -0.11294598, -0.11311495, -0.11163827,\n",
       "        -0.11275471, -0.11240431, -0.11232004, -0.11197836, -0.11105094,\n",
       "        -0.11280126, -0.11073571, -0.11270327, -0.11236985, -0.11344706,\n",
       "        -0.11338483, -0.11427463, -0.11227135, -0.11457252, -0.11368114,\n",
       "        -0.12637543, -0.12270286, -0.12355579, -0.12293207, -0.12343356,\n",
       "        -0.12280468, -0.12341959, -0.12237327, -0.12372654, -0.12350033,\n",
       "        -0.12402967, -0.1237989 , -0.12399223, -0.12237651, -0.12307397,\n",
       "        -0.12401312, -0.12409022, -0.12454225, -0.12481689, -0.12480534,\n",
       "        -0.11786121, -0.11603925, -0.11587142, -0.11498944, -0.11497369,\n",
       "        -0.11550934, -0.11639695, -0.11553552, -0.11491272, -0.11490091,\n",
       "        -0.11552831, -0.11599495, -0.11581377, -0.11614774, -0.11621039,\n",
       "        -0.11684789, -0.11638367, -0.11623594, -0.11521722, -0.11511144,\n",
       "        -0.11329478, -0.11218001, -0.1122069 , -0.11170718, -0.11172054,\n",
       "        -0.11225936, -0.11374037, -0.11155802, -0.11282695, -0.11368078,\n",
       "        -0.1138518 , -0.11354957, -0.11302197, -0.11224024, -0.11240803,\n",
       "        -0.11281517, -0.11409825, -0.11308756, -0.11406333, -0.11357656,\n",
       "        -0.11051891, -0.11170904, -0.11207743, -0.1120119 , -0.11088015,\n",
       "        -0.11202432, -0.11260659, -0.11178055, -0.11256923, -0.11347337,\n",
       "        -0.11175063, -0.11305284, -0.11253327, -0.11318332, -0.11195337,\n",
       "        -0.11283727, -0.11315291, -0.11330182, -0.11217125, -0.11305624]),\n",
       " 'mean_train_score': array([-0.14025567, -0.14009394, -0.14005573, -0.13989274, -0.13997175,\n",
       "        -0.13983299, -0.139856  , -0.13990421, -0.14018817, -0.14016852,\n",
       "        -0.13999449, -0.13968911, -0.14002699, -0.14011129, -0.14001945,\n",
       "        -0.13971586, -0.13984637, -0.14068559, -0.14053684, -0.14039136,\n",
       "        -0.1174967 , -0.1173482 , -0.1177624 , -0.11725638, -0.11739739,\n",
       "        -0.11715796, -0.11749759, -0.11741242, -0.11780071, -0.11787053,\n",
       "        -0.11831024, -0.11803248, -0.11818461, -0.11771002, -0.11787922,\n",
       "        -0.11794428, -0.11858469, -0.11817557, -0.11891685, -0.11853839,\n",
       "        -0.11384191, -0.11421987, -0.11452512, -0.11418126, -0.11456017,\n",
       "        -0.11417802, -0.11483805, -0.1145291 , -0.11465867, -0.1148231 ,\n",
       "        -0.11483097, -0.11492068, -0.11578524, -0.11458637, -0.11464967,\n",
       "        -0.1148464 , -0.11519842, -0.11598742, -0.11540811, -0.11550079,\n",
       "        -0.11301823, -0.1132154 , -0.11318786, -0.11282389, -0.11371895,\n",
       "        -0.11349607, -0.11355633, -0.11330685, -0.11366826, -0.11356365,\n",
       "        -0.11397254, -0.11437252, -0.11420837, -0.11401562, -0.11400421,\n",
       "        -0.11429097, -0.1147371 , -0.11503388, -0.11493964, -0.11501859,\n",
       "        -0.1343699 , -0.13423251, -0.13411103, -0.13439409, -0.13463244,\n",
       "        -0.13402878, -0.13471497, -0.13478487, -0.13405657, -0.13489948,\n",
       "        -0.13455794, -0.13498334, -0.13425531, -0.1339792 , -0.13420495,\n",
       "        -0.13454199, -0.13416216, -0.13423473, -0.13411951, -0.13463389,\n",
       "        -0.11741882, -0.11796939, -0.11738164, -0.11755977, -0.11741944,\n",
       "        -0.11685002, -0.11715176, -0.11742055, -0.11625012, -0.11758175,\n",
       "        -0.11804219, -0.11802021, -0.11760073, -0.11737893, -0.11767995,\n",
       "        -0.11780931, -0.11743297, -0.11762569, -0.1177276 , -0.11797191,\n",
       "        -0.11413202, -0.11470375, -0.1145147 , -0.11460526, -0.11474351,\n",
       "        -0.11425872, -0.11407686, -0.11451845, -0.11452971, -0.11512257,\n",
       "        -0.11492375, -0.11526051, -0.1150025 , -0.11503684, -0.11525545,\n",
       "        -0.11474509, -0.11542139, -0.11527757, -0.11531415, -0.11625083,\n",
       "        -0.11383899, -0.11414616, -0.11274393, -0.11355249, -0.11397987,\n",
       "        -0.11396316, -0.11338907, -0.11332477, -0.11317493, -0.11404019,\n",
       "        -0.11436522, -0.11437107, -0.11410802, -0.11427362, -0.1147607 ,\n",
       "        -0.11354976, -0.11476394, -0.11459323, -0.11453517, -0.11495531,\n",
       "        -0.12936972, -0.12929881, -0.12949054, -0.12929202, -0.12928196,\n",
       "        -0.12907079, -0.12911253, -0.1290073 , -0.1292826 , -0.1292371 ,\n",
       "        -0.12922942, -0.12936186, -0.12952107, -0.12974639, -0.12927436,\n",
       "        -0.12958513, -0.12968631, -0.1292541 , -0.12970143, -0.1299386 ,\n",
       "        -0.11725849, -0.11680265, -0.11651337, -0.11684   , -0.11678586,\n",
       "        -0.11628337, -0.11685271, -0.11753672, -0.11745405, -0.11690255,\n",
       "        -0.11677297, -0.11673547, -0.11709384, -0.11714184, -0.11746769,\n",
       "        -0.11780483, -0.11803144, -0.11789961, -0.11831494, -0.11843137,\n",
       "        -0.11411091, -0.11419541, -0.1146898 , -0.11434255, -0.11408531,\n",
       "        -0.11315498, -0.11441975, -0.11464876, -0.11422849, -0.11448299,\n",
       "        -0.11446014, -0.11470662, -0.11447287, -0.1143815 , -0.11490891,\n",
       "        -0.114412  , -0.11441299, -0.11524655, -0.11572418, -0.11490263,\n",
       "        -0.11307918, -0.11301036, -0.11304251, -0.11300639, -0.11344721,\n",
       "        -0.11310093, -0.11341912, -0.11377413, -0.11376304, -0.11363703,\n",
       "        -0.11327134, -0.11351454, -0.11389856, -0.11434011, -0.1136794 ,\n",
       "        -0.11407688, -0.11374146, -0.11420122, -0.11431159, -0.11453754,\n",
       "        -0.12621228, -0.12641372, -0.12641739, -0.12582114, -0.1261333 ,\n",
       "        -0.12667571, -0.12606633, -0.12626694, -0.12590552, -0.12621962,\n",
       "        -0.12635089, -0.12675161, -0.12620904, -0.12580185, -0.12655495,\n",
       "        -0.12670129, -0.12651067, -0.12611609, -0.12624164, -0.12673768,\n",
       "        -0.11637399, -0.11737097, -0.116698  , -0.11643795, -0.11700766,\n",
       "        -0.11691378, -0.1164569 , -0.11670411, -0.11662459, -0.11702984,\n",
       "        -0.11651737, -0.11626858, -0.11668203, -0.11638813, -0.11667863,\n",
       "        -0.11733761, -0.11742839, -0.11717203, -0.11725553, -0.11732052,\n",
       "        -0.11374504, -0.11410573, -0.1138764 , -0.11386467, -0.11500201,\n",
       "        -0.11434591, -0.11375918, -0.1139585 , -0.11376135, -0.11398759,\n",
       "        -0.11456164, -0.11427709, -0.1151422 , -0.11464213, -0.11506384,\n",
       "        -0.11531637, -0.11492684, -0.11529752, -0.11555192, -0.11556164,\n",
       "        -0.11320078, -0.11364172, -0.11292551, -0.11370608, -0.11336657,\n",
       "        -0.11419284, -0.11385354, -0.11336089, -0.11270097, -0.11325707,\n",
       "        -0.11364961, -0.11334968, -0.1141012 , -0.11365169, -0.1138531 ,\n",
       "        -0.11481638, -0.11433019, -0.11424847, -0.11511946, -0.11458569,\n",
       "        -0.12429682, -0.12369107, -0.1247549 , -0.12491949, -0.12485303,\n",
       "        -0.12438595, -0.1246861 , -0.1240007 , -0.12419912, -0.12447694,\n",
       "        -0.12457273, -0.12426344, -0.12463919, -0.12399796, -0.12428832,\n",
       "        -0.12431523, -0.12461648, -0.12502113, -0.12524319, -0.12488945,\n",
       "        -0.11638113, -0.11601281, -0.11684511, -0.1168038 , -0.11659036,\n",
       "        -0.11757491, -0.11722742, -0.11725158, -0.1170099 , -0.11695726,\n",
       "        -0.11673046, -0.11653272, -0.11641925, -0.11683201, -0.11697019,\n",
       "        -0.11728081, -0.11761604, -0.11765926, -0.1166688 , -0.11661089,\n",
       "        -0.11418894, -0.11332907, -0.1142705 , -0.11430254, -0.11408074,\n",
       "        -0.11419305, -0.11532119, -0.11353678, -0.11464065, -0.11518411,\n",
       "        -0.11471898, -0.11404246, -0.11485991, -0.11386729, -0.11462914,\n",
       "        -0.1142923 , -0.11475331, -0.11442918, -0.11460825, -0.1147042 ,\n",
       "        -0.11271689, -0.11302914, -0.11405332, -0.11294121, -0.11271969,\n",
       "        -0.11327961, -0.11371945, -0.11293392, -0.11416437, -0.11419739,\n",
       "        -0.11342058, -0.11320606, -0.11408986, -0.11329106, -0.11340642,\n",
       "        -0.11333058, -0.11424649, -0.11417422, -0.11404796, -0.11401954]),\n",
       " 'std_train_score': array([0.00121214, 0.00120728, 0.00150011, 0.00168435, 0.00153819,\n",
       "        0.00157167, 0.00125706, 0.00140647, 0.00137428, 0.00146182,\n",
       "        0.00151295, 0.0014299 , 0.00158988, 0.00083229, 0.00090754,\n",
       "        0.00099507, 0.00111329, 0.00131077, 0.00147699, 0.00145583,\n",
       "        0.00033375, 0.00071698, 0.00044742, 0.00070856, 0.00086235,\n",
       "        0.00052091, 0.00022459, 0.00067773, 0.00076583, 0.00099407,\n",
       "        0.00093688, 0.00105135, 0.0013388 , 0.0006465 , 0.00078335,\n",
       "        0.00062911, 0.00085189, 0.00080863, 0.00140508, 0.00128056,\n",
       "        0.00159203, 0.00111839, 0.00096482, 0.00114938, 0.00097064,\n",
       "        0.00077536, 0.00072486, 0.00071902, 0.00090046, 0.00129721,\n",
       "        0.0014095 , 0.00155467, 0.00158939, 0.00067392, 0.00094358,\n",
       "        0.00092812, 0.00055773, 0.00073797, 0.00143051, 0.00076075,\n",
       "        0.00098488, 0.00097726, 0.00100842, 0.00052825, 0.0007216 ,\n",
       "        0.00107429, 0.00056604, 0.00075622, 0.00116461, 0.00123497,\n",
       "        0.00061034, 0.00095794, 0.00118261, 0.00084635, 0.00069566,\n",
       "        0.00071251, 0.0004608 , 0.00115167, 0.00084001, 0.00111168,\n",
       "        0.00105397, 0.00075787, 0.0012083 , 0.00117537, 0.00087059,\n",
       "        0.00091041, 0.00108641, 0.00100834, 0.00106537, 0.00131461,\n",
       "        0.0018197 , 0.00160764, 0.00180493, 0.00146497, 0.0009599 ,\n",
       "        0.00109398, 0.00098264, 0.00089081, 0.00088188, 0.00121729,\n",
       "        0.00114006, 0.00094688, 0.00115243, 0.00078642, 0.00144057,\n",
       "        0.00086917, 0.00088987, 0.00131627, 0.00080827, 0.00044377,\n",
       "        0.00132406, 0.00144542, 0.00129846, 0.0012993 , 0.00086374,\n",
       "        0.00138975, 0.00115304, 0.00114591, 0.00158785, 0.00077644,\n",
       "        0.0017548 , 0.00175499, 0.00160471, 0.00161177, 0.00203313,\n",
       "        0.00064657, 0.00089212, 0.00117183, 0.00142249, 0.00141322,\n",
       "        0.00157486, 0.00169079, 0.00165546, 0.00115618, 0.0011174 ,\n",
       "        0.00170488, 0.00141609, 0.00086994, 0.00153518, 0.00088806,\n",
       "        0.0014602 , 0.0019044 , 0.0014602 , 0.00166637, 0.00143885,\n",
       "        0.00061388, 0.00123749, 0.00115668, 0.00136998, 0.00140487,\n",
       "        0.00189037, 0.0014392 , 0.00178921, 0.0010324 , 0.00103686,\n",
       "        0.00109994, 0.00132755, 0.00081231, 0.00112746, 0.00042009,\n",
       "        0.00075221, 0.00031765, 0.00113196, 0.00051355, 0.00066903,\n",
       "        0.00138261, 0.00111993, 0.00116872, 0.00083146, 0.00077606,\n",
       "        0.00040018, 0.00069124, 0.00110222, 0.00050098, 0.0006151 ,\n",
       "        0.00100873, 0.00075818, 0.00092711, 0.00125451, 0.00106779,\n",
       "        0.00114783, 0.00083558, 0.00104151, 0.00074934, 0.00059574,\n",
       "        0.00067678, 0.00156169, 0.00153139, 0.00111682, 0.00127236,\n",
       "        0.00124676, 0.00092344, 0.00103848, 0.00067516, 0.00110404,\n",
       "        0.00125199, 0.00109732, 0.00139212, 0.00118079, 0.00102629,\n",
       "        0.00163353, 0.00095153, 0.00158874, 0.0012405 , 0.00076623,\n",
       "        0.00105709, 0.00138266, 0.00140539, 0.00126665, 0.00112754,\n",
       "        0.00133434, 0.00047851, 0.00107194, 0.00094613, 0.00079189,\n",
       "        0.0010894 , 0.00160963, 0.00170078, 0.00098071, 0.00070014,\n",
       "        0.0014038 , 0.00103594, 0.0020451 , 0.00115182, 0.00092572,\n",
       "        0.00185751, 0.00119426, 0.0016199 , 0.00159058, 0.0012862 ,\n",
       "        0.00126201, 0.00091628, 0.00105247, 0.00127767, 0.00080154,\n",
       "        0.00069749, 0.00085365, 0.00123911, 0.00063966, 0.00079108,\n",
       "        0.00141793, 0.00111211, 0.0010139 , 0.00135227, 0.00178592,\n",
       "        0.00199828, 0.00119917, 0.00087122, 0.00146134, 0.00121507,\n",
       "        0.00077614, 0.00137343, 0.00145628, 0.00186673, 0.00097147,\n",
       "        0.00126525, 0.00115077, 0.00118162, 0.001111  , 0.0012664 ,\n",
       "        0.00073373, 0.00135155, 0.00099337, 0.00094562, 0.0009414 ,\n",
       "        0.00126037, 0.00108522, 0.00126539, 0.00162221, 0.00117059,\n",
       "        0.00134895, 0.00157819, 0.00114171, 0.00187584, 0.00116002,\n",
       "        0.0008713 , 0.00104947, 0.00122854, 0.00080167, 0.00087172,\n",
       "        0.00099663, 0.00043502, 0.00110866, 0.00134451, 0.00147902,\n",
       "        0.00133989, 0.00137076, 0.00109922, 0.00134258, 0.00111538,\n",
       "        0.00155996, 0.00153519, 0.00146763, 0.00172943, 0.00034195,\n",
       "        0.00112633, 0.00058928, 0.00068703, 0.00096906, 0.00083102,\n",
       "        0.00094645, 0.00113439, 0.00050841, 0.001268  , 0.00138915,\n",
       "        0.00082346, 0.00096518, 0.00139813, 0.00113604, 0.00120204,\n",
       "        0.00097637, 0.00139141, 0.00097761, 0.00074785, 0.0004065 ,\n",
       "        0.00077553, 0.00079078, 0.00123378, 0.00053426, 0.00063266,\n",
       "        0.00134668, 0.00109156, 0.00136484, 0.00167755, 0.00185435,\n",
       "        0.0014386 , 0.00157925, 0.00134197, 0.0008187 , 0.00124774,\n",
       "        0.00085067, 0.00079979, 0.00081972, 0.00104773, 0.00125892,\n",
       "        0.00080845, 0.0008121 , 0.00064211, 0.00076009, 0.00032418,\n",
       "        0.00154799, 0.00053353, 0.00119386, 0.00170473, 0.00132584,\n",
       "        0.00148193, 0.00142338, 0.00106495, 0.00126725, 0.00118538,\n",
       "        0.00113312, 0.00114122, 0.00082825, 0.00085007, 0.0006898 ,\n",
       "        0.0008385 , 0.0011178 , 0.0010778 , 0.00146035, 0.00111106,\n",
       "        0.00089224, 0.00086162, 0.00118552, 0.00146131, 0.00128119,\n",
       "        0.00122639, 0.00089289, 0.00115487, 0.00138951, 0.00115154,\n",
       "        0.00075238, 0.00077624, 0.00137932, 0.00116151, 0.00121249,\n",
       "        0.00101693, 0.00067249, 0.00083971, 0.00083414, 0.00095047,\n",
       "        0.00128035, 0.00103636, 0.00147804, 0.00153353, 0.00133455,\n",
       "        0.0012053 , 0.00092032, 0.00132374, 0.00093509, 0.00085685,\n",
       "        0.00134738, 0.00076686, 0.00159313, 0.00060898, 0.00112123,\n",
       "        0.00087268, 0.00123596, 0.00088978, 0.001424  , 0.00115943])}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_xgb.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1db413eca88>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(gs_xgb.cv_results_['param_eta'], gs_xgb.cv_results_['mean_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 5s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>booster</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>predictor</th>\n",
       "      <th>sampling_method</th>\n",
       "      <th>subsample</th>\n",
       "      <th>tree_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.918013</td>\n",
       "      <td>0.113513</td>\n",
       "      <td>0.133725</td>\n",
       "      <td>-0.020212</td>\n",
       "      <td>dart</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.5</td>\n",
       "      <td>3</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>gpu_predictor</td>\n",
       "      <td>gradient_based</td>\n",
       "      <td>0.545263</td>\n",
       "      <td>gpu_hist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE booster  colsample_bytree   eta  \\\n",
       "0  0.918013   0.113513  0.133725 -0.020212    dart               0.8  0.12   \n",
       "\n",
       "   gamma  max_depth  min_child_weight      predictor sampling_method  \\\n",
       "0    0.5          3          5.111111  gpu_predictor  gradient_based   \n",
       "\n",
       "   subsample tree_method  \n",
       "0   0.545263    gpu_hist  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_tree_noOutliers,\n",
    "                                                   hp_logsaleprice_noOutliers,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "# maxdepth = range(2, 6)\n",
    "# eta_ = np.linspace(1e-2, 1, 10)\n",
    "# gamma_ = np.linspace(1e-5, 100, 30)\n",
    "# subsample_ = np.linspace(0, 1, 3)\n",
    "\n",
    "maxdepth = [3]\n",
    "eta_ = [0.12]\n",
    "gamma_ = [0.5]\n",
    "subsample_ = np.linspace(0.54, 0.55, 20)\n",
    "treemethod = ['gpu_hist']\n",
    "predictor_ = ['gpu_predictor']\n",
    "booster_ = ['dart']\n",
    "samplingmethod = ['gradient_based']\n",
    "minchildwt = np.linspace(4, 6, 10)\n",
    "colsamptree = [0.8]\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_,\n",
    "              'tree_method': treemethod,\n",
    "              'predictor': predictor_,\n",
    "              'sampling_method': samplingmethod,\n",
    "              'booster': booster_,\n",
    "              'min_child_weight': minchildwt,\n",
    "              'colsample_bytree': colsamptree}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 5min 7s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>booster</th>\n",
       "      <th>colsample_bytree</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>min_child_weight</th>\n",
       "      <th>predictor</th>\n",
       "      <th>sampling_method</th>\n",
       "      <th>subsample</th>\n",
       "      <th>tree_method</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.955782</td>\n",
       "      <td>0.083362</td>\n",
       "      <td>0.119875</td>\n",
       "      <td>-0.036512</td>\n",
       "      <td>dart</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>4.888889</td>\n",
       "      <td>gpu_predictor</td>\n",
       "      <td>gradient_based</td>\n",
       "      <td>0.546316</td>\n",
       "      <td>gpu_hist</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE booster  colsample_bytree   eta  \\\n",
       "0  0.955782   0.083362  0.119875 -0.036512    dart               0.8  0.12   \n",
       "\n",
       "   gamma  max_depth  min_child_weight      predictor sampling_method  \\\n",
       "0   0.01          3          4.888889  gpu_predictor  gradient_based   \n",
       "\n",
       "   subsample tree_method  \n",
       "0   0.546316    gpu_hist  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_tree_noOutliers,\n",
    "                                                   hp_logsaleprice_noOutliers,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "# maxdepth = range(2, 6)\n",
    "# eta_ = np.linspace(1e-2, 1, 10)\n",
    "# gamma_ = np.linspace(1e-5, 100, 30)\n",
    "# subsample_ = np.linspace(0, 1, 3)\n",
    "\n",
    "maxdepth = [3]\n",
    "eta_ = [0.12]\n",
    "gamma_ = [0.01]\n",
    "subsample_ = np.linspace(0.54, 0.55, 20)\n",
    "treemethod = ['gpu_hist']\n",
    "predictor_ = ['gpu_predictor']\n",
    "booster_ = ['dart']\n",
    "samplingmethod = ['gradient_based']\n",
    "minchildwt = np.linspace(4, 6, 10)\n",
    "colsamptree = [0.8]\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_,\n",
    "              'tree_method': treemethod,\n",
    "              'predictor': predictor_,\n",
    "              'sampling_method': samplingmethod,\n",
    "              'booster': booster_,\n",
    "              'min_child_weight': minchildwt,\n",
    "              'colsample_bytree': colsamptree}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 52s\n",
      "##################################################\n",
      "\n",
      "\n",
      "Currently, the best parameters are:  {'eta': 0.23, 'gamma': 1e-05, 'max_depth': 2, 'subsample': 1.0}\n",
      "where the learning rate is:  0.23\n",
      "and the L1 ratio is:  2\n",
      "where the alpha is:  1e-05\n",
      "and the L1 ratio is:  1.0\n",
      "##################################################\n",
      "\n",
      "\n",
      "The best score is:  0.9516679205615829\n",
      "##################################################\n",
      "\n",
      "\n",
      "The RMSE is:  0.08715446048668395\n"
     ]
    }
   ],
   "source": [
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_tree_noOutliers,\n",
    "                                                   hp_logsaleprice_noOutliers,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "maxdepth = range(2, 6)\n",
    "eta_ = np.linspace(1e-2, 1, 10)\n",
    "gamma_ = np.linspace(1e-5, 100, 30)\n",
    "subsample_ = np.linspace(0, 1, 3)\n",
    "# updater_ = [grow_colmaker,prune,grow_gpu_hist]\n",
    "\n",
    "n_folds=ms.KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1, \n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "print('#'*50)\n",
    "print('\\n')\n",
    "\n",
    "print('Currently, the best parameters are: ', gs_xgb.best_params_)\n",
    "\n",
    "print('where the learning rate is: ', gs_xgb.best_params_['eta'])\n",
    "print('and the L1 ratio is: ', gs_xgb.best_params_['max_depth'])\n",
    "print('where the alpha is: ', gs_xgb.best_params_['gamma'])\n",
    "print('and the L1 ratio is: ', gs_xgb.best_params_['subsample'])\n",
    "\n",
    "print('#'*50)\n",
    "print('\\n')\n",
    "\n",
    "print('The best score is: ', gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "\n",
    "print('#'*50)\n",
    "print('\\n')\n",
    "\n",
    "print('The RMSE is: ', rmse(gs_xgb, ytrain, xtrain))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 22s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.94727</td>\n",
       "      <td>0.091034</td>\n",
       "      <td>0.125691</td>\n",
       "      <td>-0.034657</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Scores  TrainRMSE  TestRMSE  DiffRMSE  eta  gamma  max_depth  subsample\n",
       "0  0.94727   0.091034  0.125691 -0.034657  0.2      0          2   0.555556"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_tree_noOutliers,\n",
    "                                                   hp_logsaleprice_noOutliers,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 6)\n",
    "eta_ = np.linspace(0.05, 0.2, 20)\n",
    "gamma_ = [0]\n",
    "subsample_ = np.linspace(0.5, 1, 10)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 32s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.942514</td>\n",
       "      <td>0.09505</td>\n",
       "      <td>0.125443</td>\n",
       "      <td>-0.030393</td>\n",
       "      <td>0.152632</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.555556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.942514    0.09505  0.125443 -0.030393  0.152632      0          2   \n",
       "\n",
       "   subsample  \n",
       "0   0.555556  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_tree_noOutliers,\n",
    "                                                   hp_logsaleprice_noOutliers,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 6)\n",
    "eta_ = np.linspace(0.1, 0.3, 20)\n",
    "gamma_ = [0]\n",
    "subsample_ = np.linspace(0.5, 1, 10)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 25s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.964014</td>\n",
       "      <td>0.075204</td>\n",
       "      <td>0.125546</td>\n",
       "      <td>-0.050343</td>\n",
       "      <td>0.152632</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.533333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.964014   0.075204  0.125546 -0.050343  0.152632      0          3   \n",
       "\n",
       "   subsample  \n",
       "0   0.533333  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_tree_noOutliers,\n",
    "                                                   hp_logsaleprice_noOutliers,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 6)\n",
    "eta_ = np.linspace(0.1, 0.2, 20)\n",
    "gamma_ = [0]\n",
    "subsample_ = np.linspace(0.4, 0.6, 10)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_xgb.best_estimator_.n_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 33s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.884618</td>\n",
       "      <td>0.136491</td>\n",
       "      <td>0.151267</td>\n",
       "      <td>-0.014776</td>\n",
       "      <td>0.161053</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE       eta  gamma  max_depth  \\\n",
       "0  0.884618   0.136491  0.151267 -0.014776  0.161053      1          3   \n",
       "\n",
       "   subsample  \n",
       "0        0.7  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The above characteristics are:\n",
    "# {'eta': 0.13, 'gamma': 0.0001, 'max_depth': 4, 'subsample': 0.5}\n",
    "\n",
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 7)\n",
    "eta_ = np.linspace(0.12, 0.18, 20)\n",
    "gamma_ = [1]\n",
    "subsample_ = np.linspace(0.5, 0.9, 9)\n",
    "\n",
    "# evals=evals, early_stopping_rounds=10\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18min 56s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.978977</td>\n",
       "      <td>0.058262</td>\n",
       "      <td>0.134632</td>\n",
       "      <td>-0.076371</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.01</td>\n",
       "      <td>4</td>\n",
       "      <td>0.7875</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE    eta  gamma  max_depth  subsample\n",
       "0  0.978977   0.058262  0.134632 -0.076371  0.125   0.01          4     0.7875"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 6)\n",
    "eta_ = np.linspace(0.12, 0.16, 9)\n",
    "gamma_ = np.linspace(1e-2, 0.5, 9)\n",
    "subsample_ = np.linspace(0.6, 0.9, 9)\n",
    "\n",
    "# evals=evals, early_stopping_rounds=10\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 20min 21s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.955152</td>\n",
       "      <td>0.085096</td>\n",
       "      <td>0.138688</td>\n",
       "      <td>-0.053592</td>\n",
       "      <td>0.1325</td>\n",
       "      <td>0.1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE     eta  gamma  max_depth  \\\n",
       "0  0.955152   0.085096  0.138688 -0.053592  0.1325    0.1          4   \n",
       "\n",
       "   subsample  \n",
       "0       0.65  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 6)\n",
    "eta_ = np.linspace(0.11, 0.14, 9)\n",
    "gamma_ = np.linspace(1e-1, 1, 9)\n",
    "subsample_ = np.linspace(0.65, 0.9, 9)\n",
    "\n",
    "# evals=evals, early_stopping_rounds=10\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 55s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.961065</td>\n",
       "      <td>0.079288</td>\n",
       "      <td>0.137482</td>\n",
       "      <td>-0.058194</td>\n",
       "      <td>0.11375</td>\n",
       "      <td>3</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE      eta  max_depth  subsample\n",
       "0  0.961065   0.079288  0.137482 -0.058194  0.11375          3        0.9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 6)\n",
    "eta_ = np.linspace(0.11, 0.14, 9)\n",
    "# gamma_ = np.linspace(1e-1, 1, 9)\n",
    "subsample_ = np.linspace(0.65, 0.9, 9)\n",
    "\n",
    "# evals=evals, early_stopping_rounds=10\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "#               'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 0\n",
    "\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "    \n",
    "xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                   hp_logsaleprice,\n",
    "                                                   test_size=0.2,\n",
    "                                                   random_state=state)\n",
    "\n",
    "ytrain = ytrain.values.flatten()\n",
    "n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "xgb = xgbst.XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "\n",
    "maxdepth = range(2, 6)\n",
    "eta_ = np.linspace(0.11, 0.14, 9)\n",
    "# gamma_ = np.linspace(1e-1, 1, 9)\n",
    "subsample_ = np.linspace(0.65, 0.9, 9)\n",
    "\n",
    "# evals=evals, early_stopping_rounds=10\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "#               'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain, ytrain)\n",
    "\n",
    "l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "best_par_list.append(gs_xgb.best_params_)\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(gs_xgb.best_estimator_.predict(xtrain), ytrain, color='blue', alpha=0.4)\n",
    "plt.scatter(gs_xgb.best_estimator_.predict(xtest), ytest, color='red', alpha=0.4)\n",
    "plt.grid(which='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.972053</td>\n",
       "      <td>0.06724</td>\n",
       "      <td>0.10722</td>\n",
       "      <td>-0.039979</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00005</td>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Scores  TrainRMSE  TestRMSE  DiffRMSE   eta    gamma  max_depth  \\\n",
       "18  0.972053    0.06724   0.10722 -0.039979  0.14  0.00005          4   \n",
       "\n",
       "    subsample  \n",
       "18       0.45  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest_testrmse = xgb_res_df[xgb_res_df.TestRMSE==xgb_res_df.TestRMSE.min()]\n",
    "lowest_testrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:19:48\n",
      "[09:20:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:21:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:22:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:22:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:23:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:24:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:24:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:25:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:26:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:26:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:27:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:27:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:28:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:29:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:29:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:30:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:30:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:31:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:32:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:32:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "09:19:48\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.960583</td>\n",
       "      <td>0.079777</td>\n",
       "      <td>0.139362</td>\n",
       "      <td>-0.059584</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>3</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.973991</td>\n",
       "      <td>0.063589</td>\n",
       "      <td>0.140721</td>\n",
       "      <td>-0.077132</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.973230</td>\n",
       "      <td>0.064862</td>\n",
       "      <td>0.135140</td>\n",
       "      <td>-0.070279</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>4</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.983519</td>\n",
       "      <td>0.052425</td>\n",
       "      <td>0.126711</td>\n",
       "      <td>-0.074286</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>5</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.972219</td>\n",
       "      <td>0.067380</td>\n",
       "      <td>0.118663</td>\n",
       "      <td>-0.051283</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.954612</td>\n",
       "      <td>0.084473</td>\n",
       "      <td>0.128049</td>\n",
       "      <td>-0.043576</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.983763</td>\n",
       "      <td>0.050509</td>\n",
       "      <td>0.147159</td>\n",
       "      <td>-0.096650</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>5</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.983538</td>\n",
       "      <td>0.050892</td>\n",
       "      <td>0.127963</td>\n",
       "      <td>-0.077071</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>5</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.972539</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>0.135305</td>\n",
       "      <td>-0.068367</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>4</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.955898</td>\n",
       "      <td>0.084449</td>\n",
       "      <td>0.106838</td>\n",
       "      <td>-0.022390</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.971185</td>\n",
       "      <td>0.066869</td>\n",
       "      <td>0.144542</td>\n",
       "      <td>-0.077673</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>4</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.985467</td>\n",
       "      <td>0.048335</td>\n",
       "      <td>0.132536</td>\n",
       "      <td>-0.084201</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>5</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.974454</td>\n",
       "      <td>0.063372</td>\n",
       "      <td>0.138123</td>\n",
       "      <td>-0.074751</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.973487</td>\n",
       "      <td>0.064776</td>\n",
       "      <td>0.157813</td>\n",
       "      <td>-0.093037</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.956119</td>\n",
       "      <td>0.085018</td>\n",
       "      <td>0.121760</td>\n",
       "      <td>-0.036742</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.973214</td>\n",
       "      <td>0.065515</td>\n",
       "      <td>0.127307</td>\n",
       "      <td>-0.061792</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>4</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.956014</td>\n",
       "      <td>0.081885</td>\n",
       "      <td>0.162159</td>\n",
       "      <td>-0.080274</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.981331</td>\n",
       "      <td>0.054533</td>\n",
       "      <td>0.132350</td>\n",
       "      <td>-0.077817</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>5</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.972054</td>\n",
       "      <td>0.067240</td>\n",
       "      <td>0.107219</td>\n",
       "      <td>-0.039979</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.984405</td>\n",
       "      <td>0.051272</td>\n",
       "      <td>0.146368</td>\n",
       "      <td>-0.095096</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>5</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.954558</td>\n",
       "      <td>0.085392</td>\n",
       "      <td>0.132821</td>\n",
       "      <td>-0.047429</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>3</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Scores  TrainRMSE  TestRMSE  DiffRMSE    eta     gamma  max_depth  \\\n",
       "0   0.960583   0.079777  0.139362 -0.059584  0.141  0.000030          3   \n",
       "1   0.973991   0.063589  0.140721 -0.077132  0.140  0.000045          4   \n",
       "2   0.973230   0.064862  0.135140 -0.070279  0.141  0.000030          4   \n",
       "3   0.983519   0.052425  0.126711 -0.074286  0.140  0.000045          5   \n",
       "4   0.972219   0.067380  0.118663 -0.051283  0.140  0.000045          4   \n",
       "5   0.954612   0.084473  0.128049 -0.043576  0.140  0.000030          3   \n",
       "6   0.983763   0.050509  0.147159 -0.096650  0.141  0.000030          5   \n",
       "7   0.983538   0.050892  0.127963 -0.077071  0.141  0.000040          5   \n",
       "8   0.972539   0.066938  0.135305 -0.068367  0.140  0.000030          4   \n",
       "9   0.955898   0.084449  0.106838 -0.022390  0.141  0.000030          3   \n",
       "10  0.971185   0.066869  0.144542 -0.077673  0.140  0.000040          4   \n",
       "11  0.985467   0.048335  0.132536 -0.084201  0.141  0.000030          5   \n",
       "12  0.974454   0.063372  0.138123 -0.074751  0.139  0.000045          4   \n",
       "13  0.973487   0.064776  0.157813 -0.093037  0.140  0.000030          4   \n",
       "14  0.956119   0.085018  0.121760 -0.036742  0.140  0.000030          3   \n",
       "15  0.973214   0.065515  0.127307 -0.061792  0.141  0.000030          4   \n",
       "16  0.956014   0.081885  0.162159 -0.080274  0.139  0.000030          3   \n",
       "17  0.981331   0.054533  0.132350 -0.077817  0.139  0.000030          5   \n",
       "18  0.972054   0.067240  0.107219 -0.039979  0.140  0.000030          4   \n",
       "19  0.984405   0.051272  0.146368 -0.095096  0.141  0.000040          5   \n",
       "20  0.954558   0.085392  0.132821 -0.047429  0.139  0.000040          3   \n",
       "\n",
       "    subsample  \n",
       "0        0.44  \n",
       "1        0.45  \n",
       "2        0.44  \n",
       "3        0.44  \n",
       "4        0.45  \n",
       "5        0.45  \n",
       "6        0.44  \n",
       "7        0.45  \n",
       "8        0.46  \n",
       "9        0.45  \n",
       "10       0.44  \n",
       "11       0.46  \n",
       "12       0.45  \n",
       "13       0.45  \n",
       "14       0.46  \n",
       "15       0.46  \n",
       "16       0.46  \n",
       "17       0.44  \n",
       "18       0.45  \n",
       "19       0.44  \n",
       "20       0.44  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The above characteristics are:\n",
    "# {'eta': 0.13, 'gamma': 0.0001, 'max_depth': 4, 'subsample': 0.5}\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)\n",
    "\n",
    "randomstate = list(range(0, 21))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "c_list = []\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "\n",
    "for state in randomstate:\n",
    "    \n",
    "\n",
    "    xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                       hp_logsaleprice,\n",
    "                                                       test_size=0.2,\n",
    "                                                       random_state=state)\n",
    "\n",
    "    ytrain = ytrain.values.flatten()\n",
    "    \n",
    "    xgb = XGBRegressor(random_state=state, eval_method='rmse')\n",
    "    \n",
    "    n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "    \n",
    "    maxdepth = [3, 4, 5]\n",
    "    eta_ = (0.139, 0.14, 0.141)\n",
    "    gamma_ = [0.00003, 0.00004, 0.000045]\n",
    "    subsample_ = [0.44, 0.45, 0.46]\n",
    "\n",
    "\n",
    "    gparam_xgb = {'max_depth': maxdepth,\n",
    "                  'eta': eta_,\n",
    "                  'gamma': gamma_,\n",
    "                  'subsample': subsample_}\n",
    "\n",
    "    gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                          scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    gs_xgb.fit(xtrain, ytrain)\n",
    "    \n",
    "    c_list.append(counter)\n",
    "    l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "    train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "    test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "    best_par_list.append(gs_xgb.best_params_)\n",
    "    \n",
    "    counter+=1\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "\n",
    "c_list = list(map(int, c_list))\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "xgb_res_df.index = c_list\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "print(current_time)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.955898</td>\n",
       "      <td>0.084449</td>\n",
       "      <td>0.106838</td>\n",
       "      <td>-0.02239</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE    eta    gamma  max_depth  \\\n",
       "9  0.955898   0.084449  0.106838  -0.02239  0.141  0.00003          3   \n",
       "\n",
       "   subsample  \n",
       "9       0.45  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest_testrmse = xgb_res_df[xgb_res_df.TestRMSE==xgb_res_df.TestRMSE.min()]\n",
    "lowest_testrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "09:40:50\n",
      "[09:43:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:45:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:47:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:49:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:51:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:54:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:56:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:57:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[09:59:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:01:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:02:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:04:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:06:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:08:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:09:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:11:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:13:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:15:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:16:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:18:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[10:20:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "09:40:50\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.960583</td>\n",
       "      <td>0.079777</td>\n",
       "      <td>0.139362</td>\n",
       "      <td>-0.059584</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>3</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.973991</td>\n",
       "      <td>0.063589</td>\n",
       "      <td>0.140721</td>\n",
       "      <td>-0.077132</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.958725</td>\n",
       "      <td>0.080540</td>\n",
       "      <td>0.133158</td>\n",
       "      <td>-0.052618</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.935586</td>\n",
       "      <td>0.103642</td>\n",
       "      <td>0.121174</td>\n",
       "      <td>-0.017532</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.954736</td>\n",
       "      <td>0.086006</td>\n",
       "      <td>0.123490</td>\n",
       "      <td>-0.037483</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>3</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.970606</td>\n",
       "      <td>0.067980</td>\n",
       "      <td>0.125240</td>\n",
       "      <td>-0.057259</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>4</td>\n",
       "      <td>0.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.972525</td>\n",
       "      <td>0.065703</td>\n",
       "      <td>0.138295</td>\n",
       "      <td>-0.072592</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.983538</td>\n",
       "      <td>0.050892</td>\n",
       "      <td>0.127963</td>\n",
       "      <td>-0.077071</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>5</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.972539</td>\n",
       "      <td>0.066938</td>\n",
       "      <td>0.135305</td>\n",
       "      <td>-0.068367</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>4</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.955247</td>\n",
       "      <td>0.085070</td>\n",
       "      <td>0.104156</td>\n",
       "      <td>-0.019086</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.971185</td>\n",
       "      <td>0.066869</td>\n",
       "      <td>0.144542</td>\n",
       "      <td>-0.077673</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>4</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.985467</td>\n",
       "      <td>0.048335</td>\n",
       "      <td>0.132536</td>\n",
       "      <td>-0.084201</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>5</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.974454</td>\n",
       "      <td>0.063372</td>\n",
       "      <td>0.138123</td>\n",
       "      <td>-0.074751</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.973487</td>\n",
       "      <td>0.064776</td>\n",
       "      <td>0.157813</td>\n",
       "      <td>-0.093037</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.971080</td>\n",
       "      <td>0.069020</td>\n",
       "      <td>0.120966</td>\n",
       "      <td>-0.051946</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>4</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.972483</td>\n",
       "      <td>0.066404</td>\n",
       "      <td>0.124549</td>\n",
       "      <td>-0.058146</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>4</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.956014</td>\n",
       "      <td>0.081885</td>\n",
       "      <td>0.162159</td>\n",
       "      <td>-0.080274</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.981624</td>\n",
       "      <td>0.054104</td>\n",
       "      <td>0.132077</td>\n",
       "      <td>-0.077973</td>\n",
       "      <td>0.139</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>5</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.972054</td>\n",
       "      <td>0.067240</td>\n",
       "      <td>0.107219</td>\n",
       "      <td>-0.039979</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.986108</td>\n",
       "      <td>0.048392</td>\n",
       "      <td>0.144882</td>\n",
       "      <td>-0.096490</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>5</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.980892</td>\n",
       "      <td>0.055373</td>\n",
       "      <td>0.136845</td>\n",
       "      <td>-0.081472</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>5</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Scores  TrainRMSE  TestRMSE  DiffRMSE    eta     gamma  max_depth  \\\n",
       "0   0.960583   0.079777  0.139362 -0.059584  0.141  0.000020          3   \n",
       "1   0.973991   0.063589  0.140721 -0.077132  0.140  0.000045          4   \n",
       "2   0.958725   0.080540  0.133158 -0.052618  0.142  0.000020          3   \n",
       "3   0.935586   0.103642  0.121174 -0.017532  0.140  0.000020          2   \n",
       "4   0.954736   0.086006  0.123490 -0.037483  0.141  0.000020          3   \n",
       "5   0.970606   0.067980  0.125240 -0.057259  0.140  0.000045          4   \n",
       "6   0.972525   0.065703  0.138295 -0.072592  0.142  0.000020          4   \n",
       "7   0.983538   0.050892  0.127963 -0.077071  0.141  0.000040          5   \n",
       "8   0.972539   0.066938  0.135305 -0.068367  0.140  0.000030          4   \n",
       "9   0.955247   0.085070  0.104156 -0.019086  0.143  0.000020          3   \n",
       "10  0.971185   0.066869  0.144542 -0.077673  0.140  0.000040          4   \n",
       "11  0.985467   0.048335  0.132536 -0.084201  0.141  0.000030          5   \n",
       "12  0.974454   0.063372  0.138123 -0.074751  0.139  0.000045          4   \n",
       "13  0.973487   0.064776  0.157813 -0.093037  0.140  0.000030          4   \n",
       "14  0.971080   0.069020  0.120966 -0.051946  0.142  0.000045          4   \n",
       "15  0.972483   0.066404  0.124549 -0.058146  0.142  0.000030          4   \n",
       "16  0.956014   0.081885  0.162159 -0.080274  0.139  0.000020          3   \n",
       "17  0.981624   0.054104  0.132077 -0.077973  0.139  0.000020          5   \n",
       "18  0.972054   0.067240  0.107219 -0.039979  0.140  0.000030          4   \n",
       "19  0.986108   0.048392  0.144882 -0.096490  0.142  0.000040          5   \n",
       "20  0.980892   0.055373  0.136845 -0.081472  0.143  0.000040          5   \n",
       "\n",
       "    subsample  \n",
       "0        0.44  \n",
       "1        0.45  \n",
       "2        0.46  \n",
       "3        0.45  \n",
       "4        0.43  \n",
       "5        0.43  \n",
       "6        0.46  \n",
       "7        0.45  \n",
       "8        0.46  \n",
       "9        0.46  \n",
       "10       0.44  \n",
       "11       0.46  \n",
       "12       0.45  \n",
       "13       0.45  \n",
       "14       0.46  \n",
       "15       0.44  \n",
       "16       0.46  \n",
       "17       0.44  \n",
       "18       0.45  \n",
       "19       0.44  \n",
       "20       0.44  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The above characteristics are:\n",
    "# {'eta': 0.13, 'gamma': 0.0001, 'max_depth': 4, 'subsample': 0.5}\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)\n",
    "\n",
    "randomstate = list(range(0, 21))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "c_list = []\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "\n",
    "for state in randomstate:\n",
    "    \n",
    "\n",
    "    xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                       hp_logsaleprice,\n",
    "                                                       test_size=0.2,\n",
    "                                                       random_state=state)\n",
    "\n",
    "    ytrain = ytrain.values.flatten()\n",
    "    \n",
    "    xgb = XGBRegressor(random_state=state, eval_method='rmse')\n",
    "    \n",
    "    n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "    \n",
    "    maxdepth = [2, 3, 4, 5]\n",
    "    eta_ = (0.139, 0.14, 0.141, 0.142, 0.143)\n",
    "    gamma_ = [0.00002, 0.00003, 0.00004, 0.000045]\n",
    "    subsample_ = [0.43, 0.44, 0.45, 0.46] # use larger differences in the range\n",
    "\n",
    "\n",
    "    gparam_xgb = {'max_depth': maxdepth,\n",
    "                  'eta': eta_,\n",
    "                  'gamma': gamma_,\n",
    "                  'subsample': subsample_}\n",
    "\n",
    "    gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                          scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    gs_xgb.fit(xtrain, ytrain)\n",
    "    \n",
    "    c_list.append(counter)\n",
    "    l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "    train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "    test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "    best_par_list.append(gs_xgb.best_params_)\n",
    "    \n",
    "    counter+=1\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "\n",
    "c_list = list(map(int, c_list))\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "xgb_res_df.index = c_list\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "print(current_time)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.955247</td>\n",
       "      <td>0.08507</td>\n",
       "      <td>0.104156</td>\n",
       "      <td>-0.019086</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE    eta    gamma  max_depth  \\\n",
       "9  0.955247    0.08507  0.104156 -0.019086  0.143  0.00002          3   \n",
       "\n",
       "   subsample  \n",
       "9       0.46  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest_testrmse = xgb_res_df[xgb_res_df.TestRMSE==xgb_res_df.TestRMSE.min()]\n",
    "lowest_testrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11:14:09\n",
      "[11:17:14] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:20:00] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:22:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:25:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:28:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:31:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:34:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:37:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:40:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:44:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:47:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:50:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:53:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[11:57:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:01:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:05:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:08:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:11:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:14:24] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:17:15] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[12:20:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "11:14:09\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.963254</td>\n",
       "      <td>0.077026</td>\n",
       "      <td>0.135035</td>\n",
       "      <td>-0.058009</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>3</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.973839</td>\n",
       "      <td>0.063775</td>\n",
       "      <td>0.145963</td>\n",
       "      <td>-0.082188</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.984374</td>\n",
       "      <td>0.049555</td>\n",
       "      <td>0.131885</td>\n",
       "      <td>-0.082331</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>5</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.975306</td>\n",
       "      <td>0.064171</td>\n",
       "      <td>0.126402</td>\n",
       "      <td>-0.062231</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>4</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.973826</td>\n",
       "      <td>0.065402</td>\n",
       "      <td>0.125679</td>\n",
       "      <td>-0.060277</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>4</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.975160</td>\n",
       "      <td>0.062492</td>\n",
       "      <td>0.124848</td>\n",
       "      <td>-0.062356</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.972525</td>\n",
       "      <td>0.065703</td>\n",
       "      <td>0.138295</td>\n",
       "      <td>-0.072592</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.983538</td>\n",
       "      <td>0.050892</td>\n",
       "      <td>0.127963</td>\n",
       "      <td>-0.077071</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>5</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.957755</td>\n",
       "      <td>0.083024</td>\n",
       "      <td>0.137721</td>\n",
       "      <td>-0.054696</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.955247</td>\n",
       "      <td>0.085070</td>\n",
       "      <td>0.104156</td>\n",
       "      <td>-0.019086</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.973519</td>\n",
       "      <td>0.064104</td>\n",
       "      <td>0.142515</td>\n",
       "      <td>-0.078411</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.982929</td>\n",
       "      <td>0.052386</td>\n",
       "      <td>0.131992</td>\n",
       "      <td>-0.079606</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000010</td>\n",
       "      <td>5</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.958627</td>\n",
       "      <td>0.080647</td>\n",
       "      <td>0.138827</td>\n",
       "      <td>-0.058180</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.974600</td>\n",
       "      <td>0.063402</td>\n",
       "      <td>0.159504</td>\n",
       "      <td>-0.096102</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.955573</td>\n",
       "      <td>0.085545</td>\n",
       "      <td>0.118349</td>\n",
       "      <td>-0.032804</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>3</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.972483</td>\n",
       "      <td>0.066404</td>\n",
       "      <td>0.124549</td>\n",
       "      <td>-0.058146</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>4</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.956190</td>\n",
       "      <td>0.081722</td>\n",
       "      <td>0.165953</td>\n",
       "      <td>-0.084232</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.972140</td>\n",
       "      <td>0.066618</td>\n",
       "      <td>0.128383</td>\n",
       "      <td>-0.061765</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.974107</td>\n",
       "      <td>0.064722</td>\n",
       "      <td>0.107147</td>\n",
       "      <td>-0.042425</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>4</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.986108</td>\n",
       "      <td>0.048392</td>\n",
       "      <td>0.144882</td>\n",
       "      <td>-0.096490</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>5</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.980892</td>\n",
       "      <td>0.055373</td>\n",
       "      <td>0.136845</td>\n",
       "      <td>-0.081472</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>5</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Scores  TrainRMSE  TestRMSE  DiffRMSE    eta     gamma  max_depth  \\\n",
       "0   0.963254   0.077026  0.135035 -0.058009  0.142  0.000030          3   \n",
       "1   0.973839   0.063775  0.145963 -0.082188  0.141  0.000009          4   \n",
       "2   0.984374   0.049555  0.131885 -0.082331  0.141  0.000040          5   \n",
       "3   0.975306   0.064171  0.126402 -0.062231  0.143  0.000030          4   \n",
       "4   0.973826   0.065402  0.125679 -0.060277  0.144  0.000040          4   \n",
       "5   0.975160   0.062492  0.124848 -0.062356  0.145  0.000020          4   \n",
       "6   0.972525   0.065703  0.138295 -0.072592  0.142  0.000020          4   \n",
       "7   0.983538   0.050892  0.127963 -0.077071  0.141  0.000040          5   \n",
       "8   0.957755   0.083024  0.137721 -0.054696  0.142  0.000009          3   \n",
       "9   0.955247   0.085070  0.104156 -0.019086  0.143  0.000009          3   \n",
       "10  0.973519   0.064104  0.142515 -0.078411  0.141  0.000020          4   \n",
       "11  0.982929   0.052386  0.131992 -0.079606  0.143  0.000010          5   \n",
       "12  0.958627   0.080647  0.138827 -0.058180  0.143  0.000009          3   \n",
       "13  0.974600   0.063402  0.159504 -0.096102  0.145  0.000009          4   \n",
       "14  0.955573   0.085545  0.118349 -0.032804  0.143  0.000020          3   \n",
       "15  0.972483   0.066404  0.124549 -0.058146  0.142  0.000030          4   \n",
       "16  0.956190   0.081722  0.165953 -0.084232  0.143  0.000009          3   \n",
       "17  0.972140   0.066618  0.128383 -0.061765  0.141  0.000009          4   \n",
       "18  0.974107   0.064722  0.107147 -0.042425  0.145  0.000040          4   \n",
       "19  0.986108   0.048392  0.144882 -0.096490  0.142  0.000040          5   \n",
       "20  0.980892   0.055373  0.136845 -0.081472  0.143  0.000040          5   \n",
       "\n",
       "    subsample  \n",
       "0        0.48  \n",
       "1        0.45  \n",
       "2        0.47  \n",
       "3        0.47  \n",
       "4        0.47  \n",
       "5        0.47  \n",
       "6        0.46  \n",
       "7        0.45  \n",
       "8        0.46  \n",
       "9        0.46  \n",
       "10       0.45  \n",
       "11       0.48  \n",
       "12       0.47  \n",
       "13       0.47  \n",
       "14       0.48  \n",
       "15       0.44  \n",
       "16       0.45  \n",
       "17       0.47  \n",
       "18       0.48  \n",
       "19       0.44  \n",
       "20       0.44  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The above characteristics are:\n",
    "# {'eta': 0.13, 'gamma': 0.0001, 'max_depth': 4, 'subsample': 0.5}\n",
    "\n",
    "import time\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)\n",
    "\n",
    "randomstate = list(range(0, 21))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "c_list = []\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "\n",
    "for state in randomstate:\n",
    "    \n",
    "\n",
    "    xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_nonimpute,\n",
    "                                                       hp_logsaleprice,\n",
    "                                                       test_size=0.2,\n",
    "                                                       random_state=state)\n",
    "\n",
    "    ytrain = ytrain.values.flatten()\n",
    "    \n",
    "    xgb = XGBRegressor(random_state=state, eval_metric='rmse')\n",
    "    \n",
    "    n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "    \n",
    "    maxdepth = [2, 3, 4, 5]\n",
    "    eta_ = ( 0.141, 0.142, 0.143, 0.144, 0.145)\n",
    "    gamma_ = [0.000009, 0.00001, 0.00002, 0.00003, 0.00004]\n",
    "    subsample_ = [0.44, 0.45, 0.46, 0.47, 0.48]\n",
    "\n",
    "\n",
    "    gparam_xgb = {'max_depth': maxdepth,\n",
    "                  'eta': eta_,\n",
    "                  'gamma': gamma_,\n",
    "                  'subsample': subsample_}\n",
    "\n",
    "    gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                          scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    gs_xgb.fit(xtrain, ytrain)\n",
    "    \n",
    "    c_list.append(counter)\n",
    "    l_scores.append(gs_xgb.best_estimator_.score(xtrain, ytrain))\n",
    "    train_rmse.append(rmse(gs_xgb, ytrain, xtrain))\n",
    "    test_rmse.append(rmse(gs_xgb, ytest, xtest))\n",
    "    best_par_list.append(gs_xgb.best_params_)\n",
    "    \n",
    "    counter+=1\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "\n",
    "c_list = list(map(int, c_list))\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "xgb_res_df.index = c_list\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "print(current_time)\n",
    "\n",
    "xgb_res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.955247</td>\n",
       "      <td>0.08507</td>\n",
       "      <td>0.104156</td>\n",
       "      <td>-0.019086</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Scores  TrainRMSE  TestRMSE  DiffRMSE    eta     gamma  max_depth  \\\n",
       "9  0.955247    0.08507  0.104156 -0.019086  0.143  0.000009          3   \n",
       "\n",
       "   subsample  \n",
       "9       0.46  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest_testrmse = xgb_res_df[xgb_res_df.TestRMSE==xgb_res_df.TestRMSE.min()]\n",
    "lowest_testrmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# After Removing Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:07:32\n",
      "[14:10:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:13:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:18:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:22:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:26:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:30:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:33:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:37:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:40:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:43:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:47:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:50:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:53:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:56:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[14:58:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:01:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:04:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:07:28] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:10:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:13:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[15:16:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "14:07:32\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.940380</td>\n",
       "      <td>0.096799</td>\n",
       "      <td>0.125671</td>\n",
       "      <td>-0.028872</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>2</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.959704</td>\n",
       "      <td>0.079136</td>\n",
       "      <td>0.128594</td>\n",
       "      <td>-0.049459</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.974715</td>\n",
       "      <td>0.062818</td>\n",
       "      <td>0.124414</td>\n",
       "      <td>-0.061597</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.940794</td>\n",
       "      <td>0.098425</td>\n",
       "      <td>0.117985</td>\n",
       "      <td>-0.019561</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>2</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.962120</td>\n",
       "      <td>0.077991</td>\n",
       "      <td>0.123130</td>\n",
       "      <td>-0.045139</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.965291</td>\n",
       "      <td>0.074909</td>\n",
       "      <td>0.136396</td>\n",
       "      <td>-0.061486</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.937924</td>\n",
       "      <td>0.097852</td>\n",
       "      <td>0.120042</td>\n",
       "      <td>-0.022189</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>2</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.975810</td>\n",
       "      <td>0.061601</td>\n",
       "      <td>0.135599</td>\n",
       "      <td>-0.073998</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.959853</td>\n",
       "      <td>0.081374</td>\n",
       "      <td>0.117357</td>\n",
       "      <td>-0.035983</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.960989</td>\n",
       "      <td>0.078133</td>\n",
       "      <td>0.122782</td>\n",
       "      <td>-0.044649</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.961799</td>\n",
       "      <td>0.077995</td>\n",
       "      <td>0.137641</td>\n",
       "      <td>-0.059646</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.960072</td>\n",
       "      <td>0.080397</td>\n",
       "      <td>0.119085</td>\n",
       "      <td>-0.038687</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.958193</td>\n",
       "      <td>0.081176</td>\n",
       "      <td>0.123198</td>\n",
       "      <td>-0.042022</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.961212</td>\n",
       "      <td>0.078378</td>\n",
       "      <td>0.127450</td>\n",
       "      <td>-0.049072</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.942955</td>\n",
       "      <td>0.095140</td>\n",
       "      <td>0.125934</td>\n",
       "      <td>-0.030794</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.976363</td>\n",
       "      <td>0.061224</td>\n",
       "      <td>0.130999</td>\n",
       "      <td>-0.069775</td>\n",
       "      <td>0.144</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>4</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.934807</td>\n",
       "      <td>0.100752</td>\n",
       "      <td>0.117142</td>\n",
       "      <td>-0.016390</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>2</td>\n",
       "      <td>0.46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.976054</td>\n",
       "      <td>0.061715</td>\n",
       "      <td>0.122579</td>\n",
       "      <td>-0.060864</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>4</td>\n",
       "      <td>0.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.936149</td>\n",
       "      <td>0.100458</td>\n",
       "      <td>0.118412</td>\n",
       "      <td>-0.017955</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>2</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.962784</td>\n",
       "      <td>0.077086</td>\n",
       "      <td>0.130771</td>\n",
       "      <td>-0.053685</td>\n",
       "      <td>0.141</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>3</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.937904</td>\n",
       "      <td>0.098587</td>\n",
       "      <td>0.116041</td>\n",
       "      <td>-0.017454</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>2</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Scores  TrainRMSE  TestRMSE  DiffRMSE    eta     gamma  max_depth  \\\n",
       "0   0.940380   0.096799  0.125671 -0.028872  0.145  0.000009          2   \n",
       "1   0.959704   0.079136  0.128594 -0.049459  0.145  0.000009          3   \n",
       "2   0.974715   0.062818  0.124414 -0.061597  0.142  0.000009          4   \n",
       "3   0.940794   0.098425  0.117985 -0.019561  0.141  0.000009          2   \n",
       "4   0.962120   0.077991  0.123130 -0.045139  0.144  0.000009          3   \n",
       "5   0.965291   0.074909  0.136396 -0.061486  0.144  0.000009          3   \n",
       "6   0.937924   0.097852  0.120042 -0.022189  0.141  0.000009          2   \n",
       "7   0.975810   0.061601  0.135599 -0.073998  0.145  0.000020          4   \n",
       "8   0.959853   0.081374  0.117357 -0.035983  0.143  0.000009          3   \n",
       "9   0.960989   0.078133  0.122782 -0.044649  0.144  0.000009          3   \n",
       "10  0.961799   0.077995  0.137641 -0.059646  0.141  0.000009          3   \n",
       "11  0.960072   0.080397  0.119085 -0.038687  0.142  0.000009          3   \n",
       "12  0.958193   0.081176  0.123198 -0.042022  0.145  0.000009          3   \n",
       "13  0.961212   0.078378  0.127450 -0.049072  0.143  0.000009          3   \n",
       "14  0.942955   0.095140  0.125934 -0.030794  0.145  0.000009          2   \n",
       "15  0.976363   0.061224  0.130999 -0.069775  0.144  0.000020          4   \n",
       "16  0.934807   0.100752  0.117142 -0.016390  0.143  0.000009          2   \n",
       "17  0.976054   0.061715  0.122579 -0.060864  0.145  0.000009          4   \n",
       "18  0.936149   0.100458  0.118412 -0.017955  0.142  0.000009          2   \n",
       "19  0.962784   0.077086  0.130771 -0.053685  0.141  0.000009          3   \n",
       "20  0.937904   0.098587  0.116041 -0.017454  0.142  0.000009          2   \n",
       "\n",
       "    subsample  \n",
       "0        0.48  \n",
       "1        0.45  \n",
       "2        0.48  \n",
       "3        0.44  \n",
       "4        0.47  \n",
       "5        0.48  \n",
       "6        0.46  \n",
       "7        0.45  \n",
       "8        0.45  \n",
       "9        0.48  \n",
       "10       0.46  \n",
       "11       0.45  \n",
       "12       0.45  \n",
       "13       0.46  \n",
       "14       0.45  \n",
       "15       0.46  \n",
       "16       0.46  \n",
       "17       0.48  \n",
       "18       0.45  \n",
       "19       0.47  \n",
       "20       0.47  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The above characteristics are:\n",
    "# {'eta': 0.13, 'gamma': 0.0001, 'max_depth': 4, 'subsample': 0.5}\n",
    "\n",
    "import time\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)\n",
    "\n",
    "randomstate = list(range(0, 21))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "c_list = []\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "\n",
    "for state in randomstate:\n",
    "    \n",
    "    \n",
    "    xtrain_gb_out, xtest_gb_out, ytrain_gb_out, ytest_gb_out = ms.train_test_split(hp_tree_noOutliers, \n",
    "                                                                                   hp_logsaleprice_noOutliers,\n",
    "                                                                                   test_size=0.2,\n",
    "                                                                                   random_state=state)\n",
    "\n",
    "    ytrain_gb_out = ytrain_gb_out.values.flatten()\n",
    "    \n",
    "    xgb = XGBRegressor(random_state=state, eval_method='rmse')\n",
    "    \n",
    "    n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "    \n",
    "    maxdepth = [2, 3, 4, 5]\n",
    "    eta_ = ( 0.141, 0.142, 0.143, 0.144, 0.145)\n",
    "    gamma_ = [0.000009, 0.00001, 0.00002, 0.00003, 0.00004]\n",
    "    subsample_ = [0.44, 0.45, 0.46, 0.47, 0.48]\n",
    "\n",
    "\n",
    "    gparam_xgb = {'max_depth': maxdepth,\n",
    "                  'eta': eta_,\n",
    "                  'gamma': gamma_,\n",
    "                  'subsample': subsample_}\n",
    "\n",
    "    gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                          scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    gs_xgb.fit(xtrain_gb_out, ytrain_gb_out)\n",
    "    \n",
    "    c_list.append(counter)\n",
    "    l_scores.append(gs_xgb.best_estimator_.score(xtrain_gb_out, ytrain_gb_out))\n",
    "    train_rmse.append(rmse(gs_xgb, ytrain_gb_out, xtrain_gb_out))\n",
    "    test_rmse.append(rmse(gs_xgb, ytest_gb_out, xtest_gb_out))\n",
    "    best_par_list.append(gs_xgb.best_params_)\n",
    "    \n",
    "    counter+=1\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "\n",
    "c_list = list(map(int, c_list))\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "xgb_res_df.index = c_list\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "print(current_time)\n",
    "\n",
    "xgb_res_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.937904</td>\n",
       "      <td>0.098587</td>\n",
       "      <td>0.116041</td>\n",
       "      <td>-0.017454</td>\n",
       "      <td>0.142</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>2</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Scores  TrainRMSE  TestRMSE  DiffRMSE    eta     gamma  max_depth  \\\n",
       "20  0.937904   0.098587  0.116041 -0.017454  0.142  0.000009          2   \n",
       "\n",
       "    subsample  \n",
       "20       0.47  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest_testrmse = xgb_res_df[xgb_res_df.TestRMSE==xgb_res_df.TestRMSE.min()]\n",
    "lowest_testrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16:14:11\n",
      "[16:16:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:18:35] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:20:41] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:22:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:24:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:26:57] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:29:03] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:31:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:33:10] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:35:06] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:37:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:38:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:41:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:43:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:45:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:47:32] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:49:45] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:51:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:53:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:55:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[16:57:12] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "16:14:11\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.960063</td>\n",
       "      <td>0.079224</td>\n",
       "      <td>0.130279</td>\n",
       "      <td>-0.051054</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.957954</td>\n",
       "      <td>0.080835</td>\n",
       "      <td>0.125234</td>\n",
       "      <td>-0.044399</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>3</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.974715</td>\n",
       "      <td>0.062818</td>\n",
       "      <td>0.124414</td>\n",
       "      <td>-0.061597</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>4</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.958968</td>\n",
       "      <td>0.081937</td>\n",
       "      <td>0.116829</td>\n",
       "      <td>-0.034891</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.975204</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.125993</td>\n",
       "      <td>-0.062894</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>4</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.943573</td>\n",
       "      <td>0.095512</td>\n",
       "      <td>0.145727</td>\n",
       "      <td>-0.050215</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>2</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.938486</td>\n",
       "      <td>0.097409</td>\n",
       "      <td>0.122570</td>\n",
       "      <td>-0.025161</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>2</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.961685</td>\n",
       "      <td>0.077527</td>\n",
       "      <td>0.133875</td>\n",
       "      <td>-0.056347</td>\n",
       "      <td>0.1415</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3</td>\n",
       "      <td>0.480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.958867</td>\n",
       "      <td>0.082367</td>\n",
       "      <td>0.116152</td>\n",
       "      <td>-0.033785</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3</td>\n",
       "      <td>0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.974437</td>\n",
       "      <td>0.063249</td>\n",
       "      <td>0.122386</td>\n",
       "      <td>-0.059137</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>4</td>\n",
       "      <td>0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.960608</td>\n",
       "      <td>0.079202</td>\n",
       "      <td>0.128073</td>\n",
       "      <td>-0.048871</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.962925</td>\n",
       "      <td>0.077473</td>\n",
       "      <td>0.117364</td>\n",
       "      <td>-0.039891</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.938490</td>\n",
       "      <td>0.098464</td>\n",
       "      <td>0.120461</td>\n",
       "      <td>-0.021997</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>2</td>\n",
       "      <td>0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.942248</td>\n",
       "      <td>0.095639</td>\n",
       "      <td>0.127780</td>\n",
       "      <td>-0.032141</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>2</td>\n",
       "      <td>0.475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.941409</td>\n",
       "      <td>0.096420</td>\n",
       "      <td>0.122195</td>\n",
       "      <td>-0.025775</td>\n",
       "      <td>0.1410</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>2</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.961720</td>\n",
       "      <td>0.077913</td>\n",
       "      <td>0.125187</td>\n",
       "      <td>-0.047273</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.958227</td>\n",
       "      <td>0.080650</td>\n",
       "      <td>0.115014</td>\n",
       "      <td>-0.034365</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.959818</td>\n",
       "      <td>0.079945</td>\n",
       "      <td>0.121839</td>\n",
       "      <td>-0.041894</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.959651</td>\n",
       "      <td>0.079857</td>\n",
       "      <td>0.117227</td>\n",
       "      <td>-0.037369</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.962030</td>\n",
       "      <td>0.077863</td>\n",
       "      <td>0.133040</td>\n",
       "      <td>-0.055177</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3</td>\n",
       "      <td>0.465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.937909</td>\n",
       "      <td>0.098583</td>\n",
       "      <td>0.119302</td>\n",
       "      <td>-0.020720</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>2</td>\n",
       "      <td>0.475</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Scores  TrainRMSE  TestRMSE  DiffRMSE     eta     gamma  max_depth  \\\n",
       "0   0.960063   0.079224  0.130279 -0.051054  0.1430  0.000006          3   \n",
       "1   0.957954   0.080835  0.125234 -0.044399  0.1425  0.000007          3   \n",
       "2   0.974715   0.062818  0.124414 -0.061597  0.1420  0.000006          4   \n",
       "3   0.958968   0.081937  0.116829 -0.034891  0.1420  0.000006          3   \n",
       "4   0.975204   0.063100  0.125993 -0.062894  0.1425  0.000006          4   \n",
       "5   0.943573   0.095512  0.145727 -0.050215  0.1425  0.000006          2   \n",
       "6   0.938486   0.097409  0.122570 -0.025161  0.1415  0.000006          2   \n",
       "7   0.961685   0.077527  0.133875 -0.056347  0.1415  0.000006          3   \n",
       "8   0.958867   0.082367  0.116152 -0.033785  0.1430  0.000006          3   \n",
       "9   0.974437   0.063249  0.122386 -0.059137  0.1425  0.000006          4   \n",
       "10  0.960608   0.079202  0.128073 -0.048871  0.1410  0.000006          3   \n",
       "11  0.962925   0.077473  0.117364 -0.039891  0.1430  0.000006          3   \n",
       "12  0.938490   0.098464  0.120461 -0.021997  0.1430  0.000006          2   \n",
       "13  0.942248   0.095639  0.127780 -0.032141  0.1410  0.000006          2   \n",
       "14  0.941409   0.096420  0.122195 -0.025775  0.1410  0.000006          2   \n",
       "15  0.961720   0.077913  0.125187 -0.047273  0.1430  0.000006          3   \n",
       "16  0.958227   0.080650  0.115014 -0.034365  0.1430  0.000006          3   \n",
       "17  0.959818   0.079945  0.121839 -0.041894  0.1430  0.000006          3   \n",
       "18  0.959651   0.079857  0.117227 -0.037369  0.1420  0.000006          3   \n",
       "19  0.962030   0.077863  0.133040 -0.055177  0.1420  0.000006          3   \n",
       "20  0.937909   0.098583  0.119302 -0.020720  0.1420  0.000006          2   \n",
       "\n",
       "    subsample  \n",
       "0       0.465  \n",
       "1       0.465  \n",
       "2       0.480  \n",
       "3       0.480  \n",
       "4       0.470  \n",
       "5       0.465  \n",
       "6       0.465  \n",
       "7       0.480  \n",
       "8       0.475  \n",
       "9       0.475  \n",
       "10      0.465  \n",
       "11      0.470  \n",
       "12      0.475  \n",
       "13      0.475  \n",
       "14      0.465  \n",
       "15      0.470  \n",
       "16      0.470  \n",
       "17      0.465  \n",
       "18      0.470  \n",
       "19      0.465  \n",
       "20      0.475  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The above characteristics are:\n",
    "# {'eta': 0.13, 'gamma': 0.0001, 'max_depth': 4, 'subsample': 0.5}\n",
    "\n",
    "import time\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)\n",
    "\n",
    "randomstate = list(range(0, 21))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "c_list = []\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "\n",
    "for state in randomstate:\n",
    "    \n",
    "    \n",
    "    xtrain_gb_out, xtest_gb_out, ytrain_gb_out, ytest_gb_out = ms.train_test_split(hp_tree_noOutliers, \n",
    "                                                                                   hp_logsaleprice_noOutliers,\n",
    "                                                                                   test_size=0.2,\n",
    "                                                                                   random_state=state)\n",
    "\n",
    "    ytrain_gb_out = ytrain_gb_out.values.flatten()\n",
    "    \n",
    "    xgb = XGBRegressor(random_state=state, eval_method='rmse')\n",
    "    \n",
    "    n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "    \n",
    "    maxdepth = [1, 2, 3, 4]\n",
    "    eta_ = (0.141, 0.1415, 0.142, 0.1425, 0.143)\n",
    "    gamma_ = [0.000006, 0.000007, 0.000008, 0.000009, 0.00001]\n",
    "    subsample_ = [0.465, 0.47, 0.475, 0.48]\n",
    "\n",
    "\n",
    "    gparam_xgb = {'max_depth': maxdepth,\n",
    "                  'eta': eta_,\n",
    "                  'gamma': gamma_,\n",
    "                  'subsample': subsample_}\n",
    "\n",
    "    gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                          scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    gs_xgb.fit(xtrain_gb_out, ytrain_gb_out)\n",
    "    \n",
    "    c_list.append(counter)\n",
    "    l_scores.append(gs_xgb.best_estimator_.score(xtrain_gb_out, ytrain_gb_out))\n",
    "    train_rmse.append(rmse(gs_xgb, ytrain_gb_out, xtrain_gb_out))\n",
    "    test_rmse.append(rmse(gs_xgb, ytest_gb_out, xtest_gb_out))\n",
    "    best_par_list.append(gs_xgb.best_params_)\n",
    "    \n",
    "    counter+=1\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "\n",
    "c_list = list(map(int, c_list))\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "xgb_res_df.index = c_list\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "print(current_time)\n",
    "\n",
    "xgb_res_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.958227</td>\n",
       "      <td>0.08065</td>\n",
       "      <td>0.115014</td>\n",
       "      <td>-0.034365</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>3</td>\n",
       "      <td>0.47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Scores  TrainRMSE  TestRMSE  DiffRMSE    eta     gamma  max_depth  \\\n",
       "16  0.958227    0.08065  0.115014 -0.034365  0.143  0.000006          3   \n",
       "\n",
       "    subsample  \n",
       "16       0.47  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lowest_testrmse = xgb_res_df[xgb_res_df.TestRMSE==xgb_res_df.TestRMSE.min()]\n",
    "lowest_testrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:03:14\n",
      "[17:05:36] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:07:44] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:09:46] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:11:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:14:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:16:27] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:18:50] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:21:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:23:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:25:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:27:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:30:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:33:31] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:36:18] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:39:25] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:42:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:45:21] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:49:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:53:37] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[17:57:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[18:01:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.3.0/src/learner.cc:541: \n",
      "Parameters: { eval_method } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "17:03:14\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Scores</th>\n",
       "      <th>TrainRMSE</th>\n",
       "      <th>TestRMSE</th>\n",
       "      <th>DiffRMSE</th>\n",
       "      <th>eta</th>\n",
       "      <th>gamma</th>\n",
       "      <th>max_depth</th>\n",
       "      <th>subsample</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.940542</td>\n",
       "      <td>0.096667</td>\n",
       "      <td>0.131031</td>\n",
       "      <td>-0.034364</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>2</td>\n",
       "      <td>0.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.973622</td>\n",
       "      <td>0.064027</td>\n",
       "      <td>0.127201</td>\n",
       "      <td>-0.063173</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>4</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.941453</td>\n",
       "      <td>0.095587</td>\n",
       "      <td>0.129316</td>\n",
       "      <td>-0.033729</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>2</td>\n",
       "      <td>0.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.960674</td>\n",
       "      <td>0.080216</td>\n",
       "      <td>0.110393</td>\n",
       "      <td>-0.030177</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3</td>\n",
       "      <td>0.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.975204</td>\n",
       "      <td>0.063100</td>\n",
       "      <td>0.125993</td>\n",
       "      <td>-0.062894</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>4</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.946099</td>\n",
       "      <td>0.093350</td>\n",
       "      <td>0.143805</td>\n",
       "      <td>-0.050455</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>2</td>\n",
       "      <td>0.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.939947</td>\n",
       "      <td>0.096245</td>\n",
       "      <td>0.118367</td>\n",
       "      <td>-0.022122</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>2</td>\n",
       "      <td>0.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.963931</td>\n",
       "      <td>0.075221</td>\n",
       "      <td>0.132865</td>\n",
       "      <td>-0.057645</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3</td>\n",
       "      <td>0.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.959909</td>\n",
       "      <td>0.081317</td>\n",
       "      <td>0.116117</td>\n",
       "      <td>-0.034800</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3</td>\n",
       "      <td>0.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.959684</td>\n",
       "      <td>0.079430</td>\n",
       "      <td>0.121699</td>\n",
       "      <td>-0.042269</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.963337</td>\n",
       "      <td>0.076410</td>\n",
       "      <td>0.132982</td>\n",
       "      <td>-0.056572</td>\n",
       "      <td>0.1420</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3</td>\n",
       "      <td>0.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.961492</td>\n",
       "      <td>0.078955</td>\n",
       "      <td>0.117859</td>\n",
       "      <td>-0.038904</td>\n",
       "      <td>0.1450</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3</td>\n",
       "      <td>0.471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.939016</td>\n",
       "      <td>0.098042</td>\n",
       "      <td>0.121555</td>\n",
       "      <td>-0.023514</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>2</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.942294</td>\n",
       "      <td>0.095601</td>\n",
       "      <td>0.126491</td>\n",
       "      <td>-0.030890</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>2</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.962381</td>\n",
       "      <td>0.077260</td>\n",
       "      <td>0.121231</td>\n",
       "      <td>-0.043971</td>\n",
       "      <td>0.1500</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3</td>\n",
       "      <td>0.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.961720</td>\n",
       "      <td>0.077913</td>\n",
       "      <td>0.125187</td>\n",
       "      <td>-0.047273</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.958227</td>\n",
       "      <td>0.080650</td>\n",
       "      <td>0.115014</td>\n",
       "      <td>-0.034365</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.975075</td>\n",
       "      <td>0.062963</td>\n",
       "      <td>0.119102</td>\n",
       "      <td>-0.056139</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>4</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.939753</td>\n",
       "      <td>0.097581</td>\n",
       "      <td>0.117668</td>\n",
       "      <td>-0.020087</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>2</td>\n",
       "      <td>0.469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.977176</td>\n",
       "      <td>0.060369</td>\n",
       "      <td>0.127721</td>\n",
       "      <td>-0.067353</td>\n",
       "      <td>0.1440</td>\n",
       "      <td>4.000000e-06</td>\n",
       "      <td>4</td>\n",
       "      <td>0.470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.959393</td>\n",
       "      <td>0.079724</td>\n",
       "      <td>0.120381</td>\n",
       "      <td>-0.040658</td>\n",
       "      <td>0.1430</td>\n",
       "      <td>1.000000e-07</td>\n",
       "      <td>3</td>\n",
       "      <td>0.469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Scores  TrainRMSE  TestRMSE  DiffRMSE     eta         gamma  max_depth  \\\n",
       "0   0.940542   0.096667  0.131031 -0.034364  0.1420  1.000000e-07          2   \n",
       "1   0.973622   0.064027  0.127201 -0.063173  0.1420  1.000000e-07          4   \n",
       "2   0.941453   0.095587  0.129316 -0.033729  0.1425  1.000000e-07          2   \n",
       "3   0.960674   0.080216  0.110393 -0.030177  0.1420  1.000000e-07          3   \n",
       "4   0.975204   0.063100  0.125993 -0.062894  0.1425  1.000000e-07          4   \n",
       "5   0.946099   0.093350  0.143805 -0.050455  0.1450  1.000000e-07          2   \n",
       "6   0.939947   0.096245  0.118367 -0.022122  0.1500  1.000000e-07          2   \n",
       "7   0.963931   0.075221  0.132865 -0.057645  0.1440  1.000000e-07          3   \n",
       "8   0.959909   0.081317  0.116117 -0.034800  0.1425  1.000000e-07          3   \n",
       "9   0.959684   0.079430  0.121699 -0.042269  0.1450  1.000000e-07          3   \n",
       "10  0.963337   0.076410  0.132982 -0.056572  0.1420  1.000000e-07          3   \n",
       "11  0.961492   0.078955  0.117859 -0.038904  0.1450  1.000000e-07          3   \n",
       "12  0.939016   0.098042  0.121555 -0.023514  0.1500  1.000000e-07          2   \n",
       "13  0.942294   0.095601  0.126491 -0.030890  0.1500  1.000000e-07          2   \n",
       "14  0.962381   0.077260  0.121231 -0.043971  0.1500  1.000000e-07          3   \n",
       "15  0.961720   0.077913  0.125187 -0.047273  0.1430  1.000000e-07          3   \n",
       "16  0.958227   0.080650  0.115014 -0.034365  0.1430  1.000000e-07          3   \n",
       "17  0.975075   0.062963  0.119102 -0.056139  0.1425  1.000000e-07          4   \n",
       "18  0.939753   0.097581  0.117668 -0.020087  0.1425  1.000000e-07          2   \n",
       "19  0.977176   0.060369  0.127721 -0.067353  0.1440  4.000000e-06          4   \n",
       "20  0.959393   0.079724  0.120381 -0.040658  0.1430  1.000000e-07          3   \n",
       "\n",
       "    subsample  \n",
       "0       0.471  \n",
       "1       0.470  \n",
       "2       0.471  \n",
       "3       0.469  \n",
       "4       0.470  \n",
       "5       0.471  \n",
       "6       0.471  \n",
       "7       0.471  \n",
       "8       0.469  \n",
       "9       0.470  \n",
       "10      0.469  \n",
       "11      0.471  \n",
       "12      0.470  \n",
       "13      0.470  \n",
       "14      0.469  \n",
       "15      0.470  \n",
       "16      0.470  \n",
       "17      0.470  \n",
       "18      0.469  \n",
       "19      0.470  \n",
       "20      0.469  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The above characteristics are:\n",
    "# {'eta': 0.13, 'gamma': 0.0001, 'max_depth': 4, 'subsample': 0.5}\n",
    "\n",
    "import time\n",
    "t = time.localtime()\n",
    "current_time = time.strftime(\"%H:%M:%S\", t)\n",
    "print(current_time)\n",
    "\n",
    "randomstate = list(range(0, 21))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "c_list = []\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "\n",
    "for state in randomstate:\n",
    "    \n",
    "    \n",
    "    xtrain_gb_out, xtest_gb_out, ytrain_gb_out, ytest_gb_out = ms.train_test_split(hp_tree_noOutliers, \n",
    "                                                                                   hp_logsaleprice_noOutliers,\n",
    "                                                                                   test_size=0.2,\n",
    "                                                                                   random_state=state)\n",
    "\n",
    "    ytrain_gb_out = ytrain_gb_out.values.flatten()\n",
    "    \n",
    "    xgb = XGBRegressor(random_state=state, eval_method='rmse')\n",
    "    \n",
    "    n_folds = ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "    \n",
    "    maxdepth = [2, 3, 4]\n",
    "    eta_ = (0.142, 0.1425, 0.143, 0.144, 0.145, 0.15)\n",
    "    gamma_ = [1e-7, 5e-7, 1e-6, 0.000004, 0.000005, 0.000006, 0.000007]\n",
    "    subsample_ = [0.469, 0.47, 0.471]\n",
    "\n",
    "\n",
    "    gparam_xgb = {'max_depth': maxdepth,\n",
    "                  'eta': eta_,\n",
    "                  'gamma': gamma_,\n",
    "                  'subsample': subsample_}\n",
    "\n",
    "    gs_xgb = ms.GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                          scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    gs_xgb.fit(xtrain_gb_out, ytrain_gb_out)\n",
    "    \n",
    "    c_list.append(counter)\n",
    "    l_scores.append(gs_xgb.best_estimator_.score(xtrain_gb_out, ytrain_gb_out))\n",
    "    train_rmse.append(rmse(gs_xgb, ytrain_gb_out, xtrain_gb_out))\n",
    "    test_rmse.append(rmse(gs_xgb, ytest_gb_out, xtest_gb_out))\n",
    "    best_par_list.append(gs_xgb.best_params_)\n",
    "    \n",
    "    counter+=1\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "\n",
    "c_list = list(map(int, c_list))\n",
    "    \n",
    "xgb_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "xgb_res_df = pd.DataFrame(xgb_list_results).T\n",
    "xgb_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "xgb_res_df.index = c_list\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "xgb_res_df = pd.concat([xgb_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "print(current_time)\n",
    "\n",
    "xgb_res_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowest_testrmse = xgb_res_df[xgb_res_df.TestRMSE==xgb_res_df.TestRMSE.min()]\n",
    "lowest_testrmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(gs_xgb.best_estimator_.predict(xtrain), ytrain, color='blue', alpha=0.4)\n",
    "plt.scatter(gs_xgb.best_estimator_.predict(xtest), ytest, color='red', alpha=0.4)\n",
    "plt.grid(which='both')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Based on the above, we will assume that our parameters are reasonable and will vary the random_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomstate = list(range(0, 5))\n",
    "\n",
    "counter = 0\n",
    "\n",
    "c_list = []\n",
    "l_scores = []\n",
    "train_rmse = []\n",
    "test_rmse = []\n",
    "best_par_list = []\n",
    "\n",
    "for state in randomstate:\n",
    "    \n",
    "    xtrain, xtest, ytrain, ytest = ms.train_test_split(hp_linear_fullyimputed,\n",
    "                                                       hp_logsaleprice,\n",
    "                                                       test_size=0.2,\n",
    "                                                       random_state=state)\n",
    "    ytrain = ytrain.values.flatten()\n",
    "    \n",
    "    xgb = XGBRegressor(random_state=state, eval_method='rmse', tree_method='gpu_hist')\n",
    "\n",
    "    maxdepth = range(2, 6)\n",
    "    eta_ = np.linspace(1e-2, 1, 10)\n",
    "    gamma_ = np.linspace(1e-5, 100, 30)\n",
    "    subsample_ = np.linspace(0, 1, 3)\n",
    "    # updater_ = [grow_colmaker,prune,grow_gpu_hist]\n",
    "\n",
    "    n_folds=ms.KFold(n_splits=5, random_state=state, shuffle=True)\n",
    "\n",
    "    gparam_xgb = {'max_depth': maxdepth,\n",
    "                  'eta': eta_,\n",
    "                  'gamma': gamma_,\n",
    "                  'subsample': subsample_}\n",
    "\n",
    "\n",
    "    gs_lasso = ms.GridSearchCV(lasso, gparam_lasso, cv=n_folds, refit=True, n_jobs=-1,\n",
    "                          scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "    \n",
    "    gs_lasso.fit(xtrain, ytrain)\n",
    "    \n",
    "    c_list.append(counter)\n",
    "    l_scores.append(gs_lasso.best_estimator_.score(xtrain, ytrain))\n",
    "    train_rmse.append(rmse(gs_lasso, ytrain, xtrain))\n",
    "    test_rmse.append(rmse(gs_lasso, ytest, xtest))\n",
    "    best_par_list.append(gs_lasso.best_params_)\n",
    "    \n",
    "    counter+=1\n",
    "\n",
    "diff_rmse = np.subtract(train_rmse, test_rmse)\n",
    "\n",
    "c_list = list(map(int, c_list))\n",
    "    \n",
    "lasso_list_results = [l_scores, train_rmse, test_rmse, diff_rmse]\n",
    "lasso_res_df = pd.DataFrame(lasso_list_results).T\n",
    "lasso_res_df.columns = ['Scores', 'TrainRMSE', 'TestRMSE', 'DiffRMSE']\n",
    "lasso_res_df.index = c_list\n",
    "\n",
    "best_par_df = pd.DataFrame(best_par_list)\n",
    "\n",
    "lasso_res_df = pd.concat([lasso_res_df, best_par_df], axis=1, sort=False)\n",
    "\n",
    "lasso_res_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(random_state=0, eval_method='rmse', tree_method='gpu_hist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "maxdepth = range(2, 6)\n",
    "eta_ = np.linspace(1e-2, 1, 10)\n",
    "gamma_ = np.linspace(1e-5, 100, 30)\n",
    "subsample_ = np.linspace(0, 1, 3)\n",
    "# updater_ = [grow_colmaker,prune,grow_gpu_hist]\n",
    "\n",
    "n_folds=ms.KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1, \n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain_gb, ytrain_gb)\n",
    "\n",
    "print('#'*50)\n",
    "print('\\n')\n",
    "\n",
    "print('Currently, the best parameters are: ', gs_xgb.best_params_)\n",
    "\n",
    "print('where the learning rate is: ', gs_xgb.best_params_['eta'])\n",
    "print('and the L1 ratio is: ', gs_xgb.best_params_['max_depth'])\n",
    "print('where the alpha is: ', gs_xgb.best_params_['gamma'])\n",
    "print('and the L1 ratio is: ', gs_xgb.best_params_['subsample'])\n",
    "\n",
    "print('#'*50)\n",
    "print('\\n')\n",
    "\n",
    "print('The best score is: ', gs_xgb.best_estimator_.score(xtrain_gb, ytrain_gb))\n",
    "\n",
    "print('#'*50)\n",
    "print('\\n')\n",
    "\n",
    "print('The RMSE is: ', rmse(gs_xgb, ytrain_gb, xtrain_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gs_xgb.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('The test set RMSE is: ', rmse(gs_xgb, ytest_gb, xtest_gb))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(gs_xgb.best_estimator_.predict(xtrain_gb), ytrain, color='blue', alpha=0.4)\n",
    "plt.scatter(gs_xgb.best_estimator_.predict(xtest_gb), ytest, color='red', alpha=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run with Lasso Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdepth = range(2, 6)\n",
    "eta_ = np.linspace(1e-2, 1, 10)\n",
    "gamma_ = np.linspace(1e-5, 100, 30)\n",
    "subsample_ = np.linspace(0, 1, 3)\n",
    "# updater_ = [grow_colmaker,prune,grow_gpu_hist]\n",
    "\n",
    "n_folds=ms.KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1, \n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "%time gs_xgb.fit(xtrain_gb_out, lasso_trained_label)\n",
    "\n",
    "print('#'*50)\n",
    "print('\\n')\n",
    "\n",
    "print('Currently, the best parameters are: ', gs_xgb.best_params_)\n",
    "\n",
    "print('where the learning rate is: ', gs_xgb.best_params_['eta'])\n",
    "print('and the L1 ratio is: ', gs_xgb.best_params_['max_depth'])\n",
    "print('where the alpha is: ', gs_xgb.best_params_['gamma'])\n",
    "print('and the L1 ratio is: ', gs_xgb.best_params_['subsample'])\n",
    "\n",
    "print('#'*50)\n",
    "print('\\n')\n",
    "\n",
    "print('The best score is: ', gs_xgb.best_estimator_.score(xtrain_gb_out, lasso_trained_label))\n",
    "\n",
    "print('#'*50)\n",
    "print('\\n')\n",
    "\n",
    "print('The RMSE is: ', rmse(gs_xgb, lasso_trained_label, xtrain_gb_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The test set RMSE is: ', rmse(gs_xgb, ytest_gb, xtest_gb))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "plt.scatter(gs_xgb.best_estimator_.predict(xtrain_gb), lasso_trained_label, color='blue', alpha=0.4)\n",
    "plt.scatter(gs_xgb.best_estimator_.predict(xtest_gb), ytest, color='red', alpha=0.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Due to the low RMSE, we will attempt XGBoost with "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "hp_nonimpute = pd.read_csv('../hp_nonimpute.csv')\n",
    "hp_nonimpute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_nonimpute, xtest_nonimpute, ytrain_nonimpute, ytest_nonimpute = ms.train_test_split(hp_tree_fullyimputed, \n",
    "                                                                                           hp_logsaleprice, \n",
    "                                                                                           test_size=0.2, \n",
    "                                                                                           random_state=0)\n",
    "\n",
    "ytrain_nonimpute = ytrain_nonimpute.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBRegressor(random_state=0)\n",
    "\n",
    "xgb.fit(xtrain_nonimpute, ytrain_nonimpute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxdepth = range(2, 6)\n",
    "eta_ = np.linspace(1e-2, 1, 10)\n",
    "gamma_ = np.linspace(1e-5, 100, 30)\n",
    "subsample_ = np.linspace(0, 1, 3)\n",
    "# updater_ = [grow_colmaker,prune,grow_gpu_hist]\n",
    "\n",
    "n_folds=ms.KFold(n_splits=5, random_state=0, shuffle=True)\n",
    "\n",
    "gparam_xgb = {'max_depth': maxdepth,\n",
    "              'eta': eta_,\n",
    "              'gamma': gamma_,\n",
    "              'subsample': subsample_}\n",
    "\n",
    "gs_xgb = GridSearchCV(xgb, gparam_xgb, cv=n_folds, refit=True, n_jobs=-1, \n",
    "                      scoring='neg_root_mean_squared_error', return_train_score=True)\n",
    "\n",
    "print(gs_xgb.fit(xtrain_gb, ytrain_gb))\n",
    "\n",
    "print('#'*50)\n",
    "print('\\n')\n",
    "\n",
    "print('Currently, the best parameters are: ', gs_xgb.best_params_)\n",
    "\n",
    "print('where the learning rate is: ', gs_xgb.best_params_['eta'])\n",
    "print('and the L1 ratio is: ', gs_xgb.best_params_['max_depth'])\n",
    "print('where the alpha is: ', gs_xgb.best_params_['gamma'])\n",
    "print('and the L1 ratio is: ', gs_xgb.best_params_['subsample'])\n",
    "\n",
    "print('#'*50)\n",
    "print('\\n')\n",
    "\n",
    "print('The best score is: ', gs_xgb.best_estimator_.score(xtrain_gb, ytrain_gb))\n",
    "\n",
    "print('#'*50)\n",
    "print('\\n')\n",
    "\n",
    "print('The RMSE is: ', rmse(gs_xgb, ytrain_gb, xtrain_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
